const _astro_dataLayerContent = [["Map",1,2,9,10,1041,1042],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.1","content-config-digest","f02f609492111f32","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://astro-my.vercel.app/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"never\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"nord\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","learn",["Map",11,12,39,40,70,71,105,106,142,143,167,168,198,199,229,230,260,261,306,307,418,419,451,452,476,477,513,514,551,552,585,586,637,638,743,744,779,780,847,848,909,910,950,951,982,983],"envoy-proxy",{id:11,data:13,body:21,filePath:22,digest:23,rendered:24,legacyId:38},{title:14,excerpt:15,categories:16,date:18,description:19,draft:20},"Envoy Proxy","Envoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. Envoy(/what-is-envoy-proxy/) is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, distributed application.",[17],"Kubernetes",["Date","2024-09-30T02:58:20.000Z"],"Discover the power of Envoy Proxy with Tetrate. Learn how this modern proxy enhances your network's security, scalability, and performance.",false,"## What does Envoy mean?\n\nEnvoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. [Envoy](/what-is-envoy-proxy) is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, [distributed application architectures.](/blog/nist-sp-800-207a-explained-zero-trust-architecture-model-for-access-control)\n\n[Learn more ›](/what-is-envoy-proxy)","src/content/learn/envoy-proxy/index.md","c351b35c1ee273f9",{html:25,metadata:26},"<h2 id=\"what-does-envoy-mean\">What does Envoy mean?</h2>\n<p>Envoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. <a href=\"/what-is-envoy-proxy\">Envoy</a> is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, <a href=\"/blog/nist-sp-800-207a-explained-zero-trust-architecture-model-for-access-control\">distributed application architectures.</a></p>\n<p><a href=\"/what-is-envoy-proxy\">Learn more ›</a></p>",{headings:27,localImagePaths:32,remoteImagePaths:33,frontmatter:34,imagePaths:37},[28],{depth:29,slug:30,text:31},2,"what-does-envoy-mean","What does Envoy mean?",[],[],{title:14,slug:11,date:35,description:19,categories:36,excerpt:15},["Date","2024-09-30T02:58:20.000Z"],[17],[],"envoy-proxy/index.md","how-to-secure-kubernetes",{id:39,data:41,body:47,filePath:48,digest:49,rendered:50,legacyId:69},{title:42,excerpt:43,categories:44,date:45,description:46,draft:20},"How To Secure Kubernetes","Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes.",[17],["Date","2024-10-24T03:53:52.000Z"],"Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments…","Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes security.\n\n## Securing Kubernetes\n\nTo secure Kubernetes deployments with manageable overhead, it’s best to follow industry best practices that have been devised and honed over time. No single tool or process can be adopted to deliver easier management and enhanced security for Kubernetes-based infrastructure. Like other IT management and cybersecurity areas, securing Kubernetes requires a multilayered approach using built-in and external functionality. Here is a list of best practices that should be on your radar when planning and implementing Kubernetes security.\n\n1.  Use namespaces and access control—[(RBAC (role-based access control), ABAC (attribute-based access control), or NGAC (next-generation access control)](/blog/rbac-vs-abac-vs-ngac)—to isolate workloads and minimize the potential impact of a security breach. Each application should run in its own namespace with granular access control policy to enforce least privilege access.\n2.  Secure Kubernetes control plane components, such as the API server. Implement strong authentication and authorization policies, encrypt data in transit, and limit administrator (and regular user) access to the control plane.\n3.  Secure the container runtime and images. Scan images for vulnerabilities, enforce least privilege access to containers, and monitor containers in production for suspicious activity. \n4.  Network security is crucial. Employ [a service mesh](/what-is-istio-service-mesh) to encrypt traffic between services, enforce authentication and authorization policies, and segment applications. Implement network policies to restrict traffic between pods and namespaces.\n5.  Protect application secrets using secrets management tools and encrypt them at rest. Change secrets frequently to minimize the risk of their compromise.\n6.  Implement logging and monitoring to detect threats. Use sophisticated monitoring and alerting tools to flag suspicious or anomalous activity.\n7.  Develop an incident response plan and practice it regularly. Having such a plan and knowing how to use it is critical for quickly containing cyber-attacks and recovering from incidents.\n\n## How Tetrate Simplifies Kubernetes Security\n\nImplementing these security best practices across multiple microservices and containers deployed via Kubernetes is complex and time-consuming. The solutions available from Tetrate can reduce this complexity and the time needed to configure and monitor a Kubernetes-based landscape.\n\nTetrate offers a comprehensive gateway and service mesh platform designed to secure service communication within Kubernetes environments. With Tetrate, you can:\n\n*   Effortlessly encrypt all service-to-service communication.\n*   Implement detailed access policies based on identity and context.\n*   Monitor service behavior to identify potential threats.\n*   Apply consistent high-level security and access policies.\n\nTetrate integrates with authentication/authorization services, certificate management, workload identity, and other essential security tools for comprehensive cloud-native security. Organizations using Tetrate simplify their Kubernetes management experience and enhance their overall security posture.\n\n## Final Thoughts\n\nIt is crucial to secure Kubernetes, but doing so can be complex. Organizations can mitigate risks effectively by prioritizing basic best practices such as isolation, access control, runtime security, secrets management, and monitoring. Implementing these best practices consistently and at scale is greatly simplified by using Tetrate.","src/content/learn/how-to-secure-kubernetes/index.md","f0c0d02fab0099e7",{html:51,metadata:52},"<p>Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes security.</p>\n<h2 id=\"securing-kubernetes\">Securing Kubernetes</h2>\n<p>To secure Kubernetes deployments with manageable overhead, it’s best to follow industry best practices that have been devised and honed over time. No single tool or process can be adopted to deliver easier management and enhanced security for Kubernetes-based infrastructure. Like other IT management and cybersecurity areas, securing Kubernetes requires a multilayered approach using built-in and external functionality. Here is a list of best practices that should be on your radar when planning and implementing Kubernetes security.</p>\n<ol>\n<li>Use namespaces and access control—<a href=\"/blog/rbac-vs-abac-vs-ngac\">(RBAC (role-based access control), ABAC (attribute-based access control), or NGAC (next-generation access control)</a>—to isolate workloads and minimize the potential impact of a security breach. Each application should run in its own namespace with granular access control policy to enforce least privilege access.</li>\n<li>Secure Kubernetes control plane components, such as the API server. Implement strong authentication and authorization policies, encrypt data in transit, and limit administrator (and regular user) access to the control plane.</li>\n<li>Secure the container runtime and images. Scan images for vulnerabilities, enforce least privilege access to containers, and monitor containers in production for suspicious activity. </li>\n<li>Network security is crucial. Employ <a href=\"/what-is-istio-service-mesh\">a service mesh</a> to encrypt traffic between services, enforce authentication and authorization policies, and segment applications. Implement network policies to restrict traffic between pods and namespaces.</li>\n<li>Protect application secrets using secrets management tools and encrypt them at rest. Change secrets frequently to minimize the risk of their compromise.</li>\n<li>Implement logging and monitoring to detect threats. Use sophisticated monitoring and alerting tools to flag suspicious or anomalous activity.</li>\n<li>Develop an incident response plan and practice it regularly. Having such a plan and knowing how to use it is critical for quickly containing cyber-attacks and recovering from incidents.</li>\n</ol>\n<h2 id=\"how-tetrate-simplifies-kubernetes-security\">How Tetrate Simplifies Kubernetes Security</h2>\n<p>Implementing these security best practices across multiple microservices and containers deployed via Kubernetes is complex and time-consuming. The solutions available from Tetrate can reduce this complexity and the time needed to configure and monitor a Kubernetes-based landscape.</p>\n<p>Tetrate offers a comprehensive gateway and service mesh platform designed to secure service communication within Kubernetes environments. With Tetrate, you can:</p>\n<ul>\n<li>Effortlessly encrypt all service-to-service communication.</li>\n<li>Implement detailed access policies based on identity and context.</li>\n<li>Monitor service behavior to identify potential threats.</li>\n<li>Apply consistent high-level security and access policies.</li>\n</ul>\n<p>Tetrate integrates with authentication/authorization services, certificate management, workload identity, and other essential security tools for comprehensive cloud-native security. Organizations using Tetrate simplify their Kubernetes management experience and enhance their overall security posture.</p>\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n<p>It is crucial to secure Kubernetes, but doing so can be complex. Organizations can mitigate risks effectively by prioritizing basic best practices such as isolation, access control, runtime security, secrets management, and monitoring. Implementing these best practices consistently and at scale is greatly simplified by using Tetrate.</p>",{headings:53,localImagePaths:63,remoteImagePaths:64,frontmatter:65,imagePaths:68},[54,57,60],{depth:29,slug:55,text:56},"securing-kubernetes","Securing Kubernetes",{depth:29,slug:58,text:59},"how-tetrate-simplifies-kubernetes-security","How Tetrate Simplifies Kubernetes Security",{depth:29,slug:61,text:62},"final-thoughts","Final Thoughts",[],[],{title:42,slug:39,date:66,description:46,categories:67,excerpt:43},["Date","2024-10-24T03:53:52.000Z"],[17],[],"how-to-secure-kubernetes/index.md","ingress-and-egress-architecture",{id:70,data:72,body:79,filePath:80,digest:81,rendered:82,legacyId:104},{title:73,excerpt:74,categories:75,date:77,description:78,draft:20},"Ingress and Egress Architecture","Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and.",[76],"Ingress",["Date","2025-05-05T17:03:16.000Z"],"Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic…","Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and performance.\n\n## Managing Ingress Traffic in Kubernetes\n\nIngress and Egress work together to deliver the inward and outward traffic flow to and from a Kubernetes cluster. Ingress brings external requests into the cluster, while Egress ensures secure outbound communication. Together, they create a well-structured traffic flow, enhancing performance, security, and compliance.\n\nIngress in Kubernetes uses several components:\n\n- **Ingress Controllers** – Manages external access by routing traffic based on domain names, paths, and protocols. Envoy Gateway is the modern standard for Kubernetes Ingress, providing advanced security, observability, and multi-tenancy support.\n- **TLS Termination** – Ensures secure traffic by handling TLS/SSL encryption at the Ingress layer.\n- **Load Balancing & Traffic Shaping** – Distributes incoming requests efficiently, supporting QA deployments and canary releases.\n\n## Managing Egress Traffic in Kubernetes\n\nEgress in Kubernetes is built on several components:\n\n- **Egress Gateways** – Enforce outbound traffic policies, ensuring controlled communication with external APIs or cloud services.\n- **Network Policies** – Restrict Egress traffic to limit unauthorized data flows and prevent leaks.\n- **Service Mesh Integration** – Istio’s Egress gateway provides fine-grained control over external connections, enforcing authentication and mutual TLS (mTLS).\n\n## Ingress vs. Egress – Understanding the Differences\n\nThe table below compares and contrasts Kubernetes Ingress and Egress for traffic routing.\n\n**Ingress Traffic**\n\n**Egress Traffic**\n\n**Definition**\n\nTraffic coming **into** the cluster from **External**Sources\n\nTraffic **leaving** the cluster to **External**destinations\n\n**Primary Purpose**\n\nRoutes external HTTPS traffic to Kubernetes Services\n\nControls outbound communication from Pods to External clients and systems\n\n**Managed By**\n\nIngress Controllers (such as Envoy Gateway or HAProxy)\n\nEgress gateways, network policies, Kubernetes NAT configurations\n\n**Security Features**\n\nTLS termination, authentication, and access control\n\nPreventing unauthorized data exfiltration, securing external dependencies\n\n## Optimize Ingress & Egress Traffic with Tetrate\n\nMisconfigurations in Ingress and Egress traffic can lead to security risks, performance bottlenecks, and compliance issues. Tetrate [Consulting](/kubernetes-consulting) offers expert guidance on Kubernetes’ Ingress controllers, Egress gateways, and service mesh configurations. Tetrate’s Istio-based service mesh optimizes Ingress and Egress traffic, providing centralized control, monitoring, and security. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.","src/content/learn/ingress-and-egress-architecture/index.md","393795197aa68bef",{html:83,metadata:84},"<p>Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and performance.</p>\n<h2 id=\"managing-ingress-traffic-in-kubernetes\">Managing Ingress Traffic in Kubernetes</h2>\n<p>Ingress and Egress work together to deliver the inward and outward traffic flow to and from a Kubernetes cluster. Ingress brings external requests into the cluster, while Egress ensures secure outbound communication. Together, they create a well-structured traffic flow, enhancing performance, security, and compliance.</p>\n<p>Ingress in Kubernetes uses several components:</p>\n<ul>\n<li><strong>Ingress Controllers</strong> – Manages external access by routing traffic based on domain names, paths, and protocols. Envoy Gateway is the modern standard for Kubernetes Ingress, providing advanced security, observability, and multi-tenancy support.</li>\n<li><strong>TLS Termination</strong> – Ensures secure traffic by handling TLS/SSL encryption at the Ingress layer.</li>\n<li><strong>Load Balancing &#x26; Traffic Shaping</strong> – Distributes incoming requests efficiently, supporting QA deployments and canary releases.</li>\n</ul>\n<h2 id=\"managing-egress-traffic-in-kubernetes\">Managing Egress Traffic in Kubernetes</h2>\n<p>Egress in Kubernetes is built on several components:</p>\n<ul>\n<li><strong>Egress Gateways</strong> – Enforce outbound traffic policies, ensuring controlled communication with external APIs or cloud services.</li>\n<li><strong>Network Policies</strong> – Restrict Egress traffic to limit unauthorized data flows and prevent leaks.</li>\n<li><strong>Service Mesh Integration</strong> – Istio’s Egress gateway provides fine-grained control over external connections, enforcing authentication and mutual TLS (mTLS).</li>\n</ul>\n<h2 id=\"ingress-vs-egress--understanding-the-differences\">Ingress vs. Egress – Understanding the Differences</h2>\n<p>The table below compares and contrasts Kubernetes Ingress and Egress for traffic routing.</p>\n<p><strong>Ingress Traffic</strong></p>\n<p><strong>Egress Traffic</strong></p>\n<p><strong>Definition</strong></p>\n<p>Traffic coming <strong>into</strong> the cluster from <strong>External</strong>Sources</p>\n<p>Traffic <strong>leaving</strong> the cluster to <strong>External</strong>destinations</p>\n<p><strong>Primary Purpose</strong></p>\n<p>Routes external HTTPS traffic to Kubernetes Services</p>\n<p>Controls outbound communication from Pods to External clients and systems</p>\n<p><strong>Managed By</strong></p>\n<p>Ingress Controllers (such as Envoy Gateway or HAProxy)</p>\n<p>Egress gateways, network policies, Kubernetes NAT configurations</p>\n<p><strong>Security Features</strong></p>\n<p>TLS termination, authentication, and access control</p>\n<p>Preventing unauthorized data exfiltration, securing external dependencies</p>\n<h2 id=\"optimize-ingress--egress-traffic-with-tetrate\">Optimize Ingress &#x26; Egress Traffic with Tetrate</h2>\n<p>Misconfigurations in Ingress and Egress traffic can lead to security risks, performance bottlenecks, and compliance issues. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> offers expert guidance on Kubernetes’ Ingress controllers, Egress gateways, and service mesh configurations. Tetrate’s Istio-based service mesh optimizes Ingress and Egress traffic, providing centralized control, monitoring, and security. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.</p>",{headings:85,localImagePaths:98,remoteImagePaths:99,frontmatter:100,imagePaths:103},[86,89,92,95],{depth:29,slug:87,text:88},"managing-ingress-traffic-in-kubernetes","Managing Ingress Traffic in Kubernetes",{depth:29,slug:90,text:91},"managing-egress-traffic-in-kubernetes","Managing Egress Traffic in Kubernetes",{depth:29,slug:93,text:94},"ingress-vs-egress--understanding-the-differences","Ingress vs. Egress – Understanding the Differences",{depth:29,slug:96,text:97},"optimize-ingress--egress-traffic-with-tetrate","Optimize Ingress & Egress Traffic with Tetrate",[],[],{title:73,slug:70,date:101,description:78,categories:102,excerpt:74},["Date","2025-05-05T17:03:16.000Z"],[76],[],"ingress-and-egress-architecture/index.md","difference-between-ingress-and-service-in-kubernetes",{id:105,data:107,body:113,filePath:114,digest:115,rendered:116,legacyId:141},{title:108,excerpt:109,categories:110,date:111,description:112,draft:20},"Difference Between Ingress and Service in Kubernetes","Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A Service provides internal networking within a cluster, while Ingress manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network.",[17],["Date","2025-05-05T17:05:27.000Z"],"Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A Service provides internal networking within a…","Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A **Service** provides internal networking within a cluster, while **Ingress** manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network architecture.\n\n## What is a Kubernetes Service?\n\nA Service defines a logical abstraction that describes how users, applications, or other systems can access a group of Pods within a cluster. Since Pods are ephemeral and may be restarted or moved across nodes, the Service ensures stable application connectivity and prevents disruptions. There are several types of Kubernetes Services:\n\n*   **ClusterIP (Default)** – Exposes the Service on a cluster-internal IP. This makes the Service reachable only from within the cluster. It is the default type if no other service type is specified.\n*   **NodePort** – Exposes the Service on each Node’s IP at a static port (the NodePort). Kubernetes also sets up a cluster IP, making the Service accessible as if it were a ClusterIP.  \n    **LoadBalancer** – Exposes the Service externally using an external load balancer. Kubernetes does not include a built-in load balancer, so you must provide one or integrate with a cloud provider that supports it.\n*   **ExternalName** – Maps the Service to the value of the externalName field (e.g., a DNS hostname). This configures the cluster’s DNS to return a CNAME record pointing to the external hostname. No proxying is used. \n\n## What is Kubernetes Ingress?\n\nIngress is a higher-level method that provides an API for HTTP/HTTPS access to externally exposed services. Rather than assigning external IPs to each Service (via LoadBalancer or NodePort), Ingress consolidates access through a single entry point and routes traffic to the appropriate Service based on defined rules. Ingress understands web concepts such as hostnames, URIs, and paths. It allows you to map traffic to different backends using rules defined through the Kubernetes API. \n\n## Key Differences Between Ingress and Service\n\nThe table below compares Kubernetes Service and Ingress for traffic management:\n\n**Kubernetes Service**\n\n**Kubernetes Ingress**\n\n**Purpose**\n\nInternal service discovery and traffic routing\n\nExternal traffic routing to services\n\n**Scope**\n\nManages Traffic inside the cluster\n\nManages traffic from outside the cluster\n\n**Types**\n\nClusterIP, NodePort, LoadBalancer\n\nUses Ingress Controllers (e.g. Envoy Gateway)\n\n**Load Balancing**\n\nInternal load balancing across Pods\n\nHTTPS based traffic routing\n\n**TLS Support**\n\nNot built in – requires manual addition\n\nSupports TLS termination for secure traffic\n\n## How Service and Ingress Work Together\n\nKubernetes deployments typically use both Services and Ingress to manage traffic flow. Use a Service to enable stable internal access to a set of Pods. Use Ingress as a gateway to direct external traffic to the appropriate application or microservice based on hostname, path, or other routing rules. Modern deployments often combine Ingress with a service mesh like Istio to add advanced traffic control, security, and observability. \n\n## Optimize Kubernetes Networking with Tetrate\n\nDesigning Ingress and Service configurations can be complex. Tetrate [Consulting](/kubernetes-consulting) helps organizations implement secure, scalable, and optimized Kubernetes networking architectures using Istio and advanced traffic management. Our experts can help you refine your Ingress, service mesh, and overall traffic management strategies. Connect with Tetrate’s consulting services for expert guidance on Kubernetes networking.","src/content/learn/difference-between-ingress-and-service-in-kubernetes/index.md","1c4981fd4d0fbc76",{html:117,metadata:118},"<p>Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A <strong>Service</strong> provides internal networking within a cluster, while <strong>Ingress</strong> manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network architecture.</p>\n<h2 id=\"what-is-a-kubernetes-service\">What is a Kubernetes Service?</h2>\n<p>A Service defines a logical abstraction that describes how users, applications, or other systems can access a group of Pods within a cluster. Since Pods are ephemeral and may be restarted or moved across nodes, the Service ensures stable application connectivity and prevents disruptions. There are several types of Kubernetes Services:</p>\n<ul>\n<li><strong>ClusterIP (Default)</strong> – Exposes the Service on a cluster-internal IP. This makes the Service reachable only from within the cluster. It is the default type if no other service type is specified.</li>\n<li><strong>NodePort</strong> – Exposes the Service on each Node’s IP at a static port (the NodePort). Kubernetes also sets up a cluster IP, making the Service accessible as if it were a ClusterIP.<br>\n<strong>LoadBalancer</strong> – Exposes the Service externally using an external load balancer. Kubernetes does not include a built-in load balancer, so you must provide one or integrate with a cloud provider that supports it.</li>\n<li><strong>ExternalName</strong> – Maps the Service to the value of the externalName field (e.g., a DNS hostname). This configures the cluster’s DNS to return a CNAME record pointing to the external hostname. No proxying is used. </li>\n</ul>\n<h2 id=\"what-is-kubernetes-ingress\">What is Kubernetes Ingress?</h2>\n<p>Ingress is a higher-level method that provides an API for HTTP/HTTPS access to externally exposed services. Rather than assigning external IPs to each Service (via LoadBalancer or NodePort), Ingress consolidates access through a single entry point and routes traffic to the appropriate Service based on defined rules. Ingress understands web concepts such as hostnames, URIs, and paths. It allows you to map traffic to different backends using rules defined through the Kubernetes API. </p>\n<h2 id=\"key-differences-between-ingress-and-service\">Key Differences Between Ingress and Service</h2>\n<p>The table below compares Kubernetes Service and Ingress for traffic management:</p>\n<p><strong>Kubernetes Service</strong></p>\n<p><strong>Kubernetes Ingress</strong></p>\n<p><strong>Purpose</strong></p>\n<p>Internal service discovery and traffic routing</p>\n<p>External traffic routing to services</p>\n<p><strong>Scope</strong></p>\n<p>Manages Traffic inside the cluster</p>\n<p>Manages traffic from outside the cluster</p>\n<p><strong>Types</strong></p>\n<p>ClusterIP, NodePort, LoadBalancer</p>\n<p>Uses Ingress Controllers (e.g. Envoy Gateway)</p>\n<p><strong>Load Balancing</strong></p>\n<p>Internal load balancing across Pods</p>\n<p>HTTPS based traffic routing</p>\n<p><strong>TLS Support</strong></p>\n<p>Not built in – requires manual addition</p>\n<p>Supports TLS termination for secure traffic</p>\n<h2 id=\"how-service-and-ingress-work-together\">How Service and Ingress Work Together</h2>\n<p>Kubernetes deployments typically use both Services and Ingress to manage traffic flow. Use a Service to enable stable internal access to a set of Pods. Use Ingress as a gateway to direct external traffic to the appropriate application or microservice based on hostname, path, or other routing rules. Modern deployments often combine Ingress with a service mesh like Istio to add advanced traffic control, security, and observability. </p>\n<h2 id=\"optimize-kubernetes-networking-with-tetrate\">Optimize Kubernetes Networking with Tetrate</h2>\n<p>Designing Ingress and Service configurations can be complex. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> helps organizations implement secure, scalable, and optimized Kubernetes networking architectures using Istio and advanced traffic management. Our experts can help you refine your Ingress, service mesh, and overall traffic management strategies. Connect with Tetrate’s consulting services for expert guidance on Kubernetes networking.</p>",{headings:119,localImagePaths:135,remoteImagePaths:136,frontmatter:137,imagePaths:140},[120,123,126,129,132],{depth:29,slug:121,text:122},"what-is-a-kubernetes-service","What is a Kubernetes Service?",{depth:29,slug:124,text:125},"what-is-kubernetes-ingress","What is Kubernetes Ingress?",{depth:29,slug:127,text:128},"key-differences-between-ingress-and-service","Key Differences Between Ingress and Service",{depth:29,slug:130,text:131},"how-service-and-ingress-work-together","How Service and Ingress Work Together",{depth:29,slug:133,text:134},"optimize-kubernetes-networking-with-tetrate","Optimize Kubernetes Networking with Tetrate",[],[],{title:108,slug:105,date:138,description:112,categories:139,excerpt:109},["Date","2025-05-05T17:05:27.000Z"],[17],[],"difference-between-ingress-and-service-in-kubernetes/index.md","istio-service-mesh",{id:142,data:144,body:150,filePath:151,digest:152,rendered:153,legacyId:166},{title:145,excerpt:146,categories:147,date:148,description:149,draft:20},"Istio Service Mesh","Learn more.",[17],["Date","2024-10-17T08:15:11.000Z"],"Explore Tetrate's comprehensive Istio service mesh guide. Discover how Istio enhances microservices with robust traffic management, security, and observability.","## Overview\n\n[Istio](/external-link/)\n\n[Learn more ›](/what-is-istio-service-mesh)","src/content/learn/istio-service-mesh/index.md","6514eb68e7c98cdb",{html:154,metadata:155},"<h2 id=\"overview\">Overview</h2>\n<p><a href=\"/external-link/\">Istio</a></p>\n<p><a href=\"/what-is-istio-service-mesh\">Learn more ›</a></p>",{headings:156,localImagePaths:160,remoteImagePaths:161,frontmatter:162,imagePaths:165},[157],{depth:29,slug:158,text:159},"overview","Overview",[],[],{title:145,slug:142,date:163,description:149,categories:164,excerpt:146},["Date","2024-10-17T08:15:11.000Z"],[17],[],"istio-service-mesh/index.md","ingress-controller-vs-load-balancer",{id:167,data:169,body:175,filePath:176,digest:177,rendered:178,legacyId:197},{title:170,excerpt:171,categories:172,date:173,description:174,draft:20},"Ingress Controller vs Load Balancer","When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and.",[76],["Date","2025-04-14T17:07:11.000Z"],"When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress…","When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and performance.\n\n## Load Balancers in Kubernetes?\n\nA load balancer distributes incoming traffic across multiple instances of a service to ensure high availability and reliability. Kubernetes supports external load balancers, often provided by cloud platforms like AWS, Azure, and GCP, or a dedicated load balancer solution running in a virtual machine.  These load balancers can all allocate a public IP and direct traffic to backend services running in a Kubernetes cluster.  It’s typical to use a Load Balancer when:\n\n- You need direct external access to a service without additional routing logic.\n- You’re running Kubernetes on a cloud platform with managed load balancing.\n- You need global traffic distribution across multiple clusters or regions.\n\n## What is an Ingress Controller?\n\nAn Ingress Controller acts as a traffic gateway, managing HTTPS requests and routing them to the appropriate Kubernetes Services based on domain names, paths, or other routing criteria. Unlike a load balancer, an Ingress Controller consolidates external access through a single entry point, reducing the number of external IPs required. It’s appropriate to use an Ingress Controller when:\n\n- You have multiple services that need controlled access under a single domain.\n- You require advanced traffic routing (e.g., path-based, host-based, or header-based routing).\n- You need built-in TLS termination for secure HTTPS communication.\n- You want to integrate authentication, rate limiting, and observability.\n\nIngress Controllers and Load Balancers complement each other. A typical best practice is to use a Load Balancer at the infrastructure level to direct traffic to an Ingress Controller, which then manages fine-grained traffic routing to internal services.\n\n## Tetrate Simplifies Ingress and Load Balancing\n\nCorrectly configuring Ingress Controllers and Load Balancers delivers optimal scalability, security, and efficiency. The Tetrate Consulting Services team provides expert guidance on designing and deploying optimized, secure, and resilient Kubernetes environments. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.","src/content/learn/ingress-controller-vs-load-balancer/index.md","916f8acaa817df23",{html:179,metadata:180},"<p>When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and performance.</p>\n<h2 id=\"load-balancers-in-kubernetes\">Load Balancers in Kubernetes?</h2>\n<p>A load balancer distributes incoming traffic across multiple instances of a service to ensure high availability and reliability. Kubernetes supports external load balancers, often provided by cloud platforms like AWS, Azure, and GCP, or a dedicated load balancer solution running in a virtual machine.  These load balancers can all allocate a public IP and direct traffic to backend services running in a Kubernetes cluster.  It’s typical to use a Load Balancer when:</p>\n<ul>\n<li>You need direct external access to a service without additional routing logic.</li>\n<li>You’re running Kubernetes on a cloud platform with managed load balancing.</li>\n<li>You need global traffic distribution across multiple clusters or regions.</li>\n</ul>\n<h2 id=\"what-is-an-ingress-controller\">What is an Ingress Controller?</h2>\n<p>An Ingress Controller acts as a traffic gateway, managing HTTPS requests and routing them to the appropriate Kubernetes Services based on domain names, paths, or other routing criteria. Unlike a load balancer, an Ingress Controller consolidates external access through a single entry point, reducing the number of external IPs required. It’s appropriate to use an Ingress Controller when:</p>\n<ul>\n<li>You have multiple services that need controlled access under a single domain.</li>\n<li>You require advanced traffic routing (e.g., path-based, host-based, or header-based routing).</li>\n<li>You need built-in TLS termination for secure HTTPS communication.</li>\n<li>You want to integrate authentication, rate limiting, and observability.</li>\n</ul>\n<p>Ingress Controllers and Load Balancers complement each other. A typical best practice is to use a Load Balancer at the infrastructure level to direct traffic to an Ingress Controller, which then manages fine-grained traffic routing to internal services.</p>\n<h2 id=\"tetrate-simplifies-ingress-and-load-balancing\">Tetrate Simplifies Ingress and Load Balancing</h2>\n<p>Correctly configuring Ingress Controllers and Load Balancers delivers optimal scalability, security, and efficiency. The Tetrate Consulting Services team provides expert guidance on designing and deploying optimized, secure, and resilient Kubernetes environments. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.</p>",{headings:181,localImagePaths:191,remoteImagePaths:192,frontmatter:193,imagePaths:196},[182,185,188],{depth:29,slug:183,text:184},"load-balancers-in-kubernetes","Load Balancers in Kubernetes?",{depth:29,slug:186,text:187},"what-is-an-ingress-controller","What is an Ingress Controller?",{depth:29,slug:189,text:190},"tetrate-simplifies-ingress-and-load-balancing","Tetrate Simplifies Ingress and Load Balancing",[],[],{title:170,slug:167,date:194,description:174,categories:195,excerpt:171},["Date","2025-04-14T17:07:11.000Z"],[76],[],"ingress-controller-vs-load-balancer/index.md","container-ingress-traffic-management-capabilities",{id:198,data:200,body:206,filePath:207,digest:208,rendered:209,legacyId:228},{title:201,excerpt:202,categories:203,date:204,description:205,draft:20},"Container Ingress Traffic Management Capabilities","Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and.",[76],["Date","2025-05-05T17:04:33.000Z"],"Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress…","Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and security.\n\n## How Container Ingress Traffic Management Works\n\nKubernetes manages Ingress traffic using three main components: the Ingress resource, the Ingress controller, and supporting services:\n\n- **Ingress Resource** – Defines rules for routing HTTP/HTTPS traffic to internal services.\n- **Ingress Controller** – Processes the Ingress resource rules and directs traffic accordingly. Common examples include Envoy Gateway, NGINX, and HAProxy.\n\nA service mesh like Istio enhances Ingress traffic management by adding fine-grained control, mTLS encryption, and observability for monitoring traffic flows.\n\n## Why Proper Configuration is Crucial\n\nIncorrect Ingress traffic management can lead to serious problems, including:\n\n- **Application Downtime & Latency** – Poorly configured Ingress can cause misrouted traffic, overloaded services, or inefficient failover strategies, resulting in degraded performance or outages.\n- **Security Vulnerabilities** – Misconfigured TLS can expose sensitive data, creating compliance risks and potential data breaches.\n- **Traffic Bottlenecks** – Inadequate load balancing and improper rate limiting can overload some Pods while underutilizing others.\n- **Observability Gaps** – Without proper logging and monitoring, diagnosing traffic issues becomes difficult, increasing the time required to resolve incidents.\n\n## How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management\n\nManaging container Ingress traffic requires deep expertise in networking, security, and Kubernetes architecture. Tetrate [Consulting](/kubernetes-consulting) helps organizations configure Ingress controllers, service meshes, and TLS security to ensure optimal performance and protection. Our experts can help you fine-tune your Kubernetes Ingress, service mesh, and traffic management strategies.\n\nTetrate’s enterprise-grade Istio platform also simplifies and optimizes Ingress traffic management, making it easier to deploy secure, scalable applications in Kubernetes environments.","src/content/learn/container-ingress-traffic-management-capabilities/index.md","debf7542db4a22ef",{html:210,metadata:211},"<p>Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and security.</p>\n<h2 id=\"how-container-ingress-traffic-management-works\">How Container Ingress Traffic Management Works</h2>\n<p>Kubernetes manages Ingress traffic using three main components: the Ingress resource, the Ingress controller, and supporting services:</p>\n<ul>\n<li><strong>Ingress Resource</strong> – Defines rules for routing HTTP/HTTPS traffic to internal services.</li>\n<li><strong>Ingress Controller</strong> – Processes the Ingress resource rules and directs traffic accordingly. Common examples include Envoy Gateway, NGINX, and HAProxy.</li>\n</ul>\n<p>A service mesh like Istio enhances Ingress traffic management by adding fine-grained control, mTLS encryption, and observability for monitoring traffic flows.</p>\n<h2 id=\"why-proper-configuration-is-crucial\">Why Proper Configuration is Crucial</h2>\n<p>Incorrect Ingress traffic management can lead to serious problems, including:</p>\n<ul>\n<li><strong>Application Downtime &#x26; Latency</strong> – Poorly configured Ingress can cause misrouted traffic, overloaded services, or inefficient failover strategies, resulting in degraded performance or outages.</li>\n<li><strong>Security Vulnerabilities</strong> – Misconfigured TLS can expose sensitive data, creating compliance risks and potential data breaches.</li>\n<li><strong>Traffic Bottlenecks</strong> – Inadequate load balancing and improper rate limiting can overload some Pods while underutilizing others.</li>\n<li><strong>Observability Gaps</strong> – Without proper logging and monitoring, diagnosing traffic issues becomes difficult, increasing the time required to resolve incidents.</li>\n</ul>\n<h2 id=\"how-tetrates-istio-based-solutions-improve-ingress-traffic-management\">How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management</h2>\n<p>Managing container Ingress traffic requires deep expertise in networking, security, and Kubernetes architecture. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> helps organizations configure Ingress controllers, service meshes, and TLS security to ensure optimal performance and protection. Our experts can help you fine-tune your Kubernetes Ingress, service mesh, and traffic management strategies.</p>\n<p>Tetrate’s enterprise-grade Istio platform also simplifies and optimizes Ingress traffic management, making it easier to deploy secure, scalable applications in Kubernetes environments.</p>",{headings:212,localImagePaths:222,remoteImagePaths:223,frontmatter:224,imagePaths:227},[213,216,219],{depth:29,slug:214,text:215},"how-container-ingress-traffic-management-works","How Container Ingress Traffic Management Works",{depth:29,slug:217,text:218},"why-proper-configuration-is-crucial","Why Proper Configuration is Crucial",{depth:29,slug:220,text:221},"how-tetrates-istio-based-solutions-improve-ingress-traffic-management","How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management",[],[],{title:201,slug:198,date:225,description:205,categories:226,excerpt:202},["Date","2025-05-05T17:04:33.000Z"],[76],[],"container-ingress-traffic-management-capabilities/index.md","kubernetes-security-architecture",{id:229,data:231,body:237,filePath:238,digest:239,rendered:240,legacyId:259},{title:232,excerpt:233,categories:234,date:235,description:236,draft:20},"Kubernetes Security Architecture","Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security.",[17],["Date","2025-01-21T03:37:22.000Z"],"Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the…","Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security techniques:\n\n## Secure Storage\n\nThe security of the Etcd storage backend, where all cluster data is stored, requires top-tier hardware and software protection.\n\n## Use a Service Mesh\n\nA service mesh adds a layer of security across the Kubernetes platform deployment by adding features like traffic encryption and monitoring to enhance the underlying network security.\n\n## Use Namespaces\n\nDividing Kubernetes clusters into logical segments using namespaces improves resource isolation and limits the impact of any breaches to a subset of the deployed resources. This is part of the broader segmentation recommended for Kubernetes deployments.","src/content/learn/kubernetes-security-architecture/index.md","c643ed2febe02e69",{html:241,metadata:242},"<p>Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security techniques:</p>\n<h2 id=\"secure-storage\">Secure Storage</h2>\n<p>The security of the Etcd storage backend, where all cluster data is stored, requires top-tier hardware and software protection.</p>\n<h2 id=\"use-a-service-mesh\">Use a Service Mesh</h2>\n<p>A service mesh adds a layer of security across the Kubernetes platform deployment by adding features like traffic encryption and monitoring to enhance the underlying network security.</p>\n<h2 id=\"use-namespaces\">Use Namespaces</h2>\n<p>Dividing Kubernetes clusters into logical segments using namespaces improves resource isolation and limits the impact of any breaches to a subset of the deployed resources. This is part of the broader segmentation recommended for Kubernetes deployments.</p>",{headings:243,localImagePaths:253,remoteImagePaths:254,frontmatter:255,imagePaths:258},[244,247,250],{depth:29,slug:245,text:246},"secure-storage","Secure Storage",{depth:29,slug:248,text:249},"use-a-service-mesh","Use a Service Mesh",{depth:29,slug:251,text:252},"use-namespaces","Use Namespaces",[],[],{title:232,slug:229,date:256,description:236,categories:257,excerpt:233},["Date","2025-01-21T03:37:22.000Z"],[17],[],"kubernetes-security-architecture/index.md","use-case-kubernetes-architecture",{id:260,data:262,body:268,filePath:269,digest:270,rendered:271,legacyId:305},{title:263,excerpt:264,categories:265,date:266,description:267,draft:20},"Use Case and Architecture for Tetrate with Kubernetes and Istio","How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like AWS(/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide/) and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and.",[17],["Date","2025-05-05T19:18:41.000Z"],"How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like AWS…","How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like [AWS](/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide) and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and security:\n\n## Use Cases\n\n- **Large-Scale Microservices Management:** eBay’s use case demonstrates how Istio can be used to manage traffic in a complex, multi-datacenter environment with multiple Kubernetes clusters. This showcases Istio’s ability to handle large-scale microservices architectures.\n- **Zero Trust Security Implementation:** Tetrate provides a consistent way to implement Zero Trust security across thousands of microservices in various environments. This is particularly crucial for organizations dealing with sensitive data or those under regulatory compliance like PCI, HIPAA, GDPR, and FIPS/FedRAMP.\n- **Multi-Cloud and Hybrid Cloud Deployments:** Tetrate Service Bridge (TSB) enables management of services across multiple clusters, clouds, and hybrid environments. This is valuable for organizations operating in complex, distributed infrastructures.\n- **Traffic Management and Routing:** Tetrate’s service mesh provides advanced traffic management capabilities, including traffic routing, splitting, and canary deployments. This is useful for organizations looking to implement sophisticated deployment strategies.\n- **Observability and Troubleshooting:** The service mesh offers improved visibility into service health and performance, making it easier to troubleshoot application issues and investigate security incidents.\n- **Kubernetes and VM Integration:** Tetrate has extended Istio’s capabilities to integrate with legacy platforms, allowing organizations to manage both modern Kubernetes microservices and traditional VM applications under a single service mesh.\n- **Compliance and Policy Enforcement:** Tetrate automates the deployment of security policies and enables continuous compliance monitoring, which is particularly valuable for highly regulated industries.\n- **Military and Defense Applications:** The U.S. Air Force has contracted Tetrate for implementing Istio-based resilient communications and enhancing DevSecOps security. This demonstrates the platform’s applicability in high-security, mission-critical environments.\n- **Ambient Mode for Resource Optimization:** Tetrate is implementing ambient mode for the U.S. Air Force, allowing for more efficient resource utilization based on security risk profiles.\n- **Cross-Platform Compatibility:** Tetrate Service Bridge supports multiple cloud platforms like Amazon EKS, Azure AKS, and Red Hat OpenShift, making it versatile for organizations with diverse infrastructure.\n\n## Architecture\n\nTetrate Service Bridge (TSB) provides a comprehensive [service mesh](/blog/service-mesh-architecture) solution that seamlessly integrates with Kubernetes, offering:\n\n- **Unified Control Plane:** TSB extends Istio’s capabilities, providing a single pane of glass for managing multiple clusters across different cloud providers and on-premises environments.\n- **Enhanced Proxy Model in sidecar or ambient mode:** We optimize Envoy’s sidecar proxy implementation, ensuring efficient and secure service-to-service communication.\n- **Multi-Cloud Support:** TSB enables consistent policies and management across AWS EKS, Azure AKS, and other Kubernetes platforms, simplifying multi-cloud deployments.\n\n## Observability\n\nTetrate’s solutions significantly enhance observability in microservices environments:\n\n- **Advanced Metrics Collection:** TSBprovides native dashboards for in-depth performance analysis and integrates with popular tools like Prometheus.\n- **Distributed Tracing:** We offer seamless tracing, providing end-to-end visibility of request flows.\n- **Centralized Logging:** TSB aggregates and analyzes logs from across your service mesh, enabling quick troubleshooting and pattern recognition.\n- **Visualization:** TSB offers native network visualization and real-time metrics insights.\n\n## Security\n\nTetrate puts a strong emphasis on zero trust security:\n\n- **Zero Trust Implementation:** TSB provides out-of-the-box zero trust security, aligning with CISA and NIST recommendations.\n- **Automated mTLS:** We ensure all service-to-service communication is encrypted and authenticated, with automatic certificate management.\n- **Fine-grained Access Control:** TSB enables detailed RBAC and ABAC policies, allowing precise control over service interactions.\n- **Identity Management:** We support integration with external identity providers and implement strong service identity verification.\n- **Policy Enforcement:** TSB integrates with Open Policy Agent (OPA) for advanced authorization policies.\n\n## Advanced Load Balancing\n\nTetrate’s approach to load balancing goes beyond basic Kubernetes capabilities:\n\n- **Global Load Balancing:** Tier-1 gateways enable global load balancing across multiple clusters, allowing for traffic distribution based on weighted percentages.\n- **Cross-Cluster Load Balancing:** TSB facilitates load balancing across different Kubernetes clusters, supporting multi-cluster and multi-cloud deployments.\n- **L7 Load Balancing:** TSB implements Layer 7 load balancing across one or more ingress gateways in different clusters over Istio-controlled mTLS\n\n## Cloud Provider Integration\n\nTetrate’s solutions are designed to work seamlessly with major cloud providers:\n\n- **AWS Integration:** TSB integrates with AWS services like ACM for certificate management and supports deployment on EKS.\n- **Azure Support:** We provide full support for Azure AKS, enabling seamless service mesh implementation in Azure environments.\n- **Multi-Cloud Management:** TSB offers a consistent management layer across different cloud providers, simplifying hybrid and multi-cloud deployments.\n\n## Enterprise-Ready Features\n\nTetrate Service Bridge goes beyond open-source Istio to provide enterprise-grade features:\n\n- **Simplified Configuration:** TSB offers an intuitive interface for managing complex service mesh configurations across multiple clusters.\n- **Scalability:** Our solution is built to handle enterprise-scale deployments, managing thousands of services efficiently.\n- **Compliance and Governance:** TSB helps enforce organizational policies and compliance requirements across the entire service mesh.\n- **Expert Support:** We provide enterprise-grade support and professional services to ensure successful implementation and operation of your service mesh.\n\n## Enterprise consulting and support services\n\nTetrate’s [consulting](/kubernetes-consulting) and enterprise support services provide comprehensive assistance for organizations implementing service mesh solutions across AWS, Azure, and hybrid environments.\n\nOur expert consultants offer tailored guidance on architecting, deploying, and optimizing Istio and Envoy-based service meshes, ensuring seamless integration with cloud-native technologies and existing infrastructure. We specialize in addressing the unique challenges of multi-cloud deployments, helping clients leverage the strengths of both AWS and Azure while maintaining consistent security and observability policies. Our enterprise support goes beyond traditional offerings, providing 24/7 access to service mesh experts, proactive monitoring, and rapid incident response.\n\nWhether it’s optimizing performance on AWS EKS, enhancing security on Azure AKS, or implementing cross-cloud traffic management, Tetrate’s consulting and support services enable organizations to maximize the value of their service mesh investments while minimizing operational complexity and risk.\n\nBy choosing Tetrate, organizations can confidently implement a secure, observable, and scalable microservices architecture. Our solutions simplify the complexities of service mesh technology, allowing teams to focus on delivering business value while ensuring robust security and operational excellence across their Kubernetes environments.","src/content/learn/use-case-kubernetes-architecture/index.md","5d885b22895d75d2",{html:272,metadata:273},"<p>How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like <a href=\"/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide\">AWS</a> and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and security:</p>\n<h2 id=\"use-cases\">Use Cases</h2>\n<ul>\n<li><strong>Large-Scale Microservices Management:</strong> eBay’s use case demonstrates how Istio can be used to manage traffic in a complex, multi-datacenter environment with multiple Kubernetes clusters. This showcases Istio’s ability to handle large-scale microservices architectures.</li>\n<li><strong>Zero Trust Security Implementation:</strong> Tetrate provides a consistent way to implement Zero Trust security across thousands of microservices in various environments. This is particularly crucial for organizations dealing with sensitive data or those under regulatory compliance like PCI, HIPAA, GDPR, and FIPS/FedRAMP.</li>\n<li><strong>Multi-Cloud and Hybrid Cloud Deployments:</strong> Tetrate Service Bridge (TSB) enables management of services across multiple clusters, clouds, and hybrid environments. This is valuable for organizations operating in complex, distributed infrastructures.</li>\n<li><strong>Traffic Management and Routing:</strong> Tetrate’s service mesh provides advanced traffic management capabilities, including traffic routing, splitting, and canary deployments. This is useful for organizations looking to implement sophisticated deployment strategies.</li>\n<li><strong>Observability and Troubleshooting:</strong> The service mesh offers improved visibility into service health and performance, making it easier to troubleshoot application issues and investigate security incidents.</li>\n<li><strong>Kubernetes and VM Integration:</strong> Tetrate has extended Istio’s capabilities to integrate with legacy platforms, allowing organizations to manage both modern Kubernetes microservices and traditional VM applications under a single service mesh.</li>\n<li><strong>Compliance and Policy Enforcement:</strong> Tetrate automates the deployment of security policies and enables continuous compliance monitoring, which is particularly valuable for highly regulated industries.</li>\n<li><strong>Military and Defense Applications:</strong> The U.S. Air Force has contracted Tetrate for implementing Istio-based resilient communications and enhancing DevSecOps security. This demonstrates the platform’s applicability in high-security, mission-critical environments.</li>\n<li><strong>Ambient Mode for Resource Optimization:</strong> Tetrate is implementing ambient mode for the U.S. Air Force, allowing for more efficient resource utilization based on security risk profiles.</li>\n<li><strong>Cross-Platform Compatibility:</strong> Tetrate Service Bridge supports multiple cloud platforms like Amazon EKS, Azure AKS, and Red Hat OpenShift, making it versatile for organizations with diverse infrastructure.</li>\n</ul>\n<h2 id=\"architecture\">Architecture</h2>\n<p>Tetrate Service Bridge (TSB) provides a comprehensive <a href=\"/blog/service-mesh-architecture\">service mesh</a> solution that seamlessly integrates with Kubernetes, offering:</p>\n<ul>\n<li><strong>Unified Control Plane:</strong> TSB extends Istio’s capabilities, providing a single pane of glass for managing multiple clusters across different cloud providers and on-premises environments.</li>\n<li><strong>Enhanced Proxy Model in sidecar or ambient mode:</strong> We optimize Envoy’s sidecar proxy implementation, ensuring efficient and secure service-to-service communication.</li>\n<li><strong>Multi-Cloud Support:</strong> TSB enables consistent policies and management across AWS EKS, Azure AKS, and other Kubernetes platforms, simplifying multi-cloud deployments.</li>\n</ul>\n<h2 id=\"observability\">Observability</h2>\n<p>Tetrate’s solutions significantly enhance observability in microservices environments:</p>\n<ul>\n<li><strong>Advanced Metrics Collection:</strong> TSBprovides native dashboards for in-depth performance analysis and integrates with popular tools like Prometheus.</li>\n<li><strong>Distributed Tracing:</strong> We offer seamless tracing, providing end-to-end visibility of request flows.</li>\n<li><strong>Centralized Logging:</strong> TSB aggregates and analyzes logs from across your service mesh, enabling quick troubleshooting and pattern recognition.</li>\n<li><strong>Visualization:</strong> TSB offers native network visualization and real-time metrics insights.</li>\n</ul>\n<h2 id=\"security\">Security</h2>\n<p>Tetrate puts a strong emphasis on zero trust security:</p>\n<ul>\n<li><strong>Zero Trust Implementation:</strong> TSB provides out-of-the-box zero trust security, aligning with CISA and NIST recommendations.</li>\n<li><strong>Automated mTLS:</strong> We ensure all service-to-service communication is encrypted and authenticated, with automatic certificate management.</li>\n<li><strong>Fine-grained Access Control:</strong> TSB enables detailed RBAC and ABAC policies, allowing precise control over service interactions.</li>\n<li><strong>Identity Management:</strong> We support integration with external identity providers and implement strong service identity verification.</li>\n<li><strong>Policy Enforcement:</strong> TSB integrates with Open Policy Agent (OPA) for advanced authorization policies.</li>\n</ul>\n<h2 id=\"advanced-load-balancing\">Advanced Load Balancing</h2>\n<p>Tetrate’s approach to load balancing goes beyond basic Kubernetes capabilities:</p>\n<ul>\n<li><strong>Global Load Balancing:</strong> Tier-1 gateways enable global load balancing across multiple clusters, allowing for traffic distribution based on weighted percentages.</li>\n<li><strong>Cross-Cluster Load Balancing:</strong> TSB facilitates load balancing across different Kubernetes clusters, supporting multi-cluster and multi-cloud deployments.</li>\n<li><strong>L7 Load Balancing:</strong> TSB implements Layer 7 load balancing across one or more ingress gateways in different clusters over Istio-controlled mTLS</li>\n</ul>\n<h2 id=\"cloud-provider-integration\">Cloud Provider Integration</h2>\n<p>Tetrate’s solutions are designed to work seamlessly with major cloud providers:</p>\n<ul>\n<li><strong>AWS Integration:</strong> TSB integrates with AWS services like ACM for certificate management and supports deployment on EKS.</li>\n<li><strong>Azure Support:</strong> We provide full support for Azure AKS, enabling seamless service mesh implementation in Azure environments.</li>\n<li><strong>Multi-Cloud Management:</strong> TSB offers a consistent management layer across different cloud providers, simplifying hybrid and multi-cloud deployments.</li>\n</ul>\n<h2 id=\"enterprise-ready-features\">Enterprise-Ready Features</h2>\n<p>Tetrate Service Bridge goes beyond open-source Istio to provide enterprise-grade features:</p>\n<ul>\n<li><strong>Simplified Configuration:</strong> TSB offers an intuitive interface for managing complex service mesh configurations across multiple clusters.</li>\n<li><strong>Scalability:</strong> Our solution is built to handle enterprise-scale deployments, managing thousands of services efficiently.</li>\n<li><strong>Compliance and Governance:</strong> TSB helps enforce organizational policies and compliance requirements across the entire service mesh.</li>\n<li><strong>Expert Support:</strong> We provide enterprise-grade support and professional services to ensure successful implementation and operation of your service mesh.</li>\n</ul>\n<h2 id=\"enterprise-consulting-and-support-services\">Enterprise consulting and support services</h2>\n<p>Tetrate’s <a href=\"/kubernetes-consulting\">consulting</a> and enterprise support services provide comprehensive assistance for organizations implementing service mesh solutions across AWS, Azure, and hybrid environments.</p>\n<p>Our expert consultants offer tailored guidance on architecting, deploying, and optimizing Istio and Envoy-based service meshes, ensuring seamless integration with cloud-native technologies and existing infrastructure. We specialize in addressing the unique challenges of multi-cloud deployments, helping clients leverage the strengths of both AWS and Azure while maintaining consistent security and observability policies. Our enterprise support goes beyond traditional offerings, providing 24/7 access to service mesh experts, proactive monitoring, and rapid incident response.</p>\n<p>Whether it’s optimizing performance on AWS EKS, enhancing security on Azure AKS, or implementing cross-cloud traffic management, Tetrate’s consulting and support services enable organizations to maximize the value of their service mesh investments while minimizing operational complexity and risk.</p>\n<p>By choosing Tetrate, organizations can confidently implement a secure, observable, and scalable microservices architecture. Our solutions simplify the complexities of service mesh technology, allowing teams to focus on delivering business value while ensuring robust security and operational excellence across their Kubernetes environments.</p>",{headings:274,localImagePaths:299,remoteImagePaths:300,frontmatter:301,imagePaths:304},[275,278,281,284,287,290,293,296],{depth:29,slug:276,text:277},"use-cases","Use Cases",{depth:29,slug:279,text:280},"architecture","Architecture",{depth:29,slug:282,text:283},"observability","Observability",{depth:29,slug:285,text:286},"security","Security",{depth:29,slug:288,text:289},"advanced-load-balancing","Advanced Load Balancing",{depth:29,slug:291,text:292},"cloud-provider-integration","Cloud Provider Integration",{depth:29,slug:294,text:295},"enterprise-ready-features","Enterprise-Ready Features",{depth:29,slug:297,text:298},"enterprise-consulting-and-support-services","Enterprise consulting and support services",[],[],{title:263,slug:260,date:302,description:267,categories:303,excerpt:264},["Date","2025-05-05T19:18:41.000Z"],[17],[],"use-case-kubernetes-architecture/index.md","what-are-microservices",{id:306,data:308,body:315,filePath:316,digest:317,rendered:318,legacyId:417},{title:309,excerpt:310,categories:311,date:313,description:314,draft:20},"What Are Microservices?","Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over.",[312],"What",["Date","2023-06-30T07:21:19.000Z"],"Overview Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide…","## Overview\n\nMicroservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over time.\n\n## Benefits of Microservices\n\nSome of the key advantages of a [microservices architecture include](/learn/what-are-microservices):\n\n### Scalability\n\nMicroservices make it easier to scale applications as needed, by allowing developers to add or remove services as necessary.\n\n### Agility \n\nMicroservices enable faster development and deployment of new features, as each service can be developed and tested independently.\n\n### Resilience \n\nMicroservices architecture can help ensure that applications remain resilient in the face of failures, as services can be designed to fail independently of one another.\n\n### Maintainability\n\nBy breaking applications down into smaller, more manageable services, it becomes easier to maintain and update them over time.\n\n## Challenges of a Microservices Architecture\n\nThese are some of the key challenges that need to be addressed when developing and deploying microservices applications:\n\n### Complexity\n\nMicroservices architecture can be highly complex, with numerous services communicating with each other. It is important to have effective management tools and strategies in place to ensure that the entire system remains manageable and maintainable.\n\n### Observability\n\nAs highly distributed systems, it can be challenging to understand all of the interactions between services and to identify issues as they arise. It is important to have a suite of observability tools that allow you to monitor and track the performance of individual services and the system as a whole. This can help you identify and troubleshoot issues more quickly and effectively.\n\n### Service Discovery\n\nService discovery enables services to locate and communicate with each other in a scalable and efficient manner. A robust service discovery mechanism is essential for ensuring the reliable operation of the entire system.\n\n### Security \n\nAs many services communicate with each other over networks, it is important to have a comprehensive security strategy that protects sensitive data and ensures that only authorized services can access each other.\n\n### Deployment\n\nAn efficient automated deployment pipeline is key to enjoying the agility benefits of microservices applications. Having automation and continuous deployment in place will help ensure that updates are deployed quickly, efficiently and reliably.\n\n### Configuration Management\n\nManaging configurations across multiple services can be challenging. It is important to have effective configuration management tools and strategies in place to ensure that the entire system is configured properly and consistently.\n\n### Dependency Management\n\nWith multiple services interacting with each other, it is important to manage dependencies effectively to ensure that changes to one service do not impact the entire system. It is important to have effective dependency management tools and strategies in place to minimize the risk of such issues.\n\n### Resilience and Fault Tolerance\n\nFor applications with multiple dynamically deployed instances and many services, having mechanisms in place to ensure that the system remains available and responsive in the face of service failures or other issues is important. This can involve implementing resilience and fault tolerance mechanisms, such as circuit breaking and retries.\n\n## When Should You Use Microservices?\n\nMicroservices architecture can be a good fit for large, complex applications that require high scalability, flexibility and agility. Here are some scenarios where microservices architecture may be appropriate.\n\n### Large, Complex Applications\n\nMicroservices architecture is ideal for large, complex applications that require high scalability and agility. This approach can help you break down the application into smaller, more manageable components, making it easier to develop, test and deploy.\n\n### Rapid Iteration\n\nMicroservices architecture can help you develop and deploy new features more quickly and with less risk. Since each service is developed and deployed independently, it is easier to test and deploy new features without affecting the entire application.\n\n### High Traffic And Peak Loads\n\nIf your application experiences high traffic and peak loads, microservices architecture can help you scale individual components independently. This can help ensure that your application remains responsive and available even during high traffic periods.\n\n### Multiple Development Teams\n\nMicroservices architecture can help you divide the application into smaller, more manageable components, making it easier for multiple development teams to work on different services simultaneously. This can help speed up development times and reduce time-to-market.\n\n### Polyglot Development\n\nMicroservices architecture allows you to use different programming languages and technologies for different services. This can be useful if you have a diverse development team with different skill sets and preferences.\n\nMicroservices architecture can be a good fit for applications that require high scalability, agility and flexibility. However, it is important to carefully consider the complexity and potential challenges of this approach before adopting it.\n\n## Microservices vs Monolithic Architecture\n\nMicroservices and monolithic architectures are two different software development approaches that have their own advantages and disadvantages. Here are some key differences between the two:\n\n### Architecture\n\nIn a monolithic architecture, the entire application is built as a single, self-contained unit. In contrast, microservices applications are broken down into smaller, independent services that communicate with each other.\n\n### Development\n\nDevelopment of monolithic applications can be slower and more cumbersome, as any changes made to the application require the entire application to be rebuilt and redeployed. On the other hand, microservices development can be faster and more agile, as developers can work on different services simultaneously.\n\n### Deployment\n\nMonolithic applications are deployed as a single unit. This can make it challenging to deploy new features or updates without affecting the entire system. With microservices, each service can be deployed independently, which makes it easier to test and deploy new features without affecting the entire application.\n\n### Maintenance\n\nMaintenance and updates to monoliths can be more challenging, as any changes made to the application can affect the entire system. In contrast, maintenance and updates of microservices can be easier to manage, as each service can be updated independently without affecting the entire application.\n\n### Scalability\n\nScaling a monolithic application requires scaling the entire system. In contrast, with microservices, each service can be scaled independently, which means that you can allocate resources where they are needed most.\n\nMicroservices architecture can offer some advantages over monolithic architecture, including increased scalability, flexibility and faster development times. However, it also presents some unique challenges, including increased complexity. It’s important to carefully consider the advantages and disadvantages of both approaches before choosing the one that is right for your application.\n\n## Role of Service Mesh in Microservices Applications\n\nA service mesh is a dedicated infrastructure layer that handles communication between microservices. The service mesh provides a number of important functions, including:\n\n### Traffic Management\n\nA service mesh can handle traffic routing, load balancing and service discovery between services within the application. This can help ensure that traffic is directed to the appropriate service and can help improve application performance and reliability.\n\n### Service Discovery\n\nMicroservices scalability and high availability is typically accomplished by dynamically scaling up the number of service instances in response to increased load or service outage and scaling back down when needed to save costs. A service mesh keeps track of all available service instances, automatically routing requests to them. When service instances become unavailable due to down-scaling, maintenance, or failure, service discovery removes them from the list of available instances and the mesh routes new requests to remaining available instances.\n\n### Security\n\nA service mesh can provide security features such as service authentication, encryption and access control. This can help ensure that sensitive data is protected and that only authorized services can communicate with each other.\n\n### Observability \n\nA service mesh can provide monitoring and logging capabilities that allow you to track the health and performance of individual services within the application. This can help you identify and troubleshoot issues more quickly and effectively.\n\n### Resilience\n\nA service mesh can provide features such as circuit breaking and retries that can help ensure that the application remains available and responsive even during periods of high traffic or service failures.\n\nLearn more about Istio, the most widely-deployed service mesh ›","src/content/learn/what-are-microservices/index.md","97a204d6a64e0091",{html:319,metadata:320},"<h2 id=\"overview\">Overview</h2>\n<p>Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over time.</p>\n<h2 id=\"benefits-of-microservices\">Benefits of Microservices</h2>\n<p>Some of the key advantages of a <a href=\"/learn/what-are-microservices\">microservices architecture include</a>:</p>\n<h3 id=\"scalability\">Scalability</h3>\n<p>Microservices make it easier to scale applications as needed, by allowing developers to add or remove services as necessary.</p>\n<h3 id=\"agility\">Agility </h3>\n<p>Microservices enable faster development and deployment of new features, as each service can be developed and tested independently.</p>\n<h3 id=\"resilience\">Resilience </h3>\n<p>Microservices architecture can help ensure that applications remain resilient in the face of failures, as services can be designed to fail independently of one another.</p>\n<h3 id=\"maintainability\">Maintainability</h3>\n<p>By breaking applications down into smaller, more manageable services, it becomes easier to maintain and update them over time.</p>\n<h2 id=\"challenges-of-a-microservices-architecture\">Challenges of a Microservices Architecture</h2>\n<p>These are some of the key challenges that need to be addressed when developing and deploying microservices applications:</p>\n<h3 id=\"complexity\">Complexity</h3>\n<p>Microservices architecture can be highly complex, with numerous services communicating with each other. It is important to have effective management tools and strategies in place to ensure that the entire system remains manageable and maintainable.</p>\n<h3 id=\"observability\">Observability</h3>\n<p>As highly distributed systems, it can be challenging to understand all of the interactions between services and to identify issues as they arise. It is important to have a suite of observability tools that allow you to monitor and track the performance of individual services and the system as a whole. This can help you identify and troubleshoot issues more quickly and effectively.</p>\n<h3 id=\"service-discovery\">Service Discovery</h3>\n<p>Service discovery enables services to locate and communicate with each other in a scalable and efficient manner. A robust service discovery mechanism is essential for ensuring the reliable operation of the entire system.</p>\n<h3 id=\"security\">Security </h3>\n<p>As many services communicate with each other over networks, it is important to have a comprehensive security strategy that protects sensitive data and ensures that only authorized services can access each other.</p>\n<h3 id=\"deployment\">Deployment</h3>\n<p>An efficient automated deployment pipeline is key to enjoying the agility benefits of microservices applications. Having automation and continuous deployment in place will help ensure that updates are deployed quickly, efficiently and reliably.</p>\n<h3 id=\"configuration-management\">Configuration Management</h3>\n<p>Managing configurations across multiple services can be challenging. It is important to have effective configuration management tools and strategies in place to ensure that the entire system is configured properly and consistently.</p>\n<h3 id=\"dependency-management\">Dependency Management</h3>\n<p>With multiple services interacting with each other, it is important to manage dependencies effectively to ensure that changes to one service do not impact the entire system. It is important to have effective dependency management tools and strategies in place to minimize the risk of such issues.</p>\n<h3 id=\"resilience-and-fault-tolerance\">Resilience and Fault Tolerance</h3>\n<p>For applications with multiple dynamically deployed instances and many services, having mechanisms in place to ensure that the system remains available and responsive in the face of service failures or other issues is important. This can involve implementing resilience and fault tolerance mechanisms, such as circuit breaking and retries.</p>\n<h2 id=\"when-should-you-use-microservices\">When Should You Use Microservices?</h2>\n<p>Microservices architecture can be a good fit for large, complex applications that require high scalability, flexibility and agility. Here are some scenarios where microservices architecture may be appropriate.</p>\n<h3 id=\"large-complex-applications\">Large, Complex Applications</h3>\n<p>Microservices architecture is ideal for large, complex applications that require high scalability and agility. This approach can help you break down the application into smaller, more manageable components, making it easier to develop, test and deploy.</p>\n<h3 id=\"rapid-iteration\">Rapid Iteration</h3>\n<p>Microservices architecture can help you develop and deploy new features more quickly and with less risk. Since each service is developed and deployed independently, it is easier to test and deploy new features without affecting the entire application.</p>\n<h3 id=\"high-traffic-and-peak-loads\">High Traffic And Peak Loads</h3>\n<p>If your application experiences high traffic and peak loads, microservices architecture can help you scale individual components independently. This can help ensure that your application remains responsive and available even during high traffic periods.</p>\n<h3 id=\"multiple-development-teams\">Multiple Development Teams</h3>\n<p>Microservices architecture can help you divide the application into smaller, more manageable components, making it easier for multiple development teams to work on different services simultaneously. This can help speed up development times and reduce time-to-market.</p>\n<h3 id=\"polyglot-development\">Polyglot Development</h3>\n<p>Microservices architecture allows you to use different programming languages and technologies for different services. This can be useful if you have a diverse development team with different skill sets and preferences.</p>\n<p>Microservices architecture can be a good fit for applications that require high scalability, agility and flexibility. However, it is important to carefully consider the complexity and potential challenges of this approach before adopting it.</p>\n<h2 id=\"microservices-vs-monolithic-architecture\">Microservices vs Monolithic Architecture</h2>\n<p>Microservices and monolithic architectures are two different software development approaches that have their own advantages and disadvantages. Here are some key differences between the two:</p>\n<h3 id=\"architecture\">Architecture</h3>\n<p>In a monolithic architecture, the entire application is built as a single, self-contained unit. In contrast, microservices applications are broken down into smaller, independent services that communicate with each other.</p>\n<h3 id=\"development\">Development</h3>\n<p>Development of monolithic applications can be slower and more cumbersome, as any changes made to the application require the entire application to be rebuilt and redeployed. On the other hand, microservices development can be faster and more agile, as developers can work on different services simultaneously.</p>\n<h3 id=\"deployment-1\">Deployment</h3>\n<p>Monolithic applications are deployed as a single unit. This can make it challenging to deploy new features or updates without affecting the entire system. With microservices, each service can be deployed independently, which makes it easier to test and deploy new features without affecting the entire application.</p>\n<h3 id=\"maintenance\">Maintenance</h3>\n<p>Maintenance and updates to monoliths can be more challenging, as any changes made to the application can affect the entire system. In contrast, maintenance and updates of microservices can be easier to manage, as each service can be updated independently without affecting the entire application.</p>\n<h3 id=\"scalability-1\">Scalability</h3>\n<p>Scaling a monolithic application requires scaling the entire system. In contrast, with microservices, each service can be scaled independently, which means that you can allocate resources where they are needed most.</p>\n<p>Microservices architecture can offer some advantages over monolithic architecture, including increased scalability, flexibility and faster development times. However, it also presents some unique challenges, including increased complexity. It’s important to carefully consider the advantages and disadvantages of both approaches before choosing the one that is right for your application.</p>\n<h2 id=\"role-of-service-mesh-in-microservices-applications\">Role of Service Mesh in Microservices Applications</h2>\n<p>A service mesh is a dedicated infrastructure layer that handles communication between microservices. The service mesh provides a number of important functions, including:</p>\n<h3 id=\"traffic-management\">Traffic Management</h3>\n<p>A service mesh can handle traffic routing, load balancing and service discovery between services within the application. This can help ensure that traffic is directed to the appropriate service and can help improve application performance and reliability.</p>\n<h3 id=\"service-discovery-1\">Service Discovery</h3>\n<p>Microservices scalability and high availability is typically accomplished by dynamically scaling up the number of service instances in response to increased load or service outage and scaling back down when needed to save costs. A service mesh keeps track of all available service instances, automatically routing requests to them. When service instances become unavailable due to down-scaling, maintenance, or failure, service discovery removes them from the list of available instances and the mesh routes new requests to remaining available instances.</p>\n<h3 id=\"security-1\">Security</h3>\n<p>A service mesh can provide security features such as service authentication, encryption and access control. This can help ensure that sensitive data is protected and that only authorized services can communicate with each other.</p>\n<h3 id=\"observability-1\">Observability </h3>\n<p>A service mesh can provide monitoring and logging capabilities that allow you to track the health and performance of individual services within the application. This can help you identify and troubleshoot issues more quickly and effectively.</p>\n<h3 id=\"resilience-1\">Resilience</h3>\n<p>A service mesh can provide features such as circuit breaking and retries that can help ensure that the application remains available and responsive even during periods of high traffic or service failures.</p>\n<p>Learn more about Istio, the most widely-deployed service mesh ›</p>",{headings:321,localImagePaths:411,remoteImagePaths:412,frontmatter:413,imagePaths:416},[322,323,326,330,333,336,339,342,345,346,349,351,354,357,360,363,366,369,372,375,378,381,384,385,388,390,393,395,398,401,403,405,408],{depth:29,slug:158,text:159},{depth:29,slug:324,text:325},"benefits-of-microservices","Benefits of Microservices",{depth:327,slug:328,text:329},3,"scalability","Scalability",{depth:327,slug:331,text:332},"agility","Agility ",{depth:327,slug:334,text:335},"resilience","Resilience ",{depth:327,slug:337,text:338},"maintainability","Maintainability",{depth:29,slug:340,text:341},"challenges-of-a-microservices-architecture","Challenges of a Microservices Architecture",{depth:327,slug:343,text:344},"complexity","Complexity",{depth:327,slug:282,text:283},{depth:327,slug:347,text:348},"service-discovery","Service Discovery",{depth:327,slug:285,text:350},"Security ",{depth:327,slug:352,text:353},"deployment","Deployment",{depth:327,slug:355,text:356},"configuration-management","Configuration Management",{depth:327,slug:358,text:359},"dependency-management","Dependency Management",{depth:327,slug:361,text:362},"resilience-and-fault-tolerance","Resilience and Fault Tolerance",{depth:29,slug:364,text:365},"when-should-you-use-microservices","When Should You Use Microservices?",{depth:327,slug:367,text:368},"large-complex-applications","Large, Complex Applications",{depth:327,slug:370,text:371},"rapid-iteration","Rapid Iteration",{depth:327,slug:373,text:374},"high-traffic-and-peak-loads","High Traffic And Peak Loads",{depth:327,slug:376,text:377},"multiple-development-teams","Multiple Development Teams",{depth:327,slug:379,text:380},"polyglot-development","Polyglot Development",{depth:29,slug:382,text:383},"microservices-vs-monolithic-architecture","Microservices vs Monolithic Architecture",{depth:327,slug:279,text:280},{depth:327,slug:386,text:387},"development","Development",{depth:327,slug:389,text:353},"deployment-1",{depth:327,slug:391,text:392},"maintenance","Maintenance",{depth:327,slug:394,text:329},"scalability-1",{depth:29,slug:396,text:397},"role-of-service-mesh-in-microservices-applications","Role of Service Mesh in Microservices Applications",{depth:327,slug:399,text:400},"traffic-management","Traffic Management",{depth:327,slug:402,text:348},"service-discovery-1",{depth:327,slug:404,text:286},"security-1",{depth:327,slug:406,text:407},"observability-1","Observability ",{depth:327,slug:409,text:410},"resilience-1","Resilience",[],[],{title:309,slug:306,date:414,description:314,categories:415,excerpt:310},["Date","2023-06-30T07:21:19.000Z"],[312],[],"what-are-microservices/index.md","what-is-an-api-gateway",{id:418,data:420,body:427,filePath:428,digest:429,rendered:430,legacyId:450},{title:421,excerpt:422,featuredImage:423,categories:424,date:425,description:426,draft:20},"What Is an API Gateway?","Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different.","https://lh5.googleusercontent.com/WSns3Ifou8w8ifxl3PTBGIvSP6UevjPdQW0PoT0zZdbwyouY1JFvYRB2Pm6Zx0eHygBK8IkkyKr6gtc4c_K56HJfMhACIWOFF4t20WpuJOof8fB7Zr4d9xeYEBQvefKRvTBfFLSOFUC05-maH_fpNTc",[312],["Date","2024-07-19T01:05:00.000Z"],"Overview Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications…","## Overview\n\nApplication Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different protocols.\n\n[An API gateway](/learn/what-is-an-api-gateway) is a software layer that sits between API clients and the services implementing the APIs they consume. It acts as a unified entry point for all API requests and provides several functionalities, such as request routing, security, rate limiting, and API versioning. It can also perform additional functions such as protocol translation and message transformation.\n\nIn addition, API gateways provide a layer of abstraction between client applications and backend services. This means client applications do not need to know the internal workings of the backend services, or even the API itself. Instead, they can rely on the API gateway to handle requests and responses.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. [Get access now ›](/demo-request)\n\nSimplifying the management of complex microservice-based architectures by providing a single point of entry for clients and allowing for the addition or removal of services without affecting clients is also a key benefit of API gateways . They can also improve security by handling authentication and access control for all services in a consistent and centralized way.\n\nallows single-IP-port to access all services running in k8s through ingress rules. The Ingress Controller service is set to load balancer to be accessible from the public internet.\n\nAn Ingress Controller is a Layer 4 and Layer 7 proxy that routes traffic from clients to the services deployed into Kubernetes. Like an API gateway, an Ingress Controller can manage traffic and provide visibility, troubleshooting, security, and identity. An Ingress Controller is limited to only Kubernetes services, while an API gateway can manage traffic for both Kubernetes and VM workloads. Envoy proxy is used by some popular Ingress Controllers such as Ambassador and Contour. Other tools that are widely used as Ingress controllers include Kong Ingress, HAProxy Ingress, NGINX Ingress, etc.\n\n## API Gateway vs the Kubernetes Gateway API\n\nThe Gateway API is a built-in Kubernetes API that represents a superset of Kubernetes Ingress and  provides a standardized way to manage and configure inbound traffic in Kubernetes environments.\n\nAn API gateway is an API management component that provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the API endpoints of an application.\n\nImplementations of Gateway API, such as [Envoy Gateway](/learn/what-is-an-api-gateway), the  Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API gateway capabilities.\n\n## API Gateway vs Service Mesh\n\nAPI gateway and service mesh are two different architectural patterns that can be used in modern application architectures that use microservices.\n\nAn API gateway is a software layer that sits between API clients and the APIs they consume. Its primary function is to act as an entry point for all API requests and provide several functionalities, such as request routing, security, rate limiting, and API versioning. Its focus is on managing the traffic between the API clients and the backend services that provide the APIs.\n\nService mesh, on the other hand, is a dedicated infrastructure layer that provides several functionalities, such as service discovery, load balancing, traffic management, security, and observability. Its focus is on managing the communication between the microservices within an application architecture, providing a reliable and scalable infrastructure layer for microservices.\n\nWhile API gateway and service mesh are different architectural patterns, they can complement each other in modern application architectures. An API gateway can be used as an entry point for external API requests, while a service mesh can be used to manage the communication between microservices within an application architecture.\n\nAPI gateway and service mesh can also share some functionalities, such as traffic management and security. For example, a service mesh can be used to manage the traffic between microservices within an application architecture, while an API gateway can be used to manage the traffic between the external API clients and the backend services that provide the APIs.\n\nA service mesh handles traffic flowing from external clients into an application and communication between services. A service mesh can drive both north-south traffic (i.e., among services in a data center) and east-west traffic (services between various data centers).\n\nIstio is the most widely-deployed service mesh. The figure below highlights how Istio handles the communication flow among various microservices (including Kubernetes and VMs):\n\nis an open source project that can be easily used as an API gateway. It is based on the Gateway API—a resource used for service networking in Kubernetes. This means when users create Gateway API resources in Kubernetes cluster, they will be translated into native Envoy API calls, so Envoy and xDS, its native API, will not need to be changed to add this new support.\n\n### Operational Benefits of Envoy Gateway\n\n- **App developers** can use Envoy Gateway to route external traffic to their application easily, without needing to build or extend control planes to manage traffic.\n- **Infrastructure teams** can get basic gateway functionality quickly with Envoy Gateway. They can provide Envoy-native experience to the application team without purchasing a vendor solution.\n\nEnvoy Gateway makes it easy for platform architects, infrastructure administrators, and developers to quickly adopt an Envoy-based API gateway. [Learn more about getting started with Envoy Gateway ›](/learn/what-is-envoy-gateway)","src/content/learn/what-is-an-api-gateway/index.md","b7e8a91368405656",{html:431,metadata:432},"<h2 id=\"overview\">Overview</h2>\n<p>Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different protocols.</p>\n<p><a href=\"/learn/what-is-an-api-gateway\">An API gateway</a> is a software layer that sits between API clients and the services implementing the APIs they consume. It acts as a unified entry point for all API requests and provides several functionalities, such as request routing, security, rate limiting, and API versioning. It can also perform additional functions such as protocol translation and message transformation.</p>\n<p>In addition, API gateways provide a layer of abstraction between client applications and backend services. This means client applications do not need to know the internal workings of the backend services, or even the API itself. Instead, they can rely on the API gateway to handle requests and responses.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<p>Simplifying the management of complex microservice-based architectures by providing a single point of entry for clients and allowing for the addition or removal of services without affecting clients is also a key benefit of API gateways . They can also improve security by handling authentication and access control for all services in a consistent and centralized way.</p>\n<p>allows single-IP-port to access all services running in k8s through ingress rules. The Ingress Controller service is set to load balancer to be accessible from the public internet.</p>\n<p>An Ingress Controller is a Layer 4 and Layer 7 proxy that routes traffic from clients to the services deployed into Kubernetes. Like an API gateway, an Ingress Controller can manage traffic and provide visibility, troubleshooting, security, and identity. An Ingress Controller is limited to only Kubernetes services, while an API gateway can manage traffic for both Kubernetes and VM workloads. Envoy proxy is used by some popular Ingress Controllers such as Ambassador and Contour. Other tools that are widely used as Ingress controllers include Kong Ingress, HAProxy Ingress, NGINX Ingress, etc.</p>\n<h2 id=\"api-gateway-vs-the-kubernetes-gateway-api\">API Gateway vs the Kubernetes Gateway API</h2>\n<p>The Gateway API is a built-in Kubernetes API that represents a superset of Kubernetes Ingress and  provides a standardized way to manage and configure inbound traffic in Kubernetes environments.</p>\n<p>An API gateway is an API management component that provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the API endpoints of an application.</p>\n<p>Implementations of Gateway API, such as <a href=\"/learn/what-is-an-api-gateway\">Envoy Gateway</a>, the  Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API gateway capabilities.</p>\n<h2 id=\"api-gateway-vs-service-mesh\">API Gateway vs Service Mesh</h2>\n<p>API gateway and service mesh are two different architectural patterns that can be used in modern application architectures that use microservices.</p>\n<p>An API gateway is a software layer that sits between API clients and the APIs they consume. Its primary function is to act as an entry point for all API requests and provide several functionalities, such as request routing, security, rate limiting, and API versioning. Its focus is on managing the traffic between the API clients and the backend services that provide the APIs.</p>\n<p>Service mesh, on the other hand, is a dedicated infrastructure layer that provides several functionalities, such as service discovery, load balancing, traffic management, security, and observability. Its focus is on managing the communication between the microservices within an application architecture, providing a reliable and scalable infrastructure layer for microservices.</p>\n<p>While API gateway and service mesh are different architectural patterns, they can complement each other in modern application architectures. An API gateway can be used as an entry point for external API requests, while a service mesh can be used to manage the communication between microservices within an application architecture.</p>\n<p>API gateway and service mesh can also share some functionalities, such as traffic management and security. For example, a service mesh can be used to manage the traffic between microservices within an application architecture, while an API gateway can be used to manage the traffic between the external API clients and the backend services that provide the APIs.</p>\n<p>A service mesh handles traffic flowing from external clients into an application and communication between services. A service mesh can drive both north-south traffic (i.e., among services in a data center) and east-west traffic (services between various data centers).</p>\n<p>Istio is the most widely-deployed service mesh. The figure below highlights how Istio handles the communication flow among various microservices (including Kubernetes and VMs):</p>\n<p>is an open source project that can be easily used as an API gateway. It is based on the Gateway API—a resource used for service networking in Kubernetes. This means when users create Gateway API resources in Kubernetes cluster, they will be translated into native Envoy API calls, so Envoy and xDS, its native API, will not need to be changed to add this new support.</p>\n<h3 id=\"operational-benefits-of-envoy-gateway\">Operational Benefits of Envoy Gateway</h3>\n<ul>\n<li><strong>App developers</strong> can use Envoy Gateway to route external traffic to their application easily, without needing to build or extend control planes to manage traffic.</li>\n<li><strong>Infrastructure teams</strong> can get basic gateway functionality quickly with Envoy Gateway. They can provide Envoy-native experience to the application team without purchasing a vendor solution.</li>\n</ul>\n<p>Envoy Gateway makes it easy for platform architects, infrastructure administrators, and developers to quickly adopt an Envoy-based API gateway. <a href=\"/learn/what-is-envoy-gateway\">Learn more about getting started with Envoy Gateway ›</a></p>",{headings:433,localImagePaths:444,remoteImagePaths:445,frontmatter:446,imagePaths:449},[434,435,438,441],{depth:29,slug:158,text:159},{depth:29,slug:436,text:437},"api-gateway-vs-the-kubernetes-gateway-api","API Gateway vs the Kubernetes Gateway API",{depth:29,slug:439,text:440},"api-gateway-vs-service-mesh","API Gateway vs Service Mesh",{depth:327,slug:442,text:443},"operational-benefits-of-envoy-gateway","Operational Benefits of Envoy Gateway",[],[],{title:421,slug:418,date:447,description:426,featuredImage:423,categories:448,excerpt:422},["Date","2024-07-19T01:05:00.000Z"],[312],[],"what-is-an-api-gateway/index.md","kubernetes-traffic-routing-and-control",{id:451,data:453,body:459,filePath:460,digest:461,rendered:462,legacyId:475},{title:454,excerpt:455,categories:456,date:457,description:458,draft:20},"Kubernetes Traffic Routing and Control","Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include Services, Ingress Controllers, and Service Meshes. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and.",[17],["Date","2025-04-16T11:18:04.000Z"],"Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes…","Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include **Services**, **Ingress Controllers**, and **Service Meshes**. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and stability.\n\nThe core component of Kubernetes traffic management is the **Kubernetes Service**. It creates a stable endpoint to direct network traffic to pods. In more complex environments, **Ingress** and **Service Meshes** like Istio add layers of control. Ingress manages external access, while service meshes offer more sophisticated routing features, like retries, failover, and load balancing. It is important to note that Kubernetes Ingress gateways can use Kubernetes’ Ingress resource and Ingress controllers, but this approach is no longer recommended. Instead, using the **Kubernetes Gateway API** with **Gateway** resources is now the preferred method. Effective routing strategies ensure reduced latency, improved scalability, and a better overall user experience.\n\n## Kubernetes Ingress and Egress Networking: Architecture Simplified\n\nKubernetes clusters must handle both incoming (ingress) and outgoing (egress) traffic to communicate effectively with the outside world. **Ingress** in Kubernetes is managed using **Ingress Controllers**, which map external HTTP/HTTPS requests to internal services. These controllers are essential for directing user traffic from outside the cluster, often used for load balancing and exposing services. It is now recommended to use the **Kubernetes Gateway API** and **Gateway** resources instead of traditional Ingress resources for better control and scalability. Choosing the right Ingress Controller, whether it’s NGINX, Traefik, or Envoy Gateway, can significantly impact traffic flow and scalability.\n\n**Egress** handles outbound communication from pods to external resources, like APIs or databases. Configuring **Network Policies** and Egress Gateways is key for security and performance when accessing external networks. Egress also depends on controllers deployed in Kubernetes to manage outbound traffic. These components ensure that only authorized services can communicate outside of the cluster, adding an additional layer of control to the architecture.\n\nIf you need to [implement both Ingress and Egress gateway control](/learn/ingress-and-egress-architecture), it is recommended to use **Istio**. Istio provides a comprehensive solution for managing both incoming and outgoing traffic, with enhanced features for security, observability, and traffic control.\n\nTogether, Ingress and Egress form a comprehensive architecture that secures and optimizes the way Kubernetes clusters interact with both internal and external environments. Proper planning of ingress and egress strategies will improve reliability, security, and the efficiency of your Kubernetes applications.","src/content/learn/kubernetes-traffic-routing-and-control/index.md","432c995710982592",{html:463,metadata:464},"<p>Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include <strong>Services</strong>, <strong>Ingress Controllers</strong>, and <strong>Service Meshes</strong>. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and stability.</p>\n<p>The core component of Kubernetes traffic management is the <strong>Kubernetes Service</strong>. It creates a stable endpoint to direct network traffic to pods. In more complex environments, <strong>Ingress</strong> and <strong>Service Meshes</strong> like Istio add layers of control. Ingress manages external access, while service meshes offer more sophisticated routing features, like retries, failover, and load balancing. It is important to note that Kubernetes Ingress gateways can use Kubernetes’ Ingress resource and Ingress controllers, but this approach is no longer recommended. Instead, using the <strong>Kubernetes Gateway API</strong> with <strong>Gateway</strong> resources is now the preferred method. Effective routing strategies ensure reduced latency, improved scalability, and a better overall user experience.</p>\n<h2 id=\"kubernetes-ingress-and-egress-networking-architecture-simplified\">Kubernetes Ingress and Egress Networking: Architecture Simplified</h2>\n<p>Kubernetes clusters must handle both incoming (ingress) and outgoing (egress) traffic to communicate effectively with the outside world. <strong>Ingress</strong> in Kubernetes is managed using <strong>Ingress Controllers</strong>, which map external HTTP/HTTPS requests to internal services. These controllers are essential for directing user traffic from outside the cluster, often used for load balancing and exposing services. It is now recommended to use the <strong>Kubernetes Gateway API</strong> and <strong>Gateway</strong> resources instead of traditional Ingress resources for better control and scalability. Choosing the right Ingress Controller, whether it’s NGINX, Traefik, or Envoy Gateway, can significantly impact traffic flow and scalability.</p>\n<p><strong>Egress</strong> handles outbound communication from pods to external resources, like APIs or databases. Configuring <strong>Network Policies</strong> and Egress Gateways is key for security and performance when accessing external networks. Egress also depends on controllers deployed in Kubernetes to manage outbound traffic. These components ensure that only authorized services can communicate outside of the cluster, adding an additional layer of control to the architecture.</p>\n<p>If you need to <a href=\"/learn/ingress-and-egress-architecture\">implement both Ingress and Egress gateway control</a>, it is recommended to use <strong>Istio</strong>. Istio provides a comprehensive solution for managing both incoming and outgoing traffic, with enhanced features for security, observability, and traffic control.</p>\n<p>Together, Ingress and Egress form a comprehensive architecture that secures and optimizes the way Kubernetes clusters interact with both internal and external environments. Proper planning of ingress and egress strategies will improve reliability, security, and the efficiency of your Kubernetes applications.</p>",{headings:465,localImagePaths:469,remoteImagePaths:470,frontmatter:471,imagePaths:474},[466],{depth:29,slug:467,text:468},"kubernetes-ingress-and-egress-networking-architecture-simplified","Kubernetes Ingress and Egress Networking: Architecture Simplified",[],[],{title:454,slug:451,date:472,description:458,categories:473,excerpt:455},["Date","2025-04-16T11:18:04.000Z"],[17],[],"kubernetes-traffic-routing-and-control/index.md","what-does-kubernetes-do",{id:476,data:478,body:484,filePath:485,digest:486,rendered:487,legacyId:512},{title:479,excerpt:480,categories:481,date:482,description:483,draft:20},"What Does Kubernetes Do?","Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized.",[312],["Date","2025-05-05T18:06:35.000Z"],"Containerization Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized…","## Containerization\n\nKubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized applications.\n\nContainerization is a popular method for packaging applications for deployment. Software containers are self-contained and include everything an application needs to run in a single package called a container.\n\nDue to the benefits, containers have become a popular deployment option for development teams. Docker is one of the most widely used container platforms and runtime engines, but others exist, such as CRI-O, Podman, and more.\n\nThe ease of creating containers and their growing popularity has led to a management challenge for many organizations. With a large number of containers deployed in DevSecOps and Production environments, container sprawl has become a problem, similar to virtual server sprawl in virtualization platforms. The open-source [Kubernetes](/learn/what-does-kubernetes-do) platform has become a popular solution for managing containers and controlling sprawl.\n\n## Container Orchestration\n\nContainer orchestration is an essential process for IT Operations Teams when managing large numbers of containers to ensure that deployed applications are available and performing well 24×7. This is especially true for microservices-based applications that use many separately deployed modules working in unison across containers. The Kubernetes platform provides the tools to manage container deployments.\n\nKubernetes has gained popularity with developers and DevSecOps teams due to its features, toolset, and support from major cloud service providers. Kubernetes has transformed application deployment by delivering enhanced container management for organizations.\n\n## Kubernetes Components\n\nKubernetes groups software containers into pods, the smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, networking, and a configuration specification outlining how the containers should run.\n\nKubernetes has multiple components to deliver container orchestration:\n\n**Clusters** – The Kubernetes architecture uses clusters consisting of multiple worker nodes (see below) for running and managing containers and a Control Plane node for controlling and monitoring the worker nodes.\n\n**Nodes** – A compute host that can be a physical, virtual, or cloud instance. These nodes exist within a cluster, where they can act as workers or a Control Plane. The worker nodes are responsible for hosting and running the deployed containers. In contrast, the Control Plane node in each cluster manages the worker nodes in the same cluster. Each worker node runs an agent, which the master node uses to monitor and manage it.\n\n**Pods** – Groups of containers that share compute resources and a network. Kubernetes scales at the pod level. If additional capacity is needed to support an application running on containers in a pod, then Kubernetes can replicate the complete pod to add capacity via additional containers.\n\n**Deployments** – Kubernetes Deployments control the creation and deployment of containerized applications. They use declarative configuration files that specify the desired state of an application and its containers. The deployment file also specifies how many replicas of a pod should run on a cluster. Deployments monitor running container pods, and if a pod fails, they recreate it.\n\n## Kubernetes Services\n\nWithin these components, Kubernetes provides the following services and functionality:\n\n**Control Plane-Node Architecture** – Kubernetes uses a Control Plane-Node architecture. A Control Plane node is responsible for managing the state of the cluster. It schedules pods to run on nodes, maintains the desired state of the application, scales applications, and rolls out new updates.\n\n**Rollouts** – These describe the target container landscape needed for an application and let Kubernetes handle the process deployment to get there. This includes new deployments, changing existing deployed containers, and rollbacks to remove obsolete deployments. The Kubernetes rollout scheduler automatically places containers in the available infrastructure based on their resource requirements.\n\n**Self-healing** – This monitors containers for issues and restarts them automatically if necessary. The restart can be within the same pod or on another pod.\n\n**Service discovery** – Automatically expose a container to the broader network or other containers via a DNS name or an IP Address.\n\n**Load Balancing** – Manage the load across multiple containers delivering the same application to ensure consistent performance.\n\n**Storage orchestration** – Mount storage from the cloud or local resources as needed and for as long as necessary. Applications can request the storage they need, and the Kubernetes platform will allocate it dynamically from local or cloud storage resources without system administrators needing to be involved.\n\n**Secret and configuration management** – Securely manage sensitive information like passwords, tokens, and keys. Deploy and update secrets and app configurations without exposing secrets on the network.\n\n## Conclusion\n\nIn summary, Kubernetes provides the tools to automate the deployment, scaling, and management of application container clusters and pods across hosts. By abstracting the hardware infrastructure with a controlling software platform, Kubernetes presents the available IT infrastructure as a single deployment platform.\n\nKubernetes promotes a more collaborative approach between developers and operations teams, enabling them to work together more effectively and encouraging a more DevSecOps-centric approach to application deployment.","src/content/learn/what-does-kubernetes-do/index.md","5697f2801d65ecb2",{html:488,metadata:489},"<h2 id=\"containerization\">Containerization</h2>\n<p>Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized applications.</p>\n<p>Containerization is a popular method for packaging applications for deployment. Software containers are self-contained and include everything an application needs to run in a single package called a container.</p>\n<p>Due to the benefits, containers have become a popular deployment option for development teams. Docker is one of the most widely used container platforms and runtime engines, but others exist, such as CRI-O, Podman, and more.</p>\n<p>The ease of creating containers and their growing popularity has led to a management challenge for many organizations. With a large number of containers deployed in DevSecOps and Production environments, container sprawl has become a problem, similar to virtual server sprawl in virtualization platforms. The open-source <a href=\"/learn/what-does-kubernetes-do\">Kubernetes</a> platform has become a popular solution for managing containers and controlling sprawl.</p>\n<h2 id=\"container-orchestration\">Container Orchestration</h2>\n<p>Container orchestration is an essential process for IT Operations Teams when managing large numbers of containers to ensure that deployed applications are available and performing well 24×7. This is especially true for microservices-based applications that use many separately deployed modules working in unison across containers. The Kubernetes platform provides the tools to manage container deployments.</p>\n<p>Kubernetes has gained popularity with developers and DevSecOps teams due to its features, toolset, and support from major cloud service providers. Kubernetes has transformed application deployment by delivering enhanced container management for organizations.</p>\n<h2 id=\"kubernetes-components\">Kubernetes Components</h2>\n<p>Kubernetes groups software containers into pods, the smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, networking, and a configuration specification outlining how the containers should run.</p>\n<p>Kubernetes has multiple components to deliver container orchestration:</p>\n<p><strong>Clusters</strong> – The Kubernetes architecture uses clusters consisting of multiple worker nodes (see below) for running and managing containers and a Control Plane node for controlling and monitoring the worker nodes.</p>\n<p><strong>Nodes</strong> – A compute host that can be a physical, virtual, or cloud instance. These nodes exist within a cluster, where they can act as workers or a Control Plane. The worker nodes are responsible for hosting and running the deployed containers. In contrast, the Control Plane node in each cluster manages the worker nodes in the same cluster. Each worker node runs an agent, which the master node uses to monitor and manage it.</p>\n<p><strong>Pods</strong> – Groups of containers that share compute resources and a network. Kubernetes scales at the pod level. If additional capacity is needed to support an application running on containers in a pod, then Kubernetes can replicate the complete pod to add capacity via additional containers.</p>\n<p><strong>Deployments</strong> – Kubernetes Deployments control the creation and deployment of containerized applications. They use declarative configuration files that specify the desired state of an application and its containers. The deployment file also specifies how many replicas of a pod should run on a cluster. Deployments monitor running container pods, and if a pod fails, they recreate it.</p>\n<h2 id=\"kubernetes-services\">Kubernetes Services</h2>\n<p>Within these components, Kubernetes provides the following services and functionality:</p>\n<p><strong>Control Plane-Node Architecture</strong> – Kubernetes uses a Control Plane-Node architecture. A Control Plane node is responsible for managing the state of the cluster. It schedules pods to run on nodes, maintains the desired state of the application, scales applications, and rolls out new updates.</p>\n<p><strong>Rollouts</strong> – These describe the target container landscape needed for an application and let Kubernetes handle the process deployment to get there. This includes new deployments, changing existing deployed containers, and rollbacks to remove obsolete deployments. The Kubernetes rollout scheduler automatically places containers in the available infrastructure based on their resource requirements.</p>\n<p><strong>Self-healing</strong> – This monitors containers for issues and restarts them automatically if necessary. The restart can be within the same pod or on another pod.</p>\n<p><strong>Service discovery</strong> – Automatically expose a container to the broader network or other containers via a DNS name or an IP Address.</p>\n<p><strong>Load Balancing</strong> – Manage the load across multiple containers delivering the same application to ensure consistent performance.</p>\n<p><strong>Storage orchestration</strong> – Mount storage from the cloud or local resources as needed and for as long as necessary. Applications can request the storage they need, and the Kubernetes platform will allocate it dynamically from local or cloud storage resources without system administrators needing to be involved.</p>\n<p><strong>Secret and configuration management</strong> – Securely manage sensitive information like passwords, tokens, and keys. Deploy and update secrets and app configurations without exposing secrets on the network.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, Kubernetes provides the tools to automate the deployment, scaling, and management of application container clusters and pods across hosts. By abstracting the hardware infrastructure with a controlling software platform, Kubernetes presents the available IT infrastructure as a single deployment platform.</p>\n<p>Kubernetes promotes a more collaborative approach between developers and operations teams, enabling them to work together more effectively and encouraging a more DevSecOps-centric approach to application deployment.</p>",{headings:490,localImagePaths:506,remoteImagePaths:507,frontmatter:508,imagePaths:511},[491,494,497,500,503],{depth:29,slug:492,text:493},"containerization","Containerization",{depth:29,slug:495,text:496},"container-orchestration","Container Orchestration",{depth:29,slug:498,text:499},"kubernetes-components","Kubernetes Components",{depth:29,slug:501,text:502},"kubernetes-services","Kubernetes Services",{depth:29,slug:504,text:505},"conclusion","Conclusion",[],[],{title:479,slug:476,date:509,description:483,categories:510,excerpt:480},["Date","2025-05-05T18:06:35.000Z"],[312],[],"what-does-kubernetes-do/index.md","what-is-kubernetes-service-mesh",{id:513,data:515,body:521,filePath:522,digest:523,rendered:524,legacyId:550},{title:516,excerpt:517,categories:518,date:519,description:520,draft:20},"What Is Kubernetes Service Mesh","A Kubernetes service mesh(/what-is-istio-service-mesh/) is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via.",[17],["Date","2024-05-22T02:33:51.000Z"],"Traffic Management, Security, Observability and Reliability for Kubernetes A Dedicated Infrastructure Layer A Kubernetes service mesh is a software infrastructure layer…","## Traffic Management, Security, Observability and Reliability for Kubernetes\n\n### A Dedicated Infrastructure Layer\n\nA Kubernetes [service mesh](/what-is-istio-service-mesh) is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via containers.\n\nWhile containers and microservices bring many benefits when deploying applications, they also introduce issues that IT teams need to deal with.\n\n## Kubernetes Challenges\n\nCommon such issues are:\n\n**Increasing complexity**. When applications are decomposed into microservices that need to communicate within clusters, the network between them becomes very complex. Tracking the connections and their security policies as the number of microservices increases is difficult. Using a Kubernetes service mesh hides this complexity.    \n\n**Service opacity.** As the number of microservices increases, observing their interactions and communications becomes harder. A service mesh uses monitoring, logs, and connection tracing to pierce this opacity so you can see what’s happening. This is essential when optimizing performance and troubleshooting issues.\n\n**Managing dynamic changes.** Kubernetes supports auto-scaling of services, which can pose challenges for maintaining security policies and access control as services are dynamically added and removed.\n\n**Ingress and egress control.** Securing data flow into and out of a Kubernetes cluster is critical. Kubernetes’s dynamic nature may undermine any traditional security model organizations use. The service mesh makes delivering security in Kubernetes-based infrastructure easier.\n\nA Kubernetes service mesh links microservices, manages traffic flow between them, enforces policies, and collects telemetry data. The service mesh comprises several lightweight network proxies that are deployed alongside application containers in each service instance, using a sidecar proxy pattern. In a cluster, every pod has an extra container deployed, which is known as a sidecar, that intercepts traffic flowing to and from the primary application containers. It uses this network traffic to manage communication with other services and implement the features and policies that the mesh defines.\n\n## Service Mesh Solutions\n\nA service mesh delivers this key functionality within Kubernetes deployments:\n\n**Traffic management**. Service meshes provide advanced capabilities for traffic routing, including staged canary deployments, A/B testing, and blue-green deployments to production applications. They handle traffic flow within a cluster by ensuring that requests are directed to the correct service instance, even during network downtime or service updates, thus ensuring reliable routing.\n\n**Security**. Service meshes enforce policies around authentication, encryption, and authorization, ensuring communications within the cluster use secure service-to-service communication and are allowed to talk to each other. One standard method for this is mutual TLS (mTLS) for encryption and authentication. A service mesh like Istio also aligns with the Zero Trust security model defined in NIST SP 800-207A. Zero trust is becoming more important, and using a service mesh with Kubernetes helps deliver it. Also, it helps demonstrate the usage of zero trust principles to relevant third parties (such as regulators, insurers, and auditors). \n\n**Observability**. Service meshes collect telemetry data on service communication, such as container metrics, logs, and network traces. This data enables enhanced observability and is vital to understanding service behavior, detecting issues, and optimizing performance in production applications.\n\n**Reliability**. Service meshes help improve communication reliability through features like rate limiting, retries, circuit breaking, and timeouts. By handling these automatically, service meshes can ensure that the system remains resilient despite failures.\n\nIstio is one of the most popular and feature-rich service mesh offerings for Kubernetes. Istio is open source and uses a component called Envoy as the sidecar to intercept and deliver service mesh functionality.\n\n## Enterprise-Ready Service Mesh\n\nDeploying and managing Istio properly can still be complex, even if it’s not as difficult as managing Kubernetes clusters, pods, and containers directly. Tetrate is focused on the deployment and management of complex Kubernetes infrastructure using service mesh solutions. Tetrate’s enterprise service mesh products, [Tetrate Service Bridge (TSB)](/resource/tetrate-service-bridge-application-security-architecture), [Tetrate Istio Subscription (TIS)](/tetrate-istio-subscription), and [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy) are enterprise-ready service mesh solutions that build on core Istio and Envoy functionality.\n\n## Conclusion\n\nIn summary, a service mesh in Kubernetes simplifies the management of microservices-based applications. With the complexities of network management abstracted away, organizations can focus on their application’s business logic. The service mesh creates a dedicated infrastructure layer for handling service communication, improving microservice observability, reliability, and security. Because of this, it is now an essential component of modern container-based and Kubernetes-managed application architectures. Tetrate Service Bridge takes service mesh functionality to the next level by enhancing the popular Istio service mesh while also making it easier to use in enterprise deployments.","src/content/learn/what-is-kubernetes-service-mesh/index.md","89053a21fd5f4475",{html:525,metadata:526},"<h2 id=\"traffic-management-security-observability-and-reliability-for-kubernetes\">Traffic Management, Security, Observability and Reliability for Kubernetes</h2>\n<h3 id=\"a-dedicated-infrastructure-layer\">A Dedicated Infrastructure Layer</h3>\n<p>A Kubernetes <a href=\"/what-is-istio-service-mesh\">service mesh</a> is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via containers.</p>\n<p>While containers and microservices bring many benefits when deploying applications, they also introduce issues that IT teams need to deal with.</p>\n<h2 id=\"kubernetes-challenges\">Kubernetes Challenges</h2>\n<p>Common such issues are:</p>\n<p><strong>Increasing complexity</strong>. When applications are decomposed into microservices that need to communicate within clusters, the network between them becomes very complex. Tracking the connections and their security policies as the number of microservices increases is difficult. Using a Kubernetes service mesh hides this complexity.    </p>\n<p><strong>Service opacity.</strong> As the number of microservices increases, observing their interactions and communications becomes harder. A service mesh uses monitoring, logs, and connection tracing to pierce this opacity so you can see what’s happening. This is essential when optimizing performance and troubleshooting issues.</p>\n<p><strong>Managing dynamic changes.</strong> Kubernetes supports auto-scaling of services, which can pose challenges for maintaining security policies and access control as services are dynamically added and removed.</p>\n<p><strong>Ingress and egress control.</strong> Securing data flow into and out of a Kubernetes cluster is critical. Kubernetes’s dynamic nature may undermine any traditional security model organizations use. The service mesh makes delivering security in Kubernetes-based infrastructure easier.</p>\n<p>A Kubernetes service mesh links microservices, manages traffic flow between them, enforces policies, and collects telemetry data. The service mesh comprises several lightweight network proxies that are deployed alongside application containers in each service instance, using a sidecar proxy pattern. In a cluster, every pod has an extra container deployed, which is known as a sidecar, that intercepts traffic flowing to and from the primary application containers. It uses this network traffic to manage communication with other services and implement the features and policies that the mesh defines.</p>\n<h2 id=\"service-mesh-solutions\">Service Mesh Solutions</h2>\n<p>A service mesh delivers this key functionality within Kubernetes deployments:</p>\n<p><strong>Traffic management</strong>. Service meshes provide advanced capabilities for traffic routing, including staged canary deployments, A/B testing, and blue-green deployments to production applications. They handle traffic flow within a cluster by ensuring that requests are directed to the correct service instance, even during network downtime or service updates, thus ensuring reliable routing.</p>\n<p><strong>Security</strong>. Service meshes enforce policies around authentication, encryption, and authorization, ensuring communications within the cluster use secure service-to-service communication and are allowed to talk to each other. One standard method for this is mutual TLS (mTLS) for encryption and authentication. A service mesh like Istio also aligns with the Zero Trust security model defined in NIST SP 800-207A. Zero trust is becoming more important, and using a service mesh with Kubernetes helps deliver it. Also, it helps demonstrate the usage of zero trust principles to relevant third parties (such as regulators, insurers, and auditors). </p>\n<p><strong>Observability</strong>. Service meshes collect telemetry data on service communication, such as container metrics, logs, and network traces. This data enables enhanced observability and is vital to understanding service behavior, detecting issues, and optimizing performance in production applications.</p>\n<p><strong>Reliability</strong>. Service meshes help improve communication reliability through features like rate limiting, retries, circuit breaking, and timeouts. By handling these automatically, service meshes can ensure that the system remains resilient despite failures.</p>\n<p>Istio is one of the most popular and feature-rich service mesh offerings for Kubernetes. Istio is open source and uses a component called Envoy as the sidecar to intercept and deliver service mesh functionality.</p>\n<h2 id=\"enterprise-ready-service-mesh\">Enterprise-Ready Service Mesh</h2>\n<p>Deploying and managing Istio properly can still be complex, even if it’s not as difficult as managing Kubernetes clusters, pods, and containers directly. Tetrate is focused on the deployment and management of complex Kubernetes infrastructure using service mesh solutions. Tetrate’s enterprise service mesh products, <a href=\"/resource/tetrate-service-bridge-application-security-architecture\">Tetrate Service Bridge (TSB)</a>, <a href=\"/tetrate-istio-subscription\">Tetrate Istio Subscription (TIS)</a>, and <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a> are enterprise-ready service mesh solutions that build on core Istio and Envoy functionality.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, a service mesh in Kubernetes simplifies the management of microservices-based applications. With the complexities of network management abstracted away, organizations can focus on their application’s business logic. The service mesh creates a dedicated infrastructure layer for handling service communication, improving microservice observability, reliability, and security. Because of this, it is now an essential component of modern container-based and Kubernetes-managed application architectures. Tetrate Service Bridge takes service mesh functionality to the next level by enhancing the popular Istio service mesh while also making it easier to use in enterprise deployments.</p>",{headings:527,localImagePaths:544,remoteImagePaths:545,frontmatter:546,imagePaths:549},[528,531,534,537,540,543],{depth:29,slug:529,text:530},"traffic-management-security-observability-and-reliability-for-kubernetes","Traffic Management, Security, Observability and Reliability for Kubernetes",{depth:327,slug:532,text:533},"a-dedicated-infrastructure-layer","A Dedicated Infrastructure Layer",{depth:29,slug:535,text:536},"kubernetes-challenges","Kubernetes Challenges",{depth:29,slug:538,text:539},"service-mesh-solutions","Service Mesh Solutions",{depth:29,slug:541,text:542},"enterprise-ready-service-mesh","Enterprise-Ready Service Mesh",{depth:29,slug:504,text:505},[],[],{title:516,slug:513,date:547,description:520,categories:548,excerpt:517},["Date","2024-05-22T02:33:51.000Z"],[17],[],"what-is-kubernetes-service-mesh/index.md","kubernetes-security-best-practices",{id:551,data:553,body:559,filePath:560,digest:561,rendered:562,legacyId:584},{title:554,excerpt:555,categories:556,date:557,description:558,draft:20},"Kubernetes Security Best Practices","Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into Kubernetes security(/learn/kubernetes-security-best-practices) best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity.",[17],["Date","2025-02-12T00:11:19.000Z"],"Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications.…","Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into [Kubernetes security](/learn/kubernetes-security-best-practices) best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity threats.\n\nWe assume that readers know what Kubernetes is, the specialized language used to define components (clusters, pods, etc.), and typical use cases. We’ll proceed directly to discuss various Kubernetes security best practices and end with a short call to action pointing to [Tetrate’s Kubernetes management solutions](/kubernetes-consulting-services) and how adding them to your toolkit can simplify and enhance the [security and management of Kubernetes environments](/learn/kubernetes-security-architecture).\n\n## Kubernetes Network Security\n\nUsing Kubernetes to deploy and manage applications and services in containers doesn’t change the need to provide robust cybersecurity protections. It is still essential to secure the networking used within a Kubernetes cluster.\n\nA core step in delivering Kubernetes network security is defining and implementing network policies to control how pods communicate with each other and the wider network infrastructure beyond the Kubernetes environment. Another crucial step is the isolation of sensitive workloads using network segmentation techniques (discussed further in a section below).\n\nNetwork security best practices for Kubernetes deployments occur at multiple levels and via various techniques such as:\n\n**The API Server**. The central management interface for a Kubernetes deployment. It is critical to protect it against unauthorized access.\n\n**Authentication and Authorization Controls**. Implement [robust access control](/blog/rbac-vs-abac-vs-ngac) to provide granular access control for managing user and service account permissions.\n\n**Network Security Settings**. Use network policies and segmentation to control traffic flow, isolate sensitive workloads, and limit the potential attack surface available to cyber attackers.\n\n**Strong Secrets Management**. Use Kubernetes Secrets or third-party secret stores to enable the secure storage and use of sensitive security access data such as passwords and API keys.\n\n**Pod Security Policies**. Deploy well-tested Pod security policies to enforce security settings within the container environments to ensure secure configurations get deployed and limit potential attack vectors.\n\n**Image Security**. Regularly scan container deployment images for known vulnerabilities and ensure that security patches for all components that comprise a container package get updated with the latest security updates.\n\n## Kubernetes Microsegmentation\n\nMicrosegmentation in Kubernetes divides clusters into smaller, more manageable segments, each with specific security policies. This approach limits the potential impact of a breach, as attackers can only access a small portion of the network if they compromise a segment. Microsegmentation is core to delivering zero-trust networking best practices within a Kubernetes deployment.\n\nIn Kubernetes, microsegmentation is achieved using various techniques such as the namespaces outlined in the previous section, network policies to restrict traffic flows plus user/service access to Pods, and also via functionality provided by service meshes, such as encryption and traffic monitoring.\n\n## Kubernetes PCI Compliance\n\nAny organization handling credit card cardholder data is required to comply with the [Payment Card Industry (PCI) regulations](/resource/meeting-pci-compliance-standards). You don’t need to operate at the consumer or retail border and collect payments directly from customers to be required to implement PCI requirements. If you handle or store cardholder data, you must be PCI compliant.\n\nThe inherent security built into Kubernetes management containerized financial application deployments can help reduce the burden of achieving PCI compliance. Features such as implementing strict access controls, encrypting data at rest and in transit, and maintaining a [secure network architecture](/learn/kubernetes-security-architecture) directly map to PCI requirements. Kubernetes helps deliver PCI compliance through these built-in security features you will already use for basic security. It also allows you to easily integrate your deployed container-based financial applications with additional third-party security tools.\n\nSome PCI requirements that Kubernetes addresses as a core part of the platform are access control, microsegmentation, encrypting data at rest and in transit, logging and monitoring of traffic, plus vulnerability scanning and patching of deployed images and deployed containers.\n\n## Enhancing Kubernetes with Tetrate\n\nTetrate provides tools for seamlessly implementing security, identity, [traffic management, and other policies across Kubernetes](/manage-kubernetes-complexity) clusters. The tools address the unique challenges containers and Kubernetes bring regarding complexity, security, and consistency.\n\nOrganizations cannot approach security in Kubernetes with a one-size-fits-all mentality. Instead, it requires a comprehensive strategy tailored to each environment’s specific needs and architecture. By following the best practices mentioned above, in conjunction with management solutions such as Tetrate’s Istio Service Mesh and other Kubernetes tools, organizations can build solid and secure Kubernetes ecosystems capable of supporting their most critical applications without any additional management overhead.\n\nVisit our website’s Products, Solutions, and Learn section to read more. [Reach out to us today](/contact-sales) to discuss how we can collaborate to secure and simplify managing your Kubernetes container environment.","src/content/learn/kubernetes-security-best-practices/index.md","9db19c76ac960cf0",{html:563,metadata:564},"<p>Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into <a href=\"/learn/kubernetes-security-best-practices\">Kubernetes security</a> best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity threats.</p>\n<p>We assume that readers know what Kubernetes is, the specialized language used to define components (clusters, pods, etc.), and typical use cases. We’ll proceed directly to discuss various Kubernetes security best practices and end with a short call to action pointing to <a href=\"/kubernetes-consulting-services\">Tetrate’s Kubernetes management solutions</a> and how adding them to your toolkit can simplify and enhance the <a href=\"/learn/kubernetes-security-architecture\">security and management of Kubernetes environments</a>.</p>\n<h2 id=\"kubernetes-network-security\">Kubernetes Network Security</h2>\n<p>Using Kubernetes to deploy and manage applications and services in containers doesn’t change the need to provide robust cybersecurity protections. It is still essential to secure the networking used within a Kubernetes cluster.</p>\n<p>A core step in delivering Kubernetes network security is defining and implementing network policies to control how pods communicate with each other and the wider network infrastructure beyond the Kubernetes environment. Another crucial step is the isolation of sensitive workloads using network segmentation techniques (discussed further in a section below).</p>\n<p>Network security best practices for Kubernetes deployments occur at multiple levels and via various techniques such as:</p>\n<p><strong>The API Server</strong>. The central management interface for a Kubernetes deployment. It is critical to protect it against unauthorized access.</p>\n<p><strong>Authentication and Authorization Controls</strong>. Implement <a href=\"/blog/rbac-vs-abac-vs-ngac\">robust access control</a> to provide granular access control for managing user and service account permissions.</p>\n<p><strong>Network Security Settings</strong>. Use network policies and segmentation to control traffic flow, isolate sensitive workloads, and limit the potential attack surface available to cyber attackers.</p>\n<p><strong>Strong Secrets Management</strong>. Use Kubernetes Secrets or third-party secret stores to enable the secure storage and use of sensitive security access data such as passwords and API keys.</p>\n<p><strong>Pod Security Policies</strong>. Deploy well-tested Pod security policies to enforce security settings within the container environments to ensure secure configurations get deployed and limit potential attack vectors.</p>\n<p><strong>Image Security</strong>. Regularly scan container deployment images for known vulnerabilities and ensure that security patches for all components that comprise a container package get updated with the latest security updates.</p>\n<h2 id=\"kubernetes-microsegmentation\">Kubernetes Microsegmentation</h2>\n<p>Microsegmentation in Kubernetes divides clusters into smaller, more manageable segments, each with specific security policies. This approach limits the potential impact of a breach, as attackers can only access a small portion of the network if they compromise a segment. Microsegmentation is core to delivering zero-trust networking best practices within a Kubernetes deployment.</p>\n<p>In Kubernetes, microsegmentation is achieved using various techniques such as the namespaces outlined in the previous section, network policies to restrict traffic flows plus user/service access to Pods, and also via functionality provided by service meshes, such as encryption and traffic monitoring.</p>\n<h2 id=\"kubernetes-pci-compliance\">Kubernetes PCI Compliance</h2>\n<p>Any organization handling credit card cardholder data is required to comply with the <a href=\"/resource/meeting-pci-compliance-standards\">Payment Card Industry (PCI) regulations</a>. You don’t need to operate at the consumer or retail border and collect payments directly from customers to be required to implement PCI requirements. If you handle or store cardholder data, you must be PCI compliant.</p>\n<p>The inherent security built into Kubernetes management containerized financial application deployments can help reduce the burden of achieving PCI compliance. Features such as implementing strict access controls, encrypting data at rest and in transit, and maintaining a <a href=\"/learn/kubernetes-security-architecture\">secure network architecture</a> directly map to PCI requirements. Kubernetes helps deliver PCI compliance through these built-in security features you will already use for basic security. It also allows you to easily integrate your deployed container-based financial applications with additional third-party security tools.</p>\n<p>Some PCI requirements that Kubernetes addresses as a core part of the platform are access control, microsegmentation, encrypting data at rest and in transit, logging and monitoring of traffic, plus vulnerability scanning and patching of deployed images and deployed containers.</p>\n<h2 id=\"enhancing-kubernetes-with-tetrate\">Enhancing Kubernetes with Tetrate</h2>\n<p>Tetrate provides tools for seamlessly implementing security, identity, <a href=\"/manage-kubernetes-complexity\">traffic management, and other policies across Kubernetes</a> clusters. The tools address the unique challenges containers and Kubernetes bring regarding complexity, security, and consistency.</p>\n<p>Organizations cannot approach security in Kubernetes with a one-size-fits-all mentality. Instead, it requires a comprehensive strategy tailored to each environment’s specific needs and architecture. By following the best practices mentioned above, in conjunction with management solutions such as Tetrate’s Istio Service Mesh and other Kubernetes tools, organizations can build solid and secure Kubernetes ecosystems capable of supporting their most critical applications without any additional management overhead.</p>\n<p>Visit our website’s Products, Solutions, and Learn section to read more. <a href=\"/contact-sales\">Reach out to us today</a> to discuss how we can collaborate to secure and simplify managing your Kubernetes container environment.</p>",{headings:565,localImagePaths:578,remoteImagePaths:579,frontmatter:580,imagePaths:583},[566,569,572,575],{depth:29,slug:567,text:568},"kubernetes-network-security","Kubernetes Network Security",{depth:29,slug:570,text:571},"kubernetes-microsegmentation","Kubernetes Microsegmentation",{depth:29,slug:573,text:574},"kubernetes-pci-compliance","Kubernetes PCI Compliance",{depth:29,slug:576,text:577},"enhancing-kubernetes-with-tetrate","Enhancing Kubernetes with Tetrate",[],[],{title:554,slug:551,date:581,description:558,categories:582,excerpt:555},["Date","2025-02-12T00:11:19.000Z"],[17],[],"kubernetes-security-best-practices/index.md","what-is-kubernetes-gateway-api",{id:585,data:587,body:593,filePath:594,digest:595,rendered:596,legacyId:636},{title:588,excerpt:589,categories:590,date:591,description:592,draft:20},"What Is the Kubernetes Gateway API?","The Kubernetes Gateway API(/learn/what-is-the-kubernetes-gateway-api/), aka “Gateway.",[312],["Date","2025-04-17T20:28:17.000Z"],"Discover Kubernetes Gateway API for efficient traffic management. Learn benefits, design goals, and comparison to Ingress. Optimize traffic management now!","## Overview\n\nThe [Kubernetes Gateway API](/learn/what-is-the-kubernetes-gateway-api), aka “[Gateway API,](/external-link/)\n\nConceived as a successor to the earlier Ingress API, Gateway API aims to enhance the configuration and management of Kubernetes ingress, service discovery, load balancing, and traffic routing by providing a unified and extensible API that integrates with Kubernetes’ native resources such as Services, Endpoints, and Ingresses. While there is no default implementation of the Gateway API out of the box in Kubernetes, there is [a wide range of commercial and open-source implementations available](/external-link/)\n\n> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy). TEG is the easiest way to get started with Envoy for production use cases. [Get access now ›](/demo-request)\n\n## Benefits of the Gateway API\n\nGateway API represents a superset of Ingress functionality, enabling more advanced use cases. It offers the following benefits over earlier ingress implementations:\n\n- A comprehensive, unified, and standardized API for managing traffic into and out of a Kubernetes cluster.\n\n- More powerful and granular control, including expanded protocol support and routing options.\n- More flexible configuration that can be extended to address specific use cases.\n\n### Design Goals\n\nThe Gateway API is designed to be role-oriented, portable, expressive, and extensible:\n\n**Role-oriented:** Since [Kubernetes infrastructure](/learn/what-is-mtls) is typically a shared resource, multiple people with different roles and responsibilities must jointly participate in various aspects of the configuration and management of those resources. The Gateway API seeks to strike a balance between distributed flexibility and centralized control, allowing shared infrastructure to be used effectively by multiple, potentially non-coordinated teams.\n\n**Expressive:** As an advancement over Kubernetes Ingress, [the Gateway API](/learn/what-is-kubernetes-gateway-api) is meant to provide built-in core capabilities, such as header-based matching, traffic weighting, and other features that were previously only available through custom annotations that were not portable across implementations.\n\n**Portable:** The Gateway API is designed to be a universal specification supported by multiple implementations.\n\n**Flexible Conformance:** To accommodate a broad feature set and wide range of implementations, Gateway API offers three support levels: _core features_ that must be implemented; _extended features_ that are expected to be portable across implementations, but not universally supported; and _implementation-specific_ features that are not expected to be portable and are vendor-specific.\n\n**Extensible:** Users can add new features and functionality by defining their own custom resources that can be used alongside the existing resources defined in the specification. This extensibility allows the Gateway API to evolve over time and adapt to new use cases and requirements.\n\n## Gateway API vs Kubernetes Ingress\n\nBoth the Gateway API and Ingress are used for managing inbound traffic to Kubernetes clusters, but they differ in their approach and functionality.\n\n### Ingress\n\nIngress is an earlier API originally introduced to route incoming HTTP traffic to services using a straightforward, declarative syntax. It offered more control than the more limited options available at the time for exposing services in Kubernetes to the outside world.\n\nIngress controllers, such as NGINX, Traefik, or Istio, may be used to implement the Ingress resource and provide additional features like SSL termination, load balancing algorithms, and traffic shaping. More advanced features have been added by vendors, typically in the form of custom annotations that aren’t always portable across implementations.\n\n### Ingress Limitations\n\nWhile effective for basic use cases, Ingress has significant limitations for advanced uses, including:\n\n- **Limited power and flexibility:** Ingress is often not powerful or flexible enough for most real-world use and it only supports HTTP protocol routing.\n- **Limited expressiveness:** It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through non-portable annotations.\n- **Proliferation of non-portable, vendor-specific annotations:** The lack of advanced capabilities has driven a proliferation of implementation-specific annotations. For example,  URL redirection using the NGINX Ingress Controller requires configuration of the nginx.ingress.kubernetes.io/rewrite-target annotation, which makes it incompatible with a programmable proxy like Envoy.\n- **Lack of cross-namespace support.** Since Ingress can only route traffic to a single namespace, it can’t be used as a unified gateway across multiple namespaces.\n- **Lack of role-based configuration and management responsibilities.** Since there is no built-in delineation of responsibilities, operational tasks like creating and managing gateways that more properly belong to platform engineering are often shouldered by app developers.\n\n### Gateway API vs Ingress\n\nGateway API is more general-purpose than Ingress and can be used to configure a variety of traffic management features such as load balancing, TLS passthrough, traffic routing based on request headers, and integration with external services in a more consistent, portable way.\n\nLike Ingress, Gateway API is an official [Kubernetes API](/learn/what-is-the-kubernetes-gateway-api) and represents a superset of Ingress functionality, enabling more advanced use cases. Similar to Ingress, there is no default implementation of Gateway API built into Kubernetes. Instead, there are [many different implementations available](/external-link/)\n\nFor a deep dive on the history of Kubernetes Ingress and Gateway API, read our article, [Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh](/blog/why-the-gateway-api-is-the-unified-future-of-ingress-for-kubernetes-and-service-mesh).\n\n## Gateway API vs API Gateway\n\nThe Gateway API is a built-in Kubernetes API that provides a standardized way to manage and configure inbound traffic in Kubernetes environments.\n\nAn API Gateway provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the [API endpoints of an application](/learn/what-is-an-api-gateway).\n\nImplementations of Gateway API, such as the Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API Gateway capabilities.\n\n## How Does the Gateway API Work?\n\nThe Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for multiple protocols in addition to HTTP. It models more infrastructure components, providing better deployment and management options for cluster operations.\n\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API:\n\nThe following is an example of using the Gateway API in Istio:\n\n```\napiVersion: gateway.networking.k8s.io/v1alpha2\n kind: Gateway\n metadata:\n  name: gateway\n  namespace: istio-ingress\n  spec:\n  gatewayClassName: istio\n  listeners:\n\n    hostname: \"*.example.com\"\n    port: 80\n    protocol: HTTP\n    allowedRoutes:\n      namespaces:\n        from: All\n ---\n apiVersion: gateway.networking.k8s.io/v1alpha2\n kind: HTTPRoute\n metadata:\n  name: http\n  namespace: default\n  spec:\n  parentRefs:\n\n    namespace: istio-ingress\n  hostnames: [\"httpbin.example.com\"]\n  rules:\n\n        type: PathPrefix\n        value: /\n    backendRefs:\n\n      port: 8000\n```\n\nSimilar to Ingress, Gateway uses `gatewayClassName` to declare the controller it uses, which needs to be created by the platform administrator and allows client requests for the `*.example.com` domain. Application developers can create routing rules in the namespace where their service resides, in this case, default, and bind to the Gateway via parentRefs, but only if the Gateway explicitly allows them to do so (via the rules set in the `allowRoutes` field).\n\nWhen you apply the above configuration, Istio will automatically create a load-balancing gateway for you. The following diagram shows the workflow of the Gateway API:\n\nThe detailed process is as follows:\n\n1.  The infrastructure provider provides GatewayClass and Gateway Controller.\n2.  Platform operator deploy Gateway (multiple Gateways possible, or using different GatewayClasses).\n3.  Gateway Controller continuously monitors changes to the GatewayClass and Gateway objects in the Kubernetes API Server.\n4.  Gateway controller will create the corresponding gateway based on cluster operations and maintenance configuration.\n5.  Application developers apply xRoutes and bind them to the service.\n6.  If in the cloud, the client accesses the load balancer for that ingress gateway.\n7.  The gateway will route to the corresponding back-end service based on the matching criteria in the traffic request.\n\nFrom the above steps, we can see that the Gateway API has a clear division of roles compared to Ingress and that routing rules can be decoupled from the gateway configuration, significantly increasing management flexibility.\n\nThe following diagram shows the route flow after it is accessed at the gateway and processed:\n\nFrom this figure, we can see that the route is bound to the gateway. The route is generally deployed in the same namespace as its backend services. Suppose the route is in a different namespace, and you need to explicitly give the route cross-namespace reference rights in ReferenceGrant, for example. In that case, the following HTTPRoute `foo` in the `foo` namespace can refer to the `bar` service in the `bar` namespace:\n\n```\nkind: HTTPRoute\n metadata:\n  name: foo\n  namespace: foo\n spec:\n  rules:\n\n    forwardTo:\n      backend:\n\n        namespace: bar\n ---\n kind: ReferenceGrant\n metadata:\n  name: bar\n  namespace: bar\n spec:\n  from:\n\n    kind: HTTPRoute\n    namespace: foo\n  to:\n\n    kind: Service\n```\n\n## Get Started with Gateway API Using Envoy Gateway\n\nEnvoy Gateway is an implementation of the Gateway API that uses [Envoy Proxy](/what-is-envoy-proxy) as an API gateway to deliver a simplified deployment model and an API layer aimed at lighter use cases.\n\nGetting started with Gateway API and Envoy Gateway is easy. Go to the documentation on [the Envoy Gateway project site](/external-link/)\n\n- [Quick start ›](/quick-start)\n- [User guides ›](/user-guides)\n\n## Get Enterprise Support for Your Envoy Gateway Deployment\n\n[Tetrate Enterprise Gateway for Envoy](/tetrate-enterprise-gateway-for-envoy) provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.\n\nTetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.\n\n- [Get access now ›](/demo-request)\n- [Learn more ›](/tetrate-enterprise-gateway-for-envoy)\n- [Quick start ›](/quick-start)","src/content/learn/what-is-kubernetes-gateway-api/index.md","1a51146ba11799ea",{html:597,metadata:598},"<h2 id=\"overview\">Overview</h2>\n<p>The <a href=\"/learn/what-is-the-kubernetes-gateway-api\">Kubernetes Gateway API</a>, aka “<a href=\"/external-link/\">Gateway API,</a></p>\n<p>Conceived as a successor to the earlier Ingress API, Gateway API aims to enhance the configuration and management of Kubernetes ingress, service discovery, load balancing, and traffic routing by providing a unified and extensible API that integrates with Kubernetes’ native resources such as Services, Endpoints, and Ingresses. While there is no default implementation of the Gateway API out of the box in Kubernetes, there is <a href=\"/external-link/\">a wide range of commercial and open-source implementations available</a></p>\n<blockquote>\n<p>Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a>. TEG is the easiest way to get started with Envoy for production use cases. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h2 id=\"benefits-of-the-gateway-api\">Benefits of the Gateway API</h2>\n<p>Gateway API represents a superset of Ingress functionality, enabling more advanced use cases. It offers the following benefits over earlier ingress implementations:</p>\n<ul>\n<li>\n<p>A comprehensive, unified, and standardized API for managing traffic into and out of a Kubernetes cluster.</p>\n</li>\n<li>\n<p>More powerful and granular control, including expanded protocol support and routing options.</p>\n</li>\n<li>\n<p>More flexible configuration that can be extended to address specific use cases.</p>\n</li>\n</ul>\n<h3 id=\"design-goals\">Design Goals</h3>\n<p>The Gateway API is designed to be role-oriented, portable, expressive, and extensible:</p>\n<p><strong>Role-oriented:</strong> Since <a href=\"/learn/what-is-mtls\">Kubernetes infrastructure</a> is typically a shared resource, multiple people with different roles and responsibilities must jointly participate in various aspects of the configuration and management of those resources. The Gateway API seeks to strike a balance between distributed flexibility and centralized control, allowing shared infrastructure to be used effectively by multiple, potentially non-coordinated teams.</p>\n<p><strong>Expressive:</strong> As an advancement over Kubernetes Ingress, <a href=\"/learn/what-is-kubernetes-gateway-api\">the Gateway API</a> is meant to provide built-in core capabilities, such as header-based matching, traffic weighting, and other features that were previously only available through custom annotations that were not portable across implementations.</p>\n<p><strong>Portable:</strong> The Gateway API is designed to be a universal specification supported by multiple implementations.</p>\n<p><strong>Flexible Conformance:</strong> To accommodate a broad feature set and wide range of implementations, Gateway API offers three support levels: <em>core features</em> that must be implemented; <em>extended features</em> that are expected to be portable across implementations, but not universally supported; and <em>implementation-specific</em> features that are not expected to be portable and are vendor-specific.</p>\n<p><strong>Extensible:</strong> Users can add new features and functionality by defining their own custom resources that can be used alongside the existing resources defined in the specification. This extensibility allows the Gateway API to evolve over time and adapt to new use cases and requirements.</p>\n<h2 id=\"gateway-api-vs-kubernetes-ingress\">Gateway API vs Kubernetes Ingress</h2>\n<p>Both the Gateway API and Ingress are used for managing inbound traffic to Kubernetes clusters, but they differ in their approach and functionality.</p>\n<h3 id=\"ingress\">Ingress</h3>\n<p>Ingress is an earlier API originally introduced to route incoming HTTP traffic to services using a straightforward, declarative syntax. It offered more control than the more limited options available at the time for exposing services in Kubernetes to the outside world.</p>\n<p>Ingress controllers, such as NGINX, Traefik, or Istio, may be used to implement the Ingress resource and provide additional features like SSL termination, load balancing algorithms, and traffic shaping. More advanced features have been added by vendors, typically in the form of custom annotations that aren’t always portable across implementations.</p>\n<h3 id=\"ingress-limitations\">Ingress Limitations</h3>\n<p>While effective for basic use cases, Ingress has significant limitations for advanced uses, including:</p>\n<ul>\n<li><strong>Limited power and flexibility:</strong> Ingress is often not powerful or flexible enough for most real-world use and it only supports HTTP protocol routing.</li>\n<li><strong>Limited expressiveness:</strong> It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through non-portable annotations.</li>\n<li><strong>Proliferation of non-portable, vendor-specific annotations:</strong> The lack of advanced capabilities has driven a proliferation of implementation-specific annotations. For example,  URL redirection using the NGINX Ingress Controller requires configuration of the nginx.ingress.kubernetes.io/rewrite-target annotation, which makes it incompatible with a programmable proxy like Envoy.</li>\n<li><strong>Lack of cross-namespace support.</strong> Since Ingress can only route traffic to a single namespace, it can’t be used as a unified gateway across multiple namespaces.</li>\n<li><strong>Lack of role-based configuration and management responsibilities.</strong> Since there is no built-in delineation of responsibilities, operational tasks like creating and managing gateways that more properly belong to platform engineering are often shouldered by app developers.</li>\n</ul>\n<h3 id=\"gateway-api-vs-ingress\">Gateway API vs Ingress</h3>\n<p>Gateway API is more general-purpose than Ingress and can be used to configure a variety of traffic management features such as load balancing, TLS passthrough, traffic routing based on request headers, and integration with external services in a more consistent, portable way.</p>\n<p>Like Ingress, Gateway API is an official <a href=\"/learn/what-is-the-kubernetes-gateway-api\">Kubernetes API</a> and represents a superset of Ingress functionality, enabling more advanced use cases. Similar to Ingress, there is no default implementation of Gateway API built into Kubernetes. Instead, there are <a href=\"/external-link/\">many different implementations available</a></p>\n<p>For a deep dive on the history of Kubernetes Ingress and Gateway API, read our article, <a href=\"/blog/why-the-gateway-api-is-the-unified-future-of-ingress-for-kubernetes-and-service-mesh\">Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh</a>.</p>\n<h2 id=\"gateway-api-vs-api-gateway\">Gateway API vs API Gateway</h2>\n<p>The Gateway API is a built-in Kubernetes API that provides a standardized way to manage and configure inbound traffic in Kubernetes environments.</p>\n<p>An API Gateway provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the <a href=\"/learn/what-is-an-api-gateway\">API endpoints of an application</a>.</p>\n<p>Implementations of Gateway API, such as the Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API Gateway capabilities.</p>\n<h2 id=\"how-does-the-gateway-api-work\">How Does the Gateway API Work?</h2>\n<p>The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for multiple protocols in addition to HTTP. It models more infrastructure components, providing better deployment and management options for cluster operations.</p>\n<p>In addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API:</p>\n<p>The following is an example of using the Gateway API in Istio:</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: gateway.networking.k8s.io/v1alpha2</span></span>\n<span class=\"line\"><span> kind: Gateway</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: gateway</span></span>\n<span class=\"line\"><span>  namespace: istio-ingress</span></span>\n<span class=\"line\"><span>  spec:</span></span>\n<span class=\"line\"><span>  gatewayClassName: istio</span></span>\n<span class=\"line\"><span>  listeners:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    hostname: \"*.example.com\"</span></span>\n<span class=\"line\"><span>    port: 80</span></span>\n<span class=\"line\"><span>    protocol: HTTP</span></span>\n<span class=\"line\"><span>    allowedRoutes:</span></span>\n<span class=\"line\"><span>      namespaces:</span></span>\n<span class=\"line\"><span>        from: All</span></span>\n<span class=\"line\"><span> ---</span></span>\n<span class=\"line\"><span> apiVersion: gateway.networking.k8s.io/v1alpha2</span></span>\n<span class=\"line\"><span> kind: HTTPRoute</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: http</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span>  spec:</span></span>\n<span class=\"line\"><span>  parentRefs:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    namespace: istio-ingress</span></span>\n<span class=\"line\"><span>  hostnames: [\"httpbin.example.com\"]</span></span>\n<span class=\"line\"><span>  rules:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>        type: PathPrefix</span></span>\n<span class=\"line\"><span>        value: /</span></span>\n<span class=\"line\"><span>    backendRefs:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>      port: 8000</span></span></code></pre>\n<p>Similar to Ingress, Gateway uses <code>gatewayClassName</code> to declare the controller it uses, which needs to be created by the platform administrator and allows client requests for the <code>*.example.com</code> domain. Application developers can create routing rules in the namespace where their service resides, in this case, default, and bind to the Gateway via parentRefs, but only if the Gateway explicitly allows them to do so (via the rules set in the <code>allowRoutes</code> field).</p>\n<p>When you apply the above configuration, Istio will automatically create a load-balancing gateway for you. The following diagram shows the workflow of the Gateway API:</p>\n<p>The detailed process is as follows:</p>\n<ol>\n<li>The infrastructure provider provides GatewayClass and Gateway Controller.</li>\n<li>Platform operator deploy Gateway (multiple Gateways possible, or using different GatewayClasses).</li>\n<li>Gateway Controller continuously monitors changes to the GatewayClass and Gateway objects in the Kubernetes API Server.</li>\n<li>Gateway controller will create the corresponding gateway based on cluster operations and maintenance configuration.</li>\n<li>Application developers apply xRoutes and bind them to the service.</li>\n<li>If in the cloud, the client accesses the load balancer for that ingress gateway.</li>\n<li>The gateway will route to the corresponding back-end service based on the matching criteria in the traffic request.</li>\n</ol>\n<p>From the above steps, we can see that the Gateway API has a clear division of roles compared to Ingress and that routing rules can be decoupled from the gateway configuration, significantly increasing management flexibility.</p>\n<p>The following diagram shows the route flow after it is accessed at the gateway and processed:</p>\n<p>From this figure, we can see that the route is bound to the gateway. The route is generally deployed in the same namespace as its backend services. Suppose the route is in a different namespace, and you need to explicitly give the route cross-namespace reference rights in ReferenceGrant, for example. In that case, the following HTTPRoute <code>foo</code> in the <code>foo</code> namespace can refer to the <code>bar</code> service in the <code>bar</code> namespace:</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>kind: HTTPRoute</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: foo</span></span>\n<span class=\"line\"><span>  namespace: foo</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  rules:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    forwardTo:</span></span>\n<span class=\"line\"><span>      backend:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>        namespace: bar</span></span>\n<span class=\"line\"><span> ---</span></span>\n<span class=\"line\"><span> kind: ReferenceGrant</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: bar</span></span>\n<span class=\"line\"><span>  namespace: bar</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  from:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    kind: HTTPRoute</span></span>\n<span class=\"line\"><span>    namespace: foo</span></span>\n<span class=\"line\"><span>  to:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    kind: Service</span></span></code></pre>\n<h2 id=\"get-started-with-gateway-api-using-envoy-gateway\">Get Started with Gateway API Using Envoy Gateway</h2>\n<p>Envoy Gateway is an implementation of the Gateway API that uses <a href=\"/what-is-envoy-proxy\">Envoy Proxy</a> as an API gateway to deliver a simplified deployment model and an API layer aimed at lighter use cases.</p>\n<p>Getting started with Gateway API and Envoy Gateway is easy. Go to the documentation on <a href=\"/external-link/\">the Envoy Gateway project site</a></p>\n<ul>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n<li><a href=\"/user-guides\">User guides ›</a></li>\n</ul>\n<h2 id=\"get-enterprise-support-for-your-envoy-gateway-deployment\">Get Enterprise Support for Your Envoy Gateway Deployment</h2>\n<p><a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy</a> provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.</p>\n<p>Tetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.</p>\n<ul>\n<li><a href=\"/demo-request\">Get access now ›</a></li>\n<li><a href=\"/tetrate-enterprise-gateway-for-envoy\">Learn more ›</a></li>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n</ul>",{headings:599,localImagePaths:630,remoteImagePaths:631,frontmatter:632,imagePaths:635},[600,601,604,607,610,612,615,618,621,624,627],{depth:29,slug:158,text:159},{depth:29,slug:602,text:603},"benefits-of-the-gateway-api","Benefits of the Gateway API",{depth:327,slug:605,text:606},"design-goals","Design Goals",{depth:29,slug:608,text:609},"gateway-api-vs-kubernetes-ingress","Gateway API vs Kubernetes Ingress",{depth:327,slug:611,text:76},"ingress",{depth:327,slug:613,text:614},"ingress-limitations","Ingress Limitations",{depth:327,slug:616,text:617},"gateway-api-vs-ingress","Gateway API vs Ingress",{depth:29,slug:619,text:620},"gateway-api-vs-api-gateway","Gateway API vs API Gateway",{depth:29,slug:622,text:623},"how-does-the-gateway-api-work","How Does the Gateway API Work?",{depth:29,slug:625,text:626},"get-started-with-gateway-api-using-envoy-gateway","Get Started with Gateway API Using Envoy Gateway",{depth:29,slug:628,text:629},"get-enterprise-support-for-your-envoy-gateway-deployment","Get Enterprise Support for Your Envoy Gateway Deployment",[],[],{title:588,slug:585,date:633,description:592,categories:634,excerpt:589},["Date","2025-04-17T20:28:17.000Z"],[312],[],"what-is-kubernetes-gateway-api/index.md","what-is-mtls",{id:637,data:639,body:645,filePath:646,digest:647,rendered:648,legacyId:742},{title:640,excerpt:641,categories:642,date:643,description:644,draft:20},"What Is mTLS?","Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where both parties must prove their identities to each other.  mTLS extends the security provided by TLS(/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery/) by adding mutual authentication between the client and the.",[17],["Date","2025-04-16T10:10:53.000Z"],"Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed…","Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where **both parties must prove their identities to each other**.  mTLS extends the security provided by [TLS](/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery) by adding mutual authentication between the client and the server.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the easiest way to implement mTLS for cloud-native applications. [Get access now ›](/demo-request)\n\n## How Does mTLS Work?\n\nSince mTLS is essentially two-way TLS, it’s helpful to understand how TLS works and the system of public key cryptography it relies on to establish trust and secure encryption.\n\nTLS is built on top of TCP as the session layer in the OSI model and provides the encryption for multiple application-layer protocols such as the HTTPS we see when browsing the web (Figure 1).\n\n**Figure 1.** _The unencrypted HTTP protocol vs the HTTPS protocol which uses TLS for encryption._\n\n### Encryption\n\nEncryption is the process of converting plaintext into ciphertext in order to protect it from unauthorized access. The process is done using an encryption algorithm and an encryption key, which is kept secret by the parties involved in the communication.\n\nThe encrypted data, or ciphertext, can only be decrypted and read by someone who has the corresponding decryption key. This allows the original plaintext to be, for example, transmitted securely over a network without the risk of being intercepted and read by unauthorized parties. Encrypting network data is known as “encryption in transit” or “encryption on the wire.” Encrypting data when it is stored, such as on a hard drive or in a cloud storage service, is often called “encryption at rest.” When we talk about encryption in the context of TLS and mTLS, we are talking about encryption in transit.\n\nThere are two main types of encryption: symmetric encryption and asymmetric encryption. In symmetric encryption, the same key is used for both encryption and decryption, while in asymmetric encryption, a different key is used for encryption and decryption. Both symmetric and asymmetric encryption have their own strengths and weaknesses, and are often used together to provide a combination of security and convenience.\n\n### Symmetric Cryptography Using a Secret Key\n\nSymmetric cryptography uses a single secret key to encrypt and decrypt messages. The secret key must be shared between the client and the server, but it must be kept secret (only the client and server  know what it is) since anyone with the secret key can read the messages.Symmetric cryptography is less computationally intensive than asymmetric encryption, but it has some disadvantages, especially with regard to key management. For client-server communication, the key must be shared securely between the client and server—which is hard to do securely over a network, especially across  public networks like the Internet. Also, the shared key must also be kept secret, since anyone with the key can decrypt the ciphertext. For these reasons, **TLS and mTLS use asymmetric encryption to establish a secure channel, then switch to the more efficient symmetric cryptography to encrypt the rest of the communication.**\n\n**Figure 2.** _TLS uses different encryption methods for establishing connections and transmitting data._\n\n### Asymmetric Cryptography Using Pairs of Public and Private Keys\n\nAsymmetric cryptography uses pairs of public and private keys to encrypt and decrypt data. Messages are encrypted with the public key, but can only be decrypted with the corresponding private key. The client and server share their public keys with each other, but keep their private keys private. When the client sends a message to the server, it encrypts the message with the server’s public key and the server uses its private key to decrypt the message. The same thing happens in reverse when the server sends a message to the client: the server encrypts the message with the _client’s_ _public key_ and the client uses its private key to decrypt the message from the server.\n\nSince neither the client nor the server need to share a secret key and their public keys don’t need to be shared securely, asymmetric cryptography works well in hostile environments like the open Internet. But, asymmetric cryptography can be orders of magnitude more computationally intensive than symmetric cryptography.\n\n### TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency\n\nTLS uses a combination of symmetric and asymmetric cryptography to strike a balance between [security](/learn/kubernetes-security-best-practices) and computational efficiency. The more computationally expensive asymmetric encryption is used to generate and exchange shared, secret session keys during the handshake when the TLS connection is established. The shared session keys  are then used by both the client and server to encrypt and decrypt the rest of their communication using less expensive symmetric encryption. The session keys are unique to each communication session, providing additional security by ensuring that each communication session has its own set of keys, limiting the risk of session key compromise. After the communication session is complete, the session keys are discarded and a new set of session keys is generated for each subsequent session.\n\n### Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)\n\nThe public keys used in asymmetric cryptography don’t have to be secret and can be shared publicly. But, we need a way to make sure that public keys are _authentic_—that the server you’re connecting to actually _is_ the server you think it is (and not a bad actor pretending to be that server, also known as a man-in-the-middle attack) and that the public key it gives you isn’t fake. To do this, TLS relies on a system of digitally signed certificates issued by a trusted third party called a certificate authority (CA) to prove the authenticity of the public key and the identity of the server presenting the key.\n\nThis system established to support issuing, validating, and revoking public key certificates is known as a public key infrastructure (PKI) ([Recommendation ITU-T X.509 | ISO/IEC 9594-8](/external-link/)\n\nDuring the TLS handshake, the server presents the certificate to the client to prove the authenticity of the server’s identity and the public key attached to the certificate. The client can then use the public key attached to the certificate to encrypt messages to the server.\n\n### Root CAs and Root Certificates\n\nFor public key infrastructure to work, everyone (and everything) using it must agree on a set of one or more trusted third parties. Those trusted third parties are known as “root CAs”  and they create and publish a self-signed “root certificate.”\n\nTypically, devices have a set of those trusted root certificates securely installed in what’s known as a “root store” or “root CA bundle.”  The presence of a root certificate in the root store of a device (or software installed on the device) establishes trust in the root CA that issued the root certificate.\n\nFor the public Internet, the root CAs are operated by well-known commercial or non-profit organizations, but some organizations operate their own PKI with their own CAs.\n\n### Subordinate CAs and Intermediate Certificates\n\nThe security of the private key held by a root CA is critical, since there is no (practical) way to revoke a root certificate. As a result, root CAs are almost never online. Instead, root CA’s delegate their authority to subordinate CAs by issuing an “intermediate certificate” to the subordinate CA. The validity of intermediate certificates can be traced back to the root CA that issued them and intermediate certificates can be revoked. Subordinate CAs can further delegate their authority to their own subordinate CAs, also by issuing intermediate certificates.\n\n### Leaf Certificates and Certificate Chains\n\nAn “end-entity” or “leaf” certificate is typically issued to individual servers by a CA. These certificates are the “leaves” of a hierarchical tree of authority that’s traceable from the leaf certificates up through the intermediate certificates, and ultimately to a trusted root certificate.\n\nWhen a client connects to the server, the server sends its leaf certificate along with the chain of certificates that can be traced all the way back to a root certificate. The client then validates the certificate, typically with the following checks:\n\n- **Verifying the certificate’s signature** to ensure that the certificate was actually issued by the CA and has not been tampered with.\n- **Checking the certificate’s status:** The client checks the certificate’s expiration date, as well as any other relevant information such as the domain name or the subject’s public key, to ensure that the certificate is still valid.\n- **Checking the certificate chain:** The client follows the links in the certificate chain, starting with the server’s leaf certificate, and checking each intermediate certificate in the chain. It uses the information in each certificate to [verify](/external-link/)\n- **Validating the root certificate:** The client validates the trusted root certificate by checking that it is in its list of trusted root CAs. This ensures that the root certificate is a trusted and authoritative source of information about other certificates.\n\n### CA Bundle\n\nA TLS client has a list of trusted root certificates installed in a “CA bundle”. The client uses the CA bundle to verify the signature on the server’s certificate and determine if it was issued by a trusted CA. If the signature is verified, the client can trust that the certificate belongs to the server it is communicating with.\n\n### Certificate Revocation\n\nIn a public key infrastructure, certificate revocation is a process that allows a CA to declare a certificate as invalid before its expiration date. This is done when a certificate is no longer considered trustworthy, such as when the private key associated with the certificate has been compromised or the subject of the certificate is no longer authorized to use it.\n\nClients can ask the CA if the certificate offered by a server is still valid. Two commonly used techniques are  certificate revocation lists (CRL) and the newer online certificate status protocol (OCSP). For CRLs, a CA publishes a signed list of revoked certificates that a client can check to ensure a server’s certificate is still valid.  With OCSP, a client can request the status of a particular certificate.\n\nBy allowing certificates to be declared invalid when they are no longer trustworthy, certificate revocation helps to protect against security risks and ensure the confidentiality and privacy of sensitive information transmitted over secure connections.\n\n### TLS Handshake\n\nWhen a client initiates a secure connection with a server, they perform a TLS handshake to negotiate the TLS protocol to use, establish the identity of the server, and to generate and share session keys that will be used for symmetric encryption of the subsequent messages in the session.\n\nFigure 3 below represents a simplified version of what happens during the TLS handshake.\n\n**Figure 3.** _Simplified TLS handshake flow._\n\n1.  A request from the client to the server containing information such as the TLS version and password combination supported by the client.The server responds to the client request and attaches a digital certificate.\n2.  The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. The client and server agree on and exchange a shared, secret session key.\n3.  Encrypted communication commences between the client and the server using a symmetric encryption with the shared secret key.\n\n## How Is mTLS Different from TLS and SSL?\n\nmTLS extends the security provided by TLS by adding mutual authentication between the client and the server. In mTLS, both the client and the server present their own certificates to each other and verify the identity of each other before establishing a secure connection.\n\nIn contrast, TLS (and its predecessor SSL) only provides authentication of the server to the client, which is sufficient for many use cases where the client trusts the server and wants to verify the identity of the server before sending sensitive data.\n\nTLS is an IETF standard that evolved from secure sockets layer (SSL) developed by Netscape in the 1990s and are closely related. The two terms, TLS and SSL, are often used interchangeably, though SSL has been deprecated due to security issues in favor of TLS.\n\n## Why Do I Need mTLS?\n\nmTLS is a crucial component of a [zero trust architecture](/zero-trust). One of the tenets of zero trust networking is to assume that an attacker is already on your network. To limit the “blast radius” of an intrusion, it’s important to prevent the intruder from pivoting to other resources on your network. mTLS connections limit reconnaissance and provide for the authenticity of communication so that an intruder can’t eavesdrop, alter messages, impersonate resources, or otherwise intercept messages on your network.\n\n## When Do I Need mTLS?\n\nAs part of a zero trust security posture, you should use mTLS for network communication between application components that you have some control over—like between microservices in a cluster.\n\nOne-way TLS is typically used by Internet clients to connect to web services, which means that only the server needs to show identification and is unconcerned with the identity of the client. One-way TLS allows you to use passwords, tokens, two-factor authentication, and other methods when you need to confirm the identity of the client. However, when using a supporting technology like a service mesh, mTLS operates outside the application and doesn’t require many changes to the application logic to implement.\n\nSince mTLS implementation calls for certificate exchange between services, as the number of services rises, managing numerous certificates becomes a laborious task. You can implement automatic mTLS to mitigate the complexity of certificate management with the aid of a service mesh.\n\n## When Don’t I Need mTLS?\n\nAlthough mTLS is the preferred protocol for securing inter-service communication in cloud-native applications, implementing mTLS is more complex. In some cases where there is high traffic volume or CPU utilization must be optimized, terminatingTLS at the traffic entry point and turning on mTLS internally only for specific, security-sensitive services can help minimize request response times and decrease compute resource consumption for some traffic with lower security requirements.\n\nThere may be cases where mTLS for certain connections is impractical, such as health checks or access to external services.\n\n## How Do I Implement mTLS?\n\n### The Hard Part of mTLS: Proving Identity\n\nWhile mTLS offers significant security advantages, it offers some implementation challenges, not least of which is establishing a secure mechanism for services to prove their identity to each other.\n\nFor regular TLS, it used to be hard to manage the certificates that prove the identity of a server to its clients. With the advent of [Let’s Encrypt](/external-link/)\n\nRolling your own automated certificate management system is impractical and risky. Getting mTLS certificate management right is hard and the consequences of getting it wrong are bad. You need a trusted, proven way to do it. A service mesh is purpose-built to provide the infrastructure you need to safely and securely implement mTLS between services.\n\n### Use a Service Mesh, the NIST Standard for Microservices Security\n\nIn its [standards for microservices security](/external-link/)\n\nIf you want the details of NIST’s standards for microservices security and how Tetrate helps meet them, check out [Tetrate’s Guide to Federal Security Requirements for Microservices](/resource/tetrate-guide-to-federal-security-requirements-for-microservices).\n\n## How Do I Implement mTLS with Istio?\n\nFigure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\n\n**Figure 4.** _Istio mTLS flow_.\n\nIstio includes a built-in CA, and [Secret Discovery Service (SDS)](/external-link/)\n\n1.  The sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and _istiod_ issues the [SVID](/external-link/)\n2.  The sidecar of every workload intercepts all client requests within the pod.\n3.  The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the [JWT](/external-link/)\n4.  If the request is authenticated and authorized, the client and the server start to establish a connection for communication.\n\nIn Istio, authentication and authorization between services can be configured using one of three resource objects:\n\n- [**RequestAuthentication**](/external-link/)\n- [**PeerAuthentication**](/external-link/)\n- [**AuthorizationPolicy**](/external-link/)\n\n### How to Enable Automatic mTLS in Istio\n\nIn [Istio’s PeerAuthentication configuration](/external-link/)\n\n- PERMISSIVE: The workload’s default setting that allows it to accept either mTLS or plain text traffic.\n- STRICT: The workload accepts only mTLS traffic.\n- DISABLE: Disable mTLS. From a security perspective, mTLS should not be disabled unless you have your own security solution.\n- UNSET: Inherited from the parent, with the following priority: service specific > namespace scope > mesh scope setting.\n\nIstio’s peer authentication uses PERMISSIVE mode by default, automatically sending mTLS traffic to these workloads and clear text traffic to workloads without a sidecar. After including Kubernetes services in the Istio mesh, we can use PERMISSIVE mode first to prevent services from failing mTLS. We can use one of two ways to enable strict mTLS mode for certain services:\n\n- Use PeerAuthentication to define how traffic is transferred between sidecars.\n- Use DestinationRule to define the TLS settings in the traffic routing policy.\n\nThe reviews service’s mTLS configuration in the default namespace can be seen in the example below.\n\n### Use PeerAuthentication to Set mTLS for Workloads\n\nFor instance, the following configuration can be used to specify that a workload under a namespace has strict mTLS enabled.\n\n```\napiVersion: security.istio.io/v1beta1\n kind: PeerAuthentication\n metadata:\n  name: foo-peer-policy\n  namespace: default\n spec:\n  selector:\n    matchLabels:\n      app: reviews\n  mtls:\n    mode: STRICT\n```\n\nAccording to the [Istio documentation](/external-link/)\n\n### Use DestinationRule to Set up mTLS for Workloads\n\nTraffic routing policies, such as load balancing, anomaly detection, TLS settings, etc., are set using DestinationRule. In the TLS settings, there are various modes. As shown below, use `ISTIO_MUTUAL` mode to enable Istio’s workload-based automatic TLS.\n\n```\napiVersion: networking.istio.io/v1beta1\n kind: DestinationRule\n metadata:\n  name: reviews\n  namespace: default\n spec:\n  host: reviews\n  trafficPolicy:\n    tls:\n      mode: ISTIO_MUTUAL\n```\n\n## Service Mesh mTLS Best Practices for Enterprise Deployments\n\nIstio is the standard for implementing mTLS in a cloud-native environment, but there are some best practices that will help make your deployment of  mTLS in an enterprise environment more secure.\n\n### Best Practice: Don’t Use Self-Signed Certificates\n\nWhile Istio will implement mTLS for you, it uses self-signed certificates by default so you can see the mesh working right away, with minimal configuration. This makes the initial user experience easy, but it’s not not suitable for production environments. NIST’s guidance (NIST SP 800-204A, SM-DR12) is to disable the ability to generate self-signed certificates entirely.\n\n### Best Practice: Root Istio’s Trust in Your Existing PKI\n\nIf you’re not supposed to use Istio’s default self-signed certificates, what’s the alternative? The short answer is that you should [root Istio’s trust in your existing public key infrastructure (PKI)](/external-link/)\n\n### Best Practice: Use an Intermediate Certificate\n\nHow, exactly, do you root Istio’s trust in your existing PKI? Tetrate founding engineer and co-author of NIST’s security standards for microservices, Zack Butcher, [has all the details here](/external-link/)\n\n- Allow for fine-grained cert revocation without forcing new certificates across your entire infrastructure at the same time.\n- Enable easy rotation of signing certificates.\n\nFor step-by-step instructions on how to automate [Istio certificate](/blog/how-are-certificates-managed-in-istio) authority (CA) rotation, see our article on [automating Istio CA rotation in production at scale](/blog/automate-istio-ca-rotation-in-production-at-scale).\n\n## Is mTLS All I Need for Zero Trust Security?\n\nIn short, zero trust security is more than just mTLS, although mTLS is an important part of a zero trust architecture, especially for microservices. Zero trust networking is an approach governed by a few important principles more than it is a particular technology. In a zero trust network, all access to resources should be:\n\n- **Authenticated and dynamically authorized**, not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.\n- **Bounded in time:** authentication and authorization are bound to a short-lived session after which they must be re-established.\n- **Bounded in space:** the perimeter of trust around a service should be as small as possible.\n- **Encrypted**, both to prevent eavesdropping and to ensure messages are authentic and\n- unaltered.\n- **Observable**, so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.\n\nImplementing mTLS between resources like microservices in Kubernetes cluster provides authentication and encryption, but doesn’t address the rest of the full scope of a zero trust architecture. To learn more, [download our zero trust architecture white paper](/resource/zero-trust-architecture-white-paper) written by Zack Butcher, Tetrate founding engineer and co-author of the [NIST security standards for microservices applications](/blog/nist-standards-for-zero-trust-the-sp-800-204-series).","src/content/learn/what-is-mtls/index.md","88f6c3a9e2e82443",{html:649,metadata:650},"<p>Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where <strong>both parties must prove their identities to each other</strong>.  mTLS extends the security provided by <a href=\"/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery\">TLS</a> by adding mutual authentication between the client and the server.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the easiest way to implement mTLS for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h2 id=\"how-does-mtls-work\">How Does mTLS Work?</h2>\n<p>Since mTLS is essentially two-way TLS, it’s helpful to understand how TLS works and the system of public key cryptography it relies on to establish trust and secure encryption.</p>\n<p>TLS is built on top of TCP as the session layer in the OSI model and provides the encryption for multiple application-layer protocols such as the HTTPS we see when browsing the web (Figure 1).</p>\n<p><strong>Figure 1.</strong> <em>The unencrypted HTTP protocol vs the HTTPS protocol which uses TLS for encryption.</em></p>\n<h3 id=\"encryption\">Encryption</h3>\n<p>Encryption is the process of converting plaintext into ciphertext in order to protect it from unauthorized access. The process is done using an encryption algorithm and an encryption key, which is kept secret by the parties involved in the communication.</p>\n<p>The encrypted data, or ciphertext, can only be decrypted and read by someone who has the corresponding decryption key. This allows the original plaintext to be, for example, transmitted securely over a network without the risk of being intercepted and read by unauthorized parties. Encrypting network data is known as “encryption in transit” or “encryption on the wire.” Encrypting data when it is stored, such as on a hard drive or in a cloud storage service, is often called “encryption at rest.” When we talk about encryption in the context of TLS and mTLS, we are talking about encryption in transit.</p>\n<p>There are two main types of encryption: symmetric encryption and asymmetric encryption. In symmetric encryption, the same key is used for both encryption and decryption, while in asymmetric encryption, a different key is used for encryption and decryption. Both symmetric and asymmetric encryption have their own strengths and weaknesses, and are often used together to provide a combination of security and convenience.</p>\n<h3 id=\"symmetric-cryptography-using-a-secret-key\">Symmetric Cryptography Using a Secret Key</h3>\n<p>Symmetric cryptography uses a single secret key to encrypt and decrypt messages. The secret key must be shared between the client and the server, but it must be kept secret (only the client and server  know what it is) since anyone with the secret key can read the messages.Symmetric cryptography is less computationally intensive than asymmetric encryption, but it has some disadvantages, especially with regard to key management. For client-server communication, the key must be shared securely between the client and server—which is hard to do securely over a network, especially across  public networks like the Internet. Also, the shared key must also be kept secret, since anyone with the key can decrypt the ciphertext. For these reasons, <strong>TLS and mTLS use asymmetric encryption to establish a secure channel, then switch to the more efficient symmetric cryptography to encrypt the rest of the communication.</strong></p>\n<p><strong>Figure 2.</strong> <em>TLS uses different encryption methods for establishing connections and transmitting data.</em></p>\n<h3 id=\"asymmetric-cryptography-using-pairs-of-public-and-private-keys\">Asymmetric Cryptography Using Pairs of Public and Private Keys</h3>\n<p>Asymmetric cryptography uses pairs of public and private keys to encrypt and decrypt data. Messages are encrypted with the public key, but can only be decrypted with the corresponding private key. The client and server share their public keys with each other, but keep their private keys private. When the client sends a message to the server, it encrypts the message with the server’s public key and the server uses its private key to decrypt the message. The same thing happens in reverse when the server sends a message to the client: the server encrypts the message with the <em>client’s</em> <em>public key</em> and the client uses its private key to decrypt the message from the server.</p>\n<p>Since neither the client nor the server need to share a secret key and their public keys don’t need to be shared securely, asymmetric cryptography works well in hostile environments like the open Internet. But, asymmetric cryptography can be orders of magnitude more computationally intensive than symmetric cryptography.</p>\n<h3 id=\"tls-uses-both-symmetric-and-asymmetric-cryptography-to-balance-security-and-efficiency\">TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency</h3>\n<p>TLS uses a combination of symmetric and asymmetric cryptography to strike a balance between <a href=\"/learn/kubernetes-security-best-practices\">security</a> and computational efficiency. The more computationally expensive asymmetric encryption is used to generate and exchange shared, secret session keys during the handshake when the TLS connection is established. The shared session keys  are then used by both the client and server to encrypt and decrypt the rest of their communication using less expensive symmetric encryption. The session keys are unique to each communication session, providing additional security by ensuring that each communication session has its own set of keys, limiting the risk of session key compromise. After the communication session is complete, the session keys are discarded and a new set of session keys is generated for each subsequent session.</p>\n<h3 id=\"public-key-certificates-certificate-authorities-ca-and-public-key-infrastructure-pki\">Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)</h3>\n<p>The public keys used in asymmetric cryptography don’t have to be secret and can be shared publicly. But, we need a way to make sure that public keys are <em>authentic</em>—that the server you’re connecting to actually <em>is</em> the server you think it is (and not a bad actor pretending to be that server, also known as a man-in-the-middle attack) and that the public key it gives you isn’t fake. To do this, TLS relies on a system of digitally signed certificates issued by a trusted third party called a certificate authority (CA) to prove the authenticity of the public key and the identity of the server presenting the key.</p>\n<p>This system established to support issuing, validating, and revoking public key certificates is known as a public key infrastructure (PKI) (<a href=\"/external-link/\">Recommendation ITU-T X.509 | ISO/IEC 9594-8</a></p>\n<p>During the TLS handshake, the server presents the certificate to the client to prove the authenticity of the server’s identity and the public key attached to the certificate. The client can then use the public key attached to the certificate to encrypt messages to the server.</p>\n<h3 id=\"root-cas-and-root-certificates\">Root CAs and Root Certificates</h3>\n<p>For public key infrastructure to work, everyone (and everything) using it must agree on a set of one or more trusted third parties. Those trusted third parties are known as “root CAs”  and they create and publish a self-signed “root certificate.”</p>\n<p>Typically, devices have a set of those trusted root certificates securely installed in what’s known as a “root store” or “root CA bundle.”  The presence of a root certificate in the root store of a device (or software installed on the device) establishes trust in the root CA that issued the root certificate.</p>\n<p>For the public Internet, the root CAs are operated by well-known commercial or non-profit organizations, but some organizations operate their own PKI with their own CAs.</p>\n<h3 id=\"subordinate-cas-and-intermediate-certificates\">Subordinate CAs and Intermediate Certificates</h3>\n<p>The security of the private key held by a root CA is critical, since there is no (practical) way to revoke a root certificate. As a result, root CAs are almost never online. Instead, root CA’s delegate their authority to subordinate CAs by issuing an “intermediate certificate” to the subordinate CA. The validity of intermediate certificates can be traced back to the root CA that issued them and intermediate certificates can be revoked. Subordinate CAs can further delegate their authority to their own subordinate CAs, also by issuing intermediate certificates.</p>\n<h3 id=\"leaf-certificates-and-certificate-chains\">Leaf Certificates and Certificate Chains</h3>\n<p>An “end-entity” or “leaf” certificate is typically issued to individual servers by a CA. These certificates are the “leaves” of a hierarchical tree of authority that’s traceable from the leaf certificates up through the intermediate certificates, and ultimately to a trusted root certificate.</p>\n<p>When a client connects to the server, the server sends its leaf certificate along with the chain of certificates that can be traced all the way back to a root certificate. The client then validates the certificate, typically with the following checks:</p>\n<ul>\n<li><strong>Verifying the certificate’s signature</strong> to ensure that the certificate was actually issued by the CA and has not been tampered with.</li>\n<li><strong>Checking the certificate’s status:</strong> The client checks the certificate’s expiration date, as well as any other relevant information such as the domain name or the subject’s public key, to ensure that the certificate is still valid.</li>\n<li><strong>Checking the certificate chain:</strong> The client follows the links in the certificate chain, starting with the server’s leaf certificate, and checking each intermediate certificate in the chain. It uses the information in each certificate to <a href=\"/external-link/\">verify</a></li>\n<li><strong>Validating the root certificate:</strong> The client validates the trusted root certificate by checking that it is in its list of trusted root CAs. This ensures that the root certificate is a trusted and authoritative source of information about other certificates.</li>\n</ul>\n<h3 id=\"ca-bundle\">CA Bundle</h3>\n<p>A TLS client has a list of trusted root certificates installed in a “CA bundle”. The client uses the CA bundle to verify the signature on the server’s certificate and determine if it was issued by a trusted CA. If the signature is verified, the client can trust that the certificate belongs to the server it is communicating with.</p>\n<h3 id=\"certificate-revocation\">Certificate Revocation</h3>\n<p>In a public key infrastructure, certificate revocation is a process that allows a CA to declare a certificate as invalid before its expiration date. This is done when a certificate is no longer considered trustworthy, such as when the private key associated with the certificate has been compromised or the subject of the certificate is no longer authorized to use it.</p>\n<p>Clients can ask the CA if the certificate offered by a server is still valid. Two commonly used techniques are  certificate revocation lists (CRL) and the newer online certificate status protocol (OCSP). For CRLs, a CA publishes a signed list of revoked certificates that a client can check to ensure a server’s certificate is still valid.  With OCSP, a client can request the status of a particular certificate.</p>\n<p>By allowing certificates to be declared invalid when they are no longer trustworthy, certificate revocation helps to protect against security risks and ensure the confidentiality and privacy of sensitive information transmitted over secure connections.</p>\n<h3 id=\"tls-handshake\">TLS Handshake</h3>\n<p>When a client initiates a secure connection with a server, they perform a TLS handshake to negotiate the TLS protocol to use, establish the identity of the server, and to generate and share session keys that will be used for symmetric encryption of the subsequent messages in the session.</p>\n<p>Figure 3 below represents a simplified version of what happens during the TLS handshake.</p>\n<p><strong>Figure 3.</strong> <em>Simplified TLS handshake flow.</em></p>\n<ol>\n<li>A request from the client to the server containing information such as the TLS version and password combination supported by the client.The server responds to the client request and attaches a digital certificate.</li>\n<li>The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. The client and server agree on and exchange a shared, secret session key.</li>\n<li>Encrypted communication commences between the client and the server using a symmetric encryption with the shared secret key.</li>\n</ol>\n<h2 id=\"how-is-mtls-different-from-tls-and-ssl\">How Is mTLS Different from TLS and SSL?</h2>\n<p>mTLS extends the security provided by TLS by adding mutual authentication between the client and the server. In mTLS, both the client and the server present their own certificates to each other and verify the identity of each other before establishing a secure connection.</p>\n<p>In contrast, TLS (and its predecessor SSL) only provides authentication of the server to the client, which is sufficient for many use cases where the client trusts the server and wants to verify the identity of the server before sending sensitive data.</p>\n<p>TLS is an IETF standard that evolved from secure sockets layer (SSL) developed by Netscape in the 1990s and are closely related. The two terms, TLS and SSL, are often used interchangeably, though SSL has been deprecated due to security issues in favor of TLS.</p>\n<h2 id=\"why-do-i-need-mtls\">Why Do I Need mTLS?</h2>\n<p>mTLS is a crucial component of a <a href=\"/zero-trust\">zero trust architecture</a>. One of the tenets of zero trust networking is to assume that an attacker is already on your network. To limit the “blast radius” of an intrusion, it’s important to prevent the intruder from pivoting to other resources on your network. mTLS connections limit reconnaissance and provide for the authenticity of communication so that an intruder can’t eavesdrop, alter messages, impersonate resources, or otherwise intercept messages on your network.</p>\n<h2 id=\"when-do-i-need-mtls\">When Do I Need mTLS?</h2>\n<p>As part of a zero trust security posture, you should use mTLS for network communication between application components that you have some control over—like between microservices in a cluster.</p>\n<p>One-way TLS is typically used by Internet clients to connect to web services, which means that only the server needs to show identification and is unconcerned with the identity of the client. One-way TLS allows you to use passwords, tokens, two-factor authentication, and other methods when you need to confirm the identity of the client. However, when using a supporting technology like a service mesh, mTLS operates outside the application and doesn’t require many changes to the application logic to implement.</p>\n<p>Since mTLS implementation calls for certificate exchange between services, as the number of services rises, managing numerous certificates becomes a laborious task. You can implement automatic mTLS to mitigate the complexity of certificate management with the aid of a service mesh.</p>\n<h2 id=\"when-dont-i-need-mtls\">When Don’t I Need mTLS?</h2>\n<p>Although mTLS is the preferred protocol for securing inter-service communication in cloud-native applications, implementing mTLS is more complex. In some cases where there is high traffic volume or CPU utilization must be optimized, terminatingTLS at the traffic entry point and turning on mTLS internally only for specific, security-sensitive services can help minimize request response times and decrease compute resource consumption for some traffic with lower security requirements.</p>\n<p>There may be cases where mTLS for certain connections is impractical, such as health checks or access to external services.</p>\n<h2 id=\"how-do-i-implement-mtls\">How Do I Implement mTLS?</h2>\n<h3 id=\"the-hard-part-of-mtls-proving-identity\">The Hard Part of mTLS: Proving Identity</h3>\n<p>While mTLS offers significant security advantages, it offers some implementation challenges, not least of which is establishing a secure mechanism for services to prove their identity to each other.</p>\n<p>For regular TLS, it used to be hard to manage the certificates that prove the identity of a server to its clients. With the advent of <a href=\"/external-link/\">Let’s Encrypt</a></p>\n<p>Rolling your own automated certificate management system is impractical and risky. Getting mTLS certificate management right is hard and the consequences of getting it wrong are bad. You need a trusted, proven way to do it. A service mesh is purpose-built to provide the infrastructure you need to safely and securely implement mTLS between services.</p>\n<h3 id=\"use-a-service-mesh-the-nist-standard-for-microservices-security\">Use a Service Mesh, the NIST Standard for Microservices Security</h3>\n<p>In its <a href=\"/external-link/\">standards for microservices security</a></p>\n<p>If you want the details of NIST’s standards for microservices security and how Tetrate helps meet them, check out <a href=\"/resource/tetrate-guide-to-federal-security-requirements-for-microservices\">Tetrate’s Guide to Federal Security Requirements for Microservices</a>.</p>\n<h2 id=\"how-do-i-implement-mtls-with-istio\">How Do I Implement mTLS with Istio?</h2>\n<p>Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.</p>\n<p><strong>Figure 4.</strong> <em>Istio mTLS flow</em>.</p>\n<p>Istio includes a built-in CA, and <a href=\"/external-link/\">Secret Discovery Service (SDS)</a></p>\n<ol>\n<li>The sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and <em>istiod</em> issues the <a href=\"/external-link/\">SVID</a></li>\n<li>The sidecar of every workload intercepts all client requests within the pod.</li>\n<li>The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the <a href=\"/external-link/\">JWT</a></li>\n<li>If the request is authenticated and authorized, the client and the server start to establish a connection for communication.</li>\n</ol>\n<p>In Istio, authentication and authorization between services can be configured using one of three resource objects:</p>\n<ul>\n<li><a href=\"/external-link/\"><strong>RequestAuthentication</strong></a></li>\n<li><a href=\"/external-link/\"><strong>PeerAuthentication</strong></a></li>\n<li><a href=\"/external-link/\"><strong>AuthorizationPolicy</strong></a></li>\n</ul>\n<h3 id=\"how-to-enable-automatic-mtls-in-istio\">How to Enable Automatic mTLS in Istio</h3>\n<p>In <a href=\"/external-link/\">Istio’s PeerAuthentication configuration</a></p>\n<ul>\n<li>PERMISSIVE: The workload’s default setting that allows it to accept either mTLS or plain text traffic.</li>\n<li>STRICT: The workload accepts only mTLS traffic.</li>\n<li>DISABLE: Disable mTLS. From a security perspective, mTLS should not be disabled unless you have your own security solution.</li>\n<li>UNSET: Inherited from the parent, with the following priority: service specific > namespace scope > mesh scope setting.</li>\n</ul>\n<p>Istio’s peer authentication uses PERMISSIVE mode by default, automatically sending mTLS traffic to these workloads and clear text traffic to workloads without a sidecar. After including Kubernetes services in the Istio mesh, we can use PERMISSIVE mode first to prevent services from failing mTLS. We can use one of two ways to enable strict mTLS mode for certain services:</p>\n<ul>\n<li>Use PeerAuthentication to define how traffic is transferred between sidecars.</li>\n<li>Use DestinationRule to define the TLS settings in the traffic routing policy.</li>\n</ul>\n<p>The reviews service’s mTLS configuration in the default namespace can be seen in the example below.</p>\n<h3 id=\"use-peerauthentication-to-set-mtls-for-workloads\">Use PeerAuthentication to Set mTLS for Workloads</h3>\n<p>For instance, the following configuration can be used to specify that a workload under a namespace has strict mTLS enabled.</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: security.istio.io/v1beta1</span></span>\n<span class=\"line\"><span> kind: PeerAuthentication</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: foo-peer-policy</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  selector:</span></span>\n<span class=\"line\"><span>    matchLabels:</span></span>\n<span class=\"line\"><span>      app: reviews</span></span>\n<span class=\"line\"><span>  mtls:</span></span>\n<span class=\"line\"><span>    mode: STRICT</span></span></code></pre>\n<p>According to the <a href=\"/external-link/\">Istio documentation</a></p>\n<h3 id=\"use-destinationrule-to-set-up-mtls-for-workloads\">Use DestinationRule to Set up mTLS for Workloads</h3>\n<p>Traffic routing policies, such as load balancing, anomaly detection, TLS settings, etc., are set using DestinationRule. In the TLS settings, there are various modes. As shown below, use <code>ISTIO_MUTUAL</code> mode to enable Istio’s workload-based automatic TLS.</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: networking.istio.io/v1beta1</span></span>\n<span class=\"line\"><span> kind: DestinationRule</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: reviews</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  host: reviews</span></span>\n<span class=\"line\"><span>  trafficPolicy:</span></span>\n<span class=\"line\"><span>    tls:</span></span>\n<span class=\"line\"><span>      mode: ISTIO_MUTUAL</span></span></code></pre>\n<h2 id=\"service-mesh-mtls-best-practices-for-enterprise-deployments\">Service Mesh mTLS Best Practices for Enterprise Deployments</h2>\n<p>Istio is the standard for implementing mTLS in a cloud-native environment, but there are some best practices that will help make your deployment of  mTLS in an enterprise environment more secure.</p>\n<h3 id=\"best-practice-dont-use-self-signed-certificates\">Best Practice: Don’t Use Self-Signed Certificates</h3>\n<p>While Istio will implement mTLS for you, it uses self-signed certificates by default so you can see the mesh working right away, with minimal configuration. This makes the initial user experience easy, but it’s not not suitable for production environments. NIST’s guidance (NIST SP 800-204A, SM-DR12) is to disable the ability to generate self-signed certificates entirely.</p>\n<h3 id=\"best-practice-root-istios-trust-in-your-existing-pki\">Best Practice: Root Istio’s Trust in Your Existing PKI</h3>\n<p>If you’re not supposed to use Istio’s default self-signed certificates, what’s the alternative? The short answer is that you should <a href=\"/external-link/\">root Istio’s trust in your existing public key infrastructure (PKI)</a></p>\n<h3 id=\"best-practice-use-an-intermediate-certificate\">Best Practice: Use an Intermediate Certificate</h3>\n<p>How, exactly, do you root Istio’s trust in your existing PKI? Tetrate founding engineer and co-author of NIST’s security standards for microservices, Zack Butcher, <a href=\"/external-link/\">has all the details here</a></p>\n<ul>\n<li>Allow for fine-grained cert revocation without forcing new certificates across your entire infrastructure at the same time.</li>\n<li>Enable easy rotation of signing certificates.</li>\n</ul>\n<p>For step-by-step instructions on how to automate <a href=\"/blog/how-are-certificates-managed-in-istio\">Istio certificate</a> authority (CA) rotation, see our article on <a href=\"/blog/automate-istio-ca-rotation-in-production-at-scale\">automating Istio CA rotation in production at scale</a>.</p>\n<h2 id=\"is-mtls-all-i-need-for-zero-trust-security\">Is mTLS All I Need for Zero Trust Security?</h2>\n<p>In short, zero trust security is more than just mTLS, although mTLS is an important part of a zero trust architecture, especially for microservices. Zero trust networking is an approach governed by a few important principles more than it is a particular technology. In a zero trust network, all access to resources should be:</p>\n<ul>\n<li><strong>Authenticated and dynamically authorized</strong>, not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.</li>\n<li><strong>Bounded in time:</strong> authentication and authorization are bound to a short-lived session after which they must be re-established.</li>\n<li><strong>Bounded in space:</strong> the perimeter of trust around a service should be as small as possible.</li>\n<li><strong>Encrypted</strong>, both to prevent eavesdropping and to ensure messages are authentic and</li>\n<li>unaltered.</li>\n<li><strong>Observable</strong>, so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.</li>\n</ul>\n<p>Implementing mTLS between resources like microservices in Kubernetes cluster provides authentication and encryption, but doesn’t address the rest of the full scope of a zero trust architecture. To learn more, <a href=\"/resource/zero-trust-architecture-white-paper\">download our zero trust architecture white paper</a> written by Zack Butcher, Tetrate founding engineer and co-author of the <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series\">NIST security standards for microservices applications</a>.</p>",{headings:651,localImagePaths:736,remoteImagePaths:737,frontmatter:738,imagePaths:741},[652,655,658,661,664,667,670,673,676,679,682,685,688,691,694,697,700,703,706,709,712,715,718,721,724,727,730,733],{depth:29,slug:653,text:654},"how-does-mtls-work","How Does mTLS Work?",{depth:327,slug:656,text:657},"encryption","Encryption",{depth:327,slug:659,text:660},"symmetric-cryptography-using-a-secret-key","Symmetric Cryptography Using a Secret Key",{depth:327,slug:662,text:663},"asymmetric-cryptography-using-pairs-of-public-and-private-keys","Asymmetric Cryptography Using Pairs of Public and Private Keys",{depth:327,slug:665,text:666},"tls-uses-both-symmetric-and-asymmetric-cryptography-to-balance-security-and-efficiency","TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency",{depth:327,slug:668,text:669},"public-key-certificates-certificate-authorities-ca-and-public-key-infrastructure-pki","Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)",{depth:327,slug:671,text:672},"root-cas-and-root-certificates","Root CAs and Root Certificates",{depth:327,slug:674,text:675},"subordinate-cas-and-intermediate-certificates","Subordinate CAs and Intermediate Certificates",{depth:327,slug:677,text:678},"leaf-certificates-and-certificate-chains","Leaf Certificates and Certificate Chains",{depth:327,slug:680,text:681},"ca-bundle","CA Bundle",{depth:327,slug:683,text:684},"certificate-revocation","Certificate Revocation",{depth:327,slug:686,text:687},"tls-handshake","TLS Handshake",{depth:29,slug:689,text:690},"how-is-mtls-different-from-tls-and-ssl","How Is mTLS Different from TLS and SSL?",{depth:29,slug:692,text:693},"why-do-i-need-mtls","Why Do I Need mTLS?",{depth:29,slug:695,text:696},"when-do-i-need-mtls","When Do I Need mTLS?",{depth:29,slug:698,text:699},"when-dont-i-need-mtls","When Don’t I Need mTLS?",{depth:29,slug:701,text:702},"how-do-i-implement-mtls","How Do I Implement mTLS?",{depth:327,slug:704,text:705},"the-hard-part-of-mtls-proving-identity","The Hard Part of mTLS: Proving Identity",{depth:327,slug:707,text:708},"use-a-service-mesh-the-nist-standard-for-microservices-security","Use a Service Mesh, the NIST Standard for Microservices Security",{depth:29,slug:710,text:711},"how-do-i-implement-mtls-with-istio","How Do I Implement mTLS with Istio?",{depth:327,slug:713,text:714},"how-to-enable-automatic-mtls-in-istio","How to Enable Automatic mTLS in Istio",{depth:327,slug:716,text:717},"use-peerauthentication-to-set-mtls-for-workloads","Use PeerAuthentication to Set mTLS for Workloads",{depth:327,slug:719,text:720},"use-destinationrule-to-set-up-mtls-for-workloads","Use DestinationRule to Set up mTLS for Workloads",{depth:29,slug:722,text:723},"service-mesh-mtls-best-practices-for-enterprise-deployments","Service Mesh mTLS Best Practices for Enterprise Deployments",{depth:327,slug:725,text:726},"best-practice-dont-use-self-signed-certificates","Best Practice: Don’t Use Self-Signed Certificates",{depth:327,slug:728,text:729},"best-practice-root-istios-trust-in-your-existing-pki","Best Practice: Root Istio’s Trust in Your Existing PKI",{depth:327,slug:731,text:732},"best-practice-use-an-intermediate-certificate","Best Practice: Use an Intermediate Certificate",{depth:29,slug:734,text:735},"is-mtls-all-i-need-for-zero-trust-security","Is mTLS All I Need for Zero Trust Security?",[],[],{title:640,slug:637,date:739,description:644,categories:740,excerpt:641},["Date","2025-04-16T10:10:53.000Z"],[17],[],"what-is-mtls/index.md","what-is-envoy-gateway",{id:743,data:745,body:751,filePath:752,digest:753,rendered:754,legacyId:778},{title:746,excerpt:747,categories:748,date:749,description:750,draft:20},"What Is Envoy Gateway?","> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, Tetrate Enterprise Gateway for Envoy (TEG)(/tetrate-enterprise-gateway-for-envoy/). TEG is the easiest way to get started with Envoy for production use cases. Get access now.",[312],["Date","2024-10-08T07:50:50.000Z"],"Overview Envoy Gateway is an open source project that aims to make it simple to use Envoy Proxy as an API gateway by delivering a simplified deployment model and API…","## Overview\n\n[Envoy Gateway](/external-link/)\n\n> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy). TEG is the easiest way to get started with Envoy for production use cases. [Get access now ›](/demo-request)\n\n\n\nEnvoy Proxy is a high-performance open-source proxy server designed for cloud-native applications. It was initially created by Lyft in 2016, where it was used as an API gateway and edge proxy, offering observability insights [that aided Lyft’s transition](/external-link/)\n\nWhile Envoy Proxy is typically used in conjunction with other technologies like [the Istio service mesh](/what-is-istio-service-mesh), Envoy Gateway is a great way to get started with Envoy on its own as a cloud-native application gateway—especially for those coming from traditional API gateway technologies looking to adopt a more modern approach.\n\nEnvoy Gateway provides a suite of services and features including:\n\n- An [xDS](/external-link/)\n- An expressive API based on the Kubernetes Gateway API, with reasonable settings to simplify the Envoy user experience.\n- Support for heterogeneous environments (although, early versions focus on deployment in Kubernetes).\n- Extensibility to support a multitude of application gateway use cases.\n- Envoy infrastructure provisioning and management.\n- High-quality documentation, tooling, and a diverse group of project maintainers for support.\n\nEnvoy Gateway makes it easy for organizations to leverage the power of Envoy for “north-south” traffic. With its support for multiple user personas, organizations can leave their existing operational models unchanged. For example, **infrastructure admins** can use Envoy Gateway to provision and manage fleets of Envoys while **application developers** can simply route application traffic to their backend services. In addition, Envoy Gateway implements the Kubernetes Gateway API which aims to standardize and improve all the Kubernetes APIs that are currently used for ingress.\n\n## Benefits of Envoy Gateway\n\nUsing Envoy Gateway provides several benefits for microservices and cloud-native applications, including:\n\n**High performance.** Envoy Gateway is built on top of the high-performance Envoy proxy, which can handle millions of requests per second. This makes it an ideal choice for high-traffic APIs.\n\n**Extensibility.** Envoy has a flexible architecture and supports custom filters, allowing developers to extend its functionality according to their specific needs.\n\n**Dynamic configuration.** Envoy supports dynamic service discovery and configuration updates, which allows it to adapt to changes in the system without the need for manual intervention or downtime.\n\n**Security.** Envoy Gateway supports a variety of security features, such as Transport Layer Security (TLS), TLS pass-through, and secure gRPC. It also provides features for authentication and rate-limiting to protect backend services from unauthorized access and potential attacks.\n\n**Protocol support.** Envoy Gateway supports a wide range of protocols, including HTTP/1.x, HTTP/2, gRPC, and TCP. This makes it suitable for various types of applications and services.\n\n**Seamless integration.** Envoy Gateway can be easily integrated into existing infrastructure, including Kubernetes, service meshes, and other orchestration platforms. This seamless integration ensures a smoother transition and reduced operational overhead when adopting Envoy.\n\nEnvoy Gateway makes it easy to get started using a modern application gateway and a great stepping stone towards adopting cloud-native architectures like service mesh.\n\n## Envoy Gateway Project Goals\n\nThe core of Envoy Proxy is inherently low-level, designed to be fast, flexible, capable, and extensible, but not necessarily easy to use by itself. Envoy Gateway was born to [“bring Envoy to the masses”](/external-link/)\n\n**A streamlined API tailored for gateway use cases.** Envoy Gateway uses the Kubernetes Gateway API with Envoy-specific extensions. This decision is based on the project’s initial focus on deployment within Kubernetes as an ingress controller plus the broad buy-in of the Gateway API.\n\n**A comprehensive, ready-to-use experience.** This approach enables users to quickly set up and start using Envoy Gateway. It encompasses lifecycle management functionalities that handle controller resources, control plane resources, proxy instances, and more.\n\n**A flexible and expandable API surface.** While the project aims to provide out-of-the-box functionality for common API gateway features (e.g., rate limiting, authentication, Let’s Encrypt integration), vendors can offer SaaS versions of all APIs, introduce additional APIs, and incorporate value-added features such as Web Application Firewall (WAF), enhanced observability, chaos engineering, etc.\n\n**High-quality documentation and beginner-friendly guides.** The primary goal of Envoy Gateway is to simplify the process of setting up and using the most prevalent gateway use cases for the average user. This is achieved by providing comprehensive documentation and easy-to-follow getting started guides.\n\n## Envoy Gateway vs Traditional API Gateway\n\nThe main difference between Envoy Gateway and a traditional API Gateway is the architecture and the way they handle API traffic.\n\nTraditional API Gateways are often monolithic systems that handle all API traffic in a centralized manner. They are responsible for managing API routing, authentication, authorization, rate limiting, and other functions. Traditional API Gateways can become a bottleneck in high-traffic scenarios, and they can be difficult to scale.\n\nOn the other hand, Envoy Gateway is designed to be a lightweight, modular component that can be easily deployed in front of microservices. It leverages the power of Envoy to provide API Gateway capabilities such as routing, authentication, and rate limiting, but it can also be extended with custom filters to build additional functionality.\n\nEnvoy Gateway is a more flexible and scalable alternative to traditional API Gateways, designed to handle API traffic in a distributed, microservices-based architecture.\n\n## Get Enterprise Support for Your Envoy Gateway Deployment\n\n[Tetrate Enterprise Gateway for Envoy](/tetrate-enterprise-gateway-for-envoy) provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.\n\nTetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.\n\n- [Get access now ›](/demo-request)\n- [Learn more ›](/tetrate-enterprise-gateway-for-envoy)\n- [Quick start ›](/quick-start)\n\n## Get Started with Envoy Gateway\n\nGetting started with Envoy Gateway is easy. Go to the documentation on [the Envoy Gateway project site](/external-link/)\n\n- [Quick start ›](/quick-start)\n- [User guides ›](/user-guides)","src/content/learn/what-is-envoy-gateway/index.md","9632f3425cef7d05",{html:755,metadata:756},"<h2 id=\"overview\">Overview</h2>\n<p><a href=\"/external-link/\">Envoy Gateway</a></p>\n<blockquote>\n<p>Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a>. TEG is the easiest way to get started with Envoy for production use cases. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<p>Envoy Proxy is a high-performance open-source proxy server designed for cloud-native applications. It was initially created by Lyft in 2016, where it was used as an API gateway and edge proxy, offering observability insights <a href=\"/external-link/\">that aided Lyft’s transition</a></p>\n<p>While Envoy Proxy is typically used in conjunction with other technologies like <a href=\"/what-is-istio-service-mesh\">the Istio service mesh</a>, Envoy Gateway is a great way to get started with Envoy on its own as a cloud-native application gateway—especially for those coming from traditional API gateway technologies looking to adopt a more modern approach.</p>\n<p>Envoy Gateway provides a suite of services and features including:</p>\n<ul>\n<li>An <a href=\"/external-link/\">xDS</a></li>\n<li>An expressive API based on the Kubernetes Gateway API, with reasonable settings to simplify the Envoy user experience.</li>\n<li>Support for heterogeneous environments (although, early versions focus on deployment in Kubernetes).</li>\n<li>Extensibility to support a multitude of application gateway use cases.</li>\n<li>Envoy infrastructure provisioning and management.</li>\n<li>High-quality documentation, tooling, and a diverse group of project maintainers for support.</li>\n</ul>\n<p>Envoy Gateway makes it easy for organizations to leverage the power of Envoy for “north-south” traffic. With its support for multiple user personas, organizations can leave their existing operational models unchanged. For example, <strong>infrastructure admins</strong> can use Envoy Gateway to provision and manage fleets of Envoys while <strong>application developers</strong> can simply route application traffic to their backend services. In addition, Envoy Gateway implements the Kubernetes Gateway API which aims to standardize and improve all the Kubernetes APIs that are currently used for ingress.</p>\n<h2 id=\"benefits-of-envoy-gateway\">Benefits of Envoy Gateway</h2>\n<p>Using Envoy Gateway provides several benefits for microservices and cloud-native applications, including:</p>\n<p><strong>High performance.</strong> Envoy Gateway is built on top of the high-performance Envoy proxy, which can handle millions of requests per second. This makes it an ideal choice for high-traffic APIs.</p>\n<p><strong>Extensibility.</strong> Envoy has a flexible architecture and supports custom filters, allowing developers to extend its functionality according to their specific needs.</p>\n<p><strong>Dynamic configuration.</strong> Envoy supports dynamic service discovery and configuration updates, which allows it to adapt to changes in the system without the need for manual intervention or downtime.</p>\n<p><strong>Security.</strong> Envoy Gateway supports a variety of security features, such as Transport Layer Security (TLS), TLS pass-through, and secure gRPC. It also provides features for authentication and rate-limiting to protect backend services from unauthorized access and potential attacks.</p>\n<p><strong>Protocol support.</strong> Envoy Gateway supports a wide range of protocols, including HTTP/1.x, HTTP/2, gRPC, and TCP. This makes it suitable for various types of applications and services.</p>\n<p><strong>Seamless integration.</strong> Envoy Gateway can be easily integrated into existing infrastructure, including Kubernetes, service meshes, and other orchestration platforms. This seamless integration ensures a smoother transition and reduced operational overhead when adopting Envoy.</p>\n<p>Envoy Gateway makes it easy to get started using a modern application gateway and a great stepping stone towards adopting cloud-native architectures like service mesh.</p>\n<h2 id=\"envoy-gateway-project-goals\">Envoy Gateway Project Goals</h2>\n<p>The core of Envoy Proxy is inherently low-level, designed to be fast, flexible, capable, and extensible, but not necessarily easy to use by itself. Envoy Gateway was born to <a href=\"/external-link/\">“bring Envoy to the masses”</a></p>\n<p><strong>A streamlined API tailored for gateway use cases.</strong> Envoy Gateway uses the Kubernetes Gateway API with Envoy-specific extensions. This decision is based on the project’s initial focus on deployment within Kubernetes as an ingress controller plus the broad buy-in of the Gateway API.</p>\n<p><strong>A comprehensive, ready-to-use experience.</strong> This approach enables users to quickly set up and start using Envoy Gateway. It encompasses lifecycle management functionalities that handle controller resources, control plane resources, proxy instances, and more.</p>\n<p><strong>A flexible and expandable API surface.</strong> While the project aims to provide out-of-the-box functionality for common API gateway features (e.g., rate limiting, authentication, Let’s Encrypt integration), vendors can offer SaaS versions of all APIs, introduce additional APIs, and incorporate value-added features such as Web Application Firewall (WAF), enhanced observability, chaos engineering, etc.</p>\n<p><strong>High-quality documentation and beginner-friendly guides.</strong> The primary goal of Envoy Gateway is to simplify the process of setting up and using the most prevalent gateway use cases for the average user. This is achieved by providing comprehensive documentation and easy-to-follow getting started guides.</p>\n<h2 id=\"envoy-gateway-vs-traditional-api-gateway\">Envoy Gateway vs Traditional API Gateway</h2>\n<p>The main difference between Envoy Gateway and a traditional API Gateway is the architecture and the way they handle API traffic.</p>\n<p>Traditional API Gateways are often monolithic systems that handle all API traffic in a centralized manner. They are responsible for managing API routing, authentication, authorization, rate limiting, and other functions. Traditional API Gateways can become a bottleneck in high-traffic scenarios, and they can be difficult to scale.</p>\n<p>On the other hand, Envoy Gateway is designed to be a lightweight, modular component that can be easily deployed in front of microservices. It leverages the power of Envoy to provide API Gateway capabilities such as routing, authentication, and rate limiting, but it can also be extended with custom filters to build additional functionality.</p>\n<p>Envoy Gateway is a more flexible and scalable alternative to traditional API Gateways, designed to handle API traffic in a distributed, microservices-based architecture.</p>\n<h2 id=\"get-enterprise-support-for-your-envoy-gateway-deployment\">Get Enterprise Support for Your Envoy Gateway Deployment</h2>\n<p><a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy</a> provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.</p>\n<p>Tetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.</p>\n<ul>\n<li><a href=\"/demo-request\">Get access now ›</a></li>\n<li><a href=\"/tetrate-enterprise-gateway-for-envoy\">Learn more ›</a></li>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n</ul>\n<h2 id=\"get-started-with-envoy-gateway\">Get Started with Envoy Gateway</h2>\n<p>Getting started with Envoy Gateway is easy. Go to the documentation on <a href=\"/external-link/\">the Envoy Gateway project site</a></p>\n<ul>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n<li><a href=\"/user-guides\">User guides ›</a></li>\n</ul>",{headings:757,localImagePaths:772,remoteImagePaths:773,frontmatter:774,imagePaths:777},[758,759,762,765,768,769],{depth:29,slug:158,text:159},{depth:29,slug:760,text:761},"benefits-of-envoy-gateway","Benefits of Envoy Gateway",{depth:29,slug:763,text:764},"envoy-gateway-project-goals","Envoy Gateway Project Goals",{depth:29,slug:766,text:767},"envoy-gateway-vs-traditional-api-gateway","Envoy Gateway vs Traditional API Gateway",{depth:29,slug:628,text:629},{depth:29,slug:770,text:771},"get-started-with-envoy-gateway","Get Started with Envoy Gateway",[],[],{title:746,slug:743,date:775,description:750,categories:776,excerpt:747},["Date","2024-10-08T07:50:50.000Z"],[312],[],"what-is-envoy-gateway/index.md","what-is-platform-team",{id:779,data:781,body:787,filePath:788,digest:789,rendered:790,legacyId:846},{title:782,excerpt:783,categories:784,date:785,description:786,draft:20},"What Is a Platform Team?","A platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software.",[17],["Date","2025-04-18T10:02:55.000Z"],"Explore Tetrate's guide to platform teams: their role, functions, and significance in modern software development for scalability and efficiency.","## Overview\n\nA platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software [applications](/external-link/)\n\nThe platform team typically includes software engineers, systems administrators, network engineers, database administrators, security experts and other specialists who work together to ensure the platform is secure, scalable and reliable. The platform team is responsible for the technical aspects of the platform, including [architecture](/learn/kubernetes-security-architecture) design, system configuration, coding, testing, deployment and ongoing maintenance.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. [Get access now ›](/demo-request)\n\n### Origins of Platform Engineering\n\nPlatform engineering can be traced back to the rise of cloud computing and the need for scalable and reliable infrastructure to support software applications.\n\nAs organizations began to move their applications and data to the cloud, they saw the need for standardized and scalable infrastructure that could support the rapid development and deployment of software applications. This led to the development of platform-as-a-service (PaaS) offerings, which provided developers with a pre-built platform for building and deploying applications.\n\nAs PaaS offerings became more popular, organizations recognized the value of a dedicated internal developer platform and teams with a product mindset dedicated to building and managing it. This led to the emergence of platform teams and the concept of platform engineering.\n\nPlatform engineering is focused on the development and maintenance of the underlying platform and infrastructure that supports software applications, and involves designing and building the tools, frameworks and systems that developers can use to build and deploy their applications.\n\n## Platform Team Goals and Benefits\n\nThe goals and benefits of platform teams and platform engineering include:\n\n### Increased Efficiency\n\nBy standardizing and streamlining software development, deployment, and maintenance processes, platform teams can help developers to deliver and improve applications more efficiently, reducing errors, improving consistency and increasing speed.\n\n### Improved Scalability and Reliability\n\nBy designing and building a scalable and reliable platform, platform teams can reduce downtime and improve the end-user experience.\n\n### Improved Security\n\nA well-engineered platform can help prevent [data breaches](/resource/common-vulnerabilities-and-exposures-cve-explained), unauthorized access, or other security issues that can compromise sensitive information or harm the business.\n\n### Technical Support and Expertise\n\nBy providing technical support and expertise to development teams, platform teams can help to solve complex technical challenges and ensure that applications are built to industry standards.\n\n### Improved Collaboration and Communication\n\nBy improving collaboration and communication between development teams and IT operations teams, platform teams can help to break down the barriers between these traditionally separate teams and create a culture of continuous integration and delivery.\n\n### Ongoing Evolution and Improvement\n\nBy managing the ongoing evolution and improvement of the platform, platform teams can ensure that it remains up-to-date and able to support the changing needs of the organization, providing ongoing value to the organization and its customers.\n\n## Product Mindset\n\nPlatform teams with a product mindset approach their work as if they are building a product that meets the needs of application teams, treating them as their internal customers. Instead of only focusing on technical requirements, a product-focused platform team considers the user experience and business value that the platform provides.\n\nThis approach involves working closely with application teams to understand their requirements and priorities and using that information to inform the development of the platform. A product-focused platform team takes a product management approach to designing, prioritizing and building the features and capabilities that are most important to application teams and the broader organization. This also involves an ongoing focus on continuous improvement and evolution of the platform based on feedback and changing requirements to better meet the needs of its users.\n\nA product-focused platform team also takes responsibility for the success of the platform, including its adoption and usage by development teams. They work to ensure that the platform is easy to use and integrate with, and that it provides clear benefits to application developers and the organization as a whole.\n\n## Frictionless Self Service\n\nThe product mindset of a platform team can help deliver frictionless self-service to application development teams by focusing on the needs and priorities of those teams and delivering a platform that is easy to use, integrate and operate. This approach involves a few key elements:\n\n- **Understanding the needs of application development teams.** A product-focused platform team will work closely with application development teams to understand their needs, priorities and pain points. This understanding allows the platform team to prioritize the features and capabilities that are most important to application development teams and deliver a platform that meets their needs.\n- **Delivering a user-friendly platform.** A product-focused platform team will design and build a platform that is user-friendly and easy to navigate. This can involve creating clear documentation, providing intuitive interfaces and offering self-service capabilities that enable app teams to easily provision and manage the resources they need.\n- **Providing ongoing support and feedback.** A product-focused platform team will provide ongoing support and feedback to application development teams, helping them to troubleshoot issues, optimize their usage of the platform and improve their overall experience.\n\nBy taking this approach, a product-focused platform team can deliver frictionless self-service to app teams, making it easy for them to provision, deploy and manage the resources they need to build and operate their applications. This can help to improve the efficiency and quality of software development, reduce the workload on operations teams, and ultimately deliver better software products to customers.\n\n## What’s the Difference Between Platform Engineering and DevOps?\n\nPlatform engineering refers to the process of designing, building and maintaining the infrastructure and underlying platform that supports the development and deployment of software applications. Platform engineers are responsible for creating the foundational tools, frameworks, and systems that software developers and other stakeholders can use to build and deliver software products. They focus on issues such as scalability, reliability, security and performance, and work to create a stable and efficient platform that can support the needs of the organization.\n\nDevOps, on the other hand, is a broader set of practices and principles that aim to improve collaboration and communication between software development teams and IT operations teams. DevOps is focused on breaking down the barriers between these traditionally separate teams and creating a culture of continuous integration and delivery. DevOps teams use tools and processes such as version control, automated testing and continuous deployment to streamline the software development and deployment process and improve the speed and quality of software releases.\n\n## What Role Does Service Mesh Play in the Platform?\n\n[A service mesh](/what-is-istio-service-mesh) provides several features and capability that are particularly important for internal developer platforms, including:\n\n### Traffic Management\n\nService mesh can manage traffic between microservices, [providing load balancing](/load-balance-failover-kafka-redhat-amq-streams-k8s-tsb), routing and service discovery capabilities. This can help developers to easily connect their microservices and create complex applications.\n\n### Security\n\nService mesh can provide secure communication between microservices, using features such as mutual TLS and access control policies. This can help to ensure that sensitive data is protected and that communication between microservices is authenticated, authorized and encrypted.\n\n### Observability\n\nService mesh can provide insights and visibility into the communication between microservices, allowing developers to monitor and troubleshoot issues in real-time. This can help to reduce the time and effort required to diagnose and fix problems, and ensure that applications are functioning correctly.\n\n### Resiliency \n\nService mesh provides features such as circuit breaking and retry logic, improving the resilience of the platform and reducing the impact of failures. This can help to ensure that applications are available and functioning correctly, even in the face of failures or errors.\n\n## Enterprise Service Mesh\n\nPlatform teams for large organizations with fleets of applications across multiple clusters, clouds, and data centers often need to “connect the dots” across service mesh deployments to provide a unified and consistent operational model for their customers. For platform teams in these larger organizations, Tetrate offers enterprise service mesh solutions to do just that.\n\nTetrate’s enterprise-grade service mesh platform, Tetrate Service Bridge, unifies and simplifies the connectivity, security, observability, and reliability for your entire application fleet—across [Kubernetes](/learn/kubernetes-security-best-practices) clusters, virtual machines, bare metal servers and across clouds and on-premises deployments\n\nTSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.\n\n[Learn more ›](/tetrate-service-bridge)","src/content/learn/what-is-platform-team/index.md","b241f95e6831103e",{html:791,metadata:792},"<h2 id=\"overview\">Overview</h2>\n<p>A platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software <a href=\"/external-link/\">applications</a></p>\n<p>The platform team typically includes software engineers, systems administrators, network engineers, database administrators, security experts and other specialists who work together to ensure the platform is secure, scalable and reliable. The platform team is responsible for the technical aspects of the platform, including <a href=\"/learn/kubernetes-security-architecture\">architecture</a> design, system configuration, coding, testing, deployment and ongoing maintenance.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h3 id=\"origins-of-platform-engineering\">Origins of Platform Engineering</h3>\n<p>Platform engineering can be traced back to the rise of cloud computing and the need for scalable and reliable infrastructure to support software applications.</p>\n<p>As organizations began to move their applications and data to the cloud, they saw the need for standardized and scalable infrastructure that could support the rapid development and deployment of software applications. This led to the development of platform-as-a-service (PaaS) offerings, which provided developers with a pre-built platform for building and deploying applications.</p>\n<p>As PaaS offerings became more popular, organizations recognized the value of a dedicated internal developer platform and teams with a product mindset dedicated to building and managing it. This led to the emergence of platform teams and the concept of platform engineering.</p>\n<p>Platform engineering is focused on the development and maintenance of the underlying platform and infrastructure that supports software applications, and involves designing and building the tools, frameworks and systems that developers can use to build and deploy their applications.</p>\n<h2 id=\"platform-team-goals-and-benefits\">Platform Team Goals and Benefits</h2>\n<p>The goals and benefits of platform teams and platform engineering include:</p>\n<h3 id=\"increased-efficiency\">Increased Efficiency</h3>\n<p>By standardizing and streamlining software development, deployment, and maintenance processes, platform teams can help developers to deliver and improve applications more efficiently, reducing errors, improving consistency and increasing speed.</p>\n<h3 id=\"improved-scalability-and-reliability\">Improved Scalability and Reliability</h3>\n<p>By designing and building a scalable and reliable platform, platform teams can reduce downtime and improve the end-user experience.</p>\n<h3 id=\"improved-security\">Improved Security</h3>\n<p>A well-engineered platform can help prevent <a href=\"/resource/common-vulnerabilities-and-exposures-cve-explained\">data breaches</a>, unauthorized access, or other security issues that can compromise sensitive information or harm the business.</p>\n<h3 id=\"technical-support-and-expertise\">Technical Support and Expertise</h3>\n<p>By providing technical support and expertise to development teams, platform teams can help to solve complex technical challenges and ensure that applications are built to industry standards.</p>\n<h3 id=\"improved-collaboration-and-communication\">Improved Collaboration and Communication</h3>\n<p>By improving collaboration and communication between development teams and IT operations teams, platform teams can help to break down the barriers between these traditionally separate teams and create a culture of continuous integration and delivery.</p>\n<h3 id=\"ongoing-evolution-and-improvement\">Ongoing Evolution and Improvement</h3>\n<p>By managing the ongoing evolution and improvement of the platform, platform teams can ensure that it remains up-to-date and able to support the changing needs of the organization, providing ongoing value to the organization and its customers.</p>\n<h2 id=\"product-mindset\">Product Mindset</h2>\n<p>Platform teams with a product mindset approach their work as if they are building a product that meets the needs of application teams, treating them as their internal customers. Instead of only focusing on technical requirements, a product-focused platform team considers the user experience and business value that the platform provides.</p>\n<p>This approach involves working closely with application teams to understand their requirements and priorities and using that information to inform the development of the platform. A product-focused platform team takes a product management approach to designing, prioritizing and building the features and capabilities that are most important to application teams and the broader organization. This also involves an ongoing focus on continuous improvement and evolution of the platform based on feedback and changing requirements to better meet the needs of its users.</p>\n<p>A product-focused platform team also takes responsibility for the success of the platform, including its adoption and usage by development teams. They work to ensure that the platform is easy to use and integrate with, and that it provides clear benefits to application developers and the organization as a whole.</p>\n<h2 id=\"frictionless-self-service\">Frictionless Self Service</h2>\n<p>The product mindset of a platform team can help deliver frictionless self-service to application development teams by focusing on the needs and priorities of those teams and delivering a platform that is easy to use, integrate and operate. This approach involves a few key elements:</p>\n<ul>\n<li><strong>Understanding the needs of application development teams.</strong> A product-focused platform team will work closely with application development teams to understand their needs, priorities and pain points. This understanding allows the platform team to prioritize the features and capabilities that are most important to application development teams and deliver a platform that meets their needs.</li>\n<li><strong>Delivering a user-friendly platform.</strong> A product-focused platform team will design and build a platform that is user-friendly and easy to navigate. This can involve creating clear documentation, providing intuitive interfaces and offering self-service capabilities that enable app teams to easily provision and manage the resources they need.</li>\n<li><strong>Providing ongoing support and feedback.</strong> A product-focused platform team will provide ongoing support and feedback to application development teams, helping them to troubleshoot issues, optimize their usage of the platform and improve their overall experience.</li>\n</ul>\n<p>By taking this approach, a product-focused platform team can deliver frictionless self-service to app teams, making it easy for them to provision, deploy and manage the resources they need to build and operate their applications. This can help to improve the efficiency and quality of software development, reduce the workload on operations teams, and ultimately deliver better software products to customers.</p>\n<h2 id=\"whats-the-difference-between-platform-engineering-and-devops\">What’s the Difference Between Platform Engineering and DevOps?</h2>\n<p>Platform engineering refers to the process of designing, building and maintaining the infrastructure and underlying platform that supports the development and deployment of software applications. Platform engineers are responsible for creating the foundational tools, frameworks, and systems that software developers and other stakeholders can use to build and deliver software products. They focus on issues such as scalability, reliability, security and performance, and work to create a stable and efficient platform that can support the needs of the organization.</p>\n<p>DevOps, on the other hand, is a broader set of practices and principles that aim to improve collaboration and communication between software development teams and IT operations teams. DevOps is focused on breaking down the barriers between these traditionally separate teams and creating a culture of continuous integration and delivery. DevOps teams use tools and processes such as version control, automated testing and continuous deployment to streamline the software development and deployment process and improve the speed and quality of software releases.</p>\n<h2 id=\"what-role-does-service-mesh-play-in-the-platform\">What Role Does Service Mesh Play in the Platform?</h2>\n<p><a href=\"/what-is-istio-service-mesh\">A service mesh</a> provides several features and capability that are particularly important for internal developer platforms, including:</p>\n<h3 id=\"traffic-management\">Traffic Management</h3>\n<p>Service mesh can manage traffic between microservices, <a href=\"/load-balance-failover-kafka-redhat-amq-streams-k8s-tsb\">providing load balancing</a>, routing and service discovery capabilities. This can help developers to easily connect their microservices and create complex applications.</p>\n<h3 id=\"security\">Security</h3>\n<p>Service mesh can provide secure communication between microservices, using features such as mutual TLS and access control policies. This can help to ensure that sensitive data is protected and that communication between microservices is authenticated, authorized and encrypted.</p>\n<h3 id=\"observability\">Observability</h3>\n<p>Service mesh can provide insights and visibility into the communication between microservices, allowing developers to monitor and troubleshoot issues in real-time. This can help to reduce the time and effort required to diagnose and fix problems, and ensure that applications are functioning correctly.</p>\n<h3 id=\"resiliency\">Resiliency </h3>\n<p>Service mesh provides features such as circuit breaking and retry logic, improving the resilience of the platform and reducing the impact of failures. This can help to ensure that applications are available and functioning correctly, even in the face of failures or errors.</p>\n<h2 id=\"enterprise-service-mesh\">Enterprise Service Mesh</h2>\n<p>Platform teams for large organizations with fleets of applications across multiple clusters, clouds, and data centers often need to “connect the dots” across service mesh deployments to provide a unified and consistent operational model for their customers. For platform teams in these larger organizations, Tetrate offers enterprise service mesh solutions to do just that.</p>\n<p>Tetrate’s enterprise-grade service mesh platform, Tetrate Service Bridge, unifies and simplifies the connectivity, security, observability, and reliability for your entire application fleet—across <a href=\"/learn/kubernetes-security-best-practices\">Kubernetes</a> clusters, virtual machines, bare metal servers and across clouds and on-premises deployments</p>\n<p>TSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.</p>\n<p><a href=\"/tetrate-service-bridge\">Learn more ›</a></p>",{headings:793,localImagePaths:840,remoteImagePaths:841,frontmatter:842,imagePaths:845},[794,795,798,801,804,807,810,813,816,819,822,825,828,831,832,833,834,837],{depth:29,slug:158,text:159},{depth:327,slug:796,text:797},"origins-of-platform-engineering","Origins of Platform Engineering",{depth:29,slug:799,text:800},"platform-team-goals-and-benefits","Platform Team Goals and Benefits",{depth:327,slug:802,text:803},"increased-efficiency","Increased Efficiency",{depth:327,slug:805,text:806},"improved-scalability-and-reliability","Improved Scalability and Reliability",{depth:327,slug:808,text:809},"improved-security","Improved Security",{depth:327,slug:811,text:812},"technical-support-and-expertise","Technical Support and Expertise",{depth:327,slug:814,text:815},"improved-collaboration-and-communication","Improved Collaboration and Communication",{depth:327,slug:817,text:818},"ongoing-evolution-and-improvement","Ongoing Evolution and Improvement",{depth:29,slug:820,text:821},"product-mindset","Product Mindset",{depth:29,slug:823,text:824},"frictionless-self-service","Frictionless Self Service",{depth:29,slug:826,text:827},"whats-the-difference-between-platform-engineering-and-devops","What’s the Difference Between Platform Engineering and DevOps?",{depth:29,slug:829,text:830},"what-role-does-service-mesh-play-in-the-platform","What Role Does Service Mesh Play in the Platform?",{depth:327,slug:399,text:400},{depth:327,slug:285,text:286},{depth:327,slug:282,text:283},{depth:327,slug:835,text:836},"resiliency","Resiliency ",{depth:29,slug:838,text:839},"enterprise-service-mesh","Enterprise Service Mesh",[],[],{title:782,slug:779,date:843,description:786,categories:844,excerpt:783},["Date","2025-04-18T10:02:55.000Z"],[17],[],"what-is-platform-team/index.md","what-is-wasm",{id:847,data:849,body:855,filePath:856,digest:857,rendered:858,legacyId:908},{title:850,excerpt:851,categories:852,date:853,description:854,draft:20},"What Is Wasm?","WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and.",[17],["Date","2024-09-15T03:52:11.000Z"],"Uncover WebAssembly's power: revolutionizing web dev with enhanced performance, portability, security. Explore wazero, Go-based Wasm runtime.","WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and [security](/learn/kubernetes-security-best-practices).\n\nOne of the key advantages of Wasm is its performance. Unlike JavaScript, which is an interpreted language, Wasm is a compiled language, meaning that it can be executed more quickly and efficiently. This makes Wasm a good choice for applications that require high performance.\n\nAnother advantage of Wasm is its portability. Since it is a bytecode format, Wasm can be compiled from multiple high-level languages and run on any platform that has a Wasm runtime, such as web browsers, servers or even IoT devices.\n\n## History Of Wasm\n\nWebAssembly was first announced in 2015 as a collaboration between major browser vendors such as Mozilla, Google, Microsoft and Apple. It was designed to improve the performance of web applications and to make it possible to write apps in multiple languages that compile to a common, portable format.\n\n## How Wasm Works\n\nWebAssembly is a binary format designed to be executed in a virtual machine. The virtual machine is implemented in the browser or in other environments such as Node.js or Go (using [wazero](/external-link/)\n\nWebAssembly code is typically generated by compilers targeting the Wasm format. The code is optimized for performance and size and can be loaded and executed quickly.\n\n## Benefits Of Wasm\n\nWebAssembly has several benefits over other web technologies such as JavaScript:\n\n### Performance\n\nWebAssembly code executes much faster than JavaScript code, especially for computationally intensive tasks.\n\n### Portability\n\nWebAssembly code can be executed in any environment with a Wasm runtime, including web browsers and Node.js.\n\n### Security\n\nWebAssembly code is executed in a sandboxed environment, isolating it from the rest of the system, improving security.\n\n### Interoperability\n\nWebAssembly code can be written in any language that can be compiled to the Wasm format, enabling code reuse and interoperability between different programming languages.\n\n## Wasm Use Cases\n\nWasm can be used for a variety of purposes, ranging from improving the performance of web applications to enabling new types of applications to run in the browser. Here are some of the most common use cases for Wasm:\n\n### Web Applications\n\nOne of the most obvious Wasm use cases is improving the performance of web applications, particularly for tasks such as video editing and scientific simulations.\n\n### Game Development\n\nAnother use case is game development, where wasm can be used to build high-performance games that run in the browser.\n\n### Serverless Computing \n\nWebAssembly can be used to write serverless functions that can be executed in a serverless environment, such as AWS Lambda or Google Cloud Functions. This can improve the performance and scalability of serverless applications.\n\n### Desktop And Mobile Applications\n\nAnother use case for WebAssembly is building cross-platform desktop and mobile applications that can run on multiple platforms without requiring separate builds for each platform. This can reduce development time and cost compared to building separate applications for each platform.\n\n## What’s The Difference Between Wasm And Javascript?\n\nJavaScript is a high-level programming language that is commonly used for web development. In client side applications, it is interpreted and executed in the browser, allowing developers to create dynamic, interactive web pages. JavaScript is a versatile language that can be used for a wide range of purposes, including building web applications, server-side applications, and mobile applications.\n\nWebAssembly, on the other hand, is a low-level bytecode format suitable for compilation from multiple higher-level languages. It provides a portable, efficient, and secure way to execute code in the browser—and other environments, and it can be used alongside JavaScript or other programming languages.\n\nA key difference between Wasm and JavaScript is their performance characteristics. JavaScript is an interpreted language, which means that it can be slower than compiled code. WebAssembly, on the other hand, is designed to execute at near-native speeds, making it more suitable for high performance tasks.\n\nAnother difference between Wasm and JavaScript is their syntax and programming models. JavaScript is a high-level language that supports object-oriented and functional programming paradigms. WebAssembly, on the other hand, is a low-level language that is more akin to machine code. Wasm modules are typically written in higher-level languages and compiled to the Wasm format for execution in a Wasm runtime environment.\n\nDespite these differences, Wasm and JavaScript can be used together to provide a more powerful and versatile web development environment. For example, Wasm can be used to improve the performance of JavaScript applications by offloading computationally intensive tasks to Wasm modules.\n\n## Wazero And Wasm For Go\n\n[wazero](/external-link/)","src/content/learn/what-is-wasm/index.md","da058c8eb6ff5bc4",{html:859,metadata:860},"<p>WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and <a href=\"/learn/kubernetes-security-best-practices\">security</a>.</p>\n<p>One of the key advantages of Wasm is its performance. Unlike JavaScript, which is an interpreted language, Wasm is a compiled language, meaning that it can be executed more quickly and efficiently. This makes Wasm a good choice for applications that require high performance.</p>\n<p>Another advantage of Wasm is its portability. Since it is a bytecode format, Wasm can be compiled from multiple high-level languages and run on any platform that has a Wasm runtime, such as web browsers, servers or even IoT devices.</p>\n<h2 id=\"history-of-wasm\">History Of Wasm</h2>\n<p>WebAssembly was first announced in 2015 as a collaboration between major browser vendors such as Mozilla, Google, Microsoft and Apple. It was designed to improve the performance of web applications and to make it possible to write apps in multiple languages that compile to a common, portable format.</p>\n<h2 id=\"how-wasm-works\">How Wasm Works</h2>\n<p>WebAssembly is a binary format designed to be executed in a virtual machine. The virtual machine is implemented in the browser or in other environments such as Node.js or Go (using <a href=\"/external-link/\">wazero</a></p>\n<p>WebAssembly code is typically generated by compilers targeting the Wasm format. The code is optimized for performance and size and can be loaded and executed quickly.</p>\n<h2 id=\"benefits-of-wasm\">Benefits Of Wasm</h2>\n<p>WebAssembly has several benefits over other web technologies such as JavaScript:</p>\n<h3 id=\"performance\">Performance</h3>\n<p>WebAssembly code executes much faster than JavaScript code, especially for computationally intensive tasks.</p>\n<h3 id=\"portability\">Portability</h3>\n<p>WebAssembly code can be executed in any environment with a Wasm runtime, including web browsers and Node.js.</p>\n<h3 id=\"security\">Security</h3>\n<p>WebAssembly code is executed in a sandboxed environment, isolating it from the rest of the system, improving security.</p>\n<h3 id=\"interoperability\">Interoperability</h3>\n<p>WebAssembly code can be written in any language that can be compiled to the Wasm format, enabling code reuse and interoperability between different programming languages.</p>\n<h2 id=\"wasm-use-cases\">Wasm Use Cases</h2>\n<p>Wasm can be used for a variety of purposes, ranging from improving the performance of web applications to enabling new types of applications to run in the browser. Here are some of the most common use cases for Wasm:</p>\n<h3 id=\"web-applications\">Web Applications</h3>\n<p>One of the most obvious Wasm use cases is improving the performance of web applications, particularly for tasks such as video editing and scientific simulations.</p>\n<h3 id=\"game-development\">Game Development</h3>\n<p>Another use case is game development, where wasm can be used to build high-performance games that run in the browser.</p>\n<h3 id=\"serverless-computing\">Serverless Computing </h3>\n<p>WebAssembly can be used to write serverless functions that can be executed in a serverless environment, such as AWS Lambda or Google Cloud Functions. This can improve the performance and scalability of serverless applications.</p>\n<h3 id=\"desktop-and-mobile-applications\">Desktop And Mobile Applications</h3>\n<p>Another use case for WebAssembly is building cross-platform desktop and mobile applications that can run on multiple platforms without requiring separate builds for each platform. This can reduce development time and cost compared to building separate applications for each platform.</p>\n<h2 id=\"whats-the-difference-between-wasm-and-javascript\">What’s The Difference Between Wasm And Javascript?</h2>\n<p>JavaScript is a high-level programming language that is commonly used for web development. In client side applications, it is interpreted and executed in the browser, allowing developers to create dynamic, interactive web pages. JavaScript is a versatile language that can be used for a wide range of purposes, including building web applications, server-side applications, and mobile applications.</p>\n<p>WebAssembly, on the other hand, is a low-level bytecode format suitable for compilation from multiple higher-level languages. It provides a portable, efficient, and secure way to execute code in the browser—and other environments, and it can be used alongside JavaScript or other programming languages.</p>\n<p>A key difference between Wasm and JavaScript is their performance characteristics. JavaScript is an interpreted language, which means that it can be slower than compiled code. WebAssembly, on the other hand, is designed to execute at near-native speeds, making it more suitable for high performance tasks.</p>\n<p>Another difference between Wasm and JavaScript is their syntax and programming models. JavaScript is a high-level language that supports object-oriented and functional programming paradigms. WebAssembly, on the other hand, is a low-level language that is more akin to machine code. Wasm modules are typically written in higher-level languages and compiled to the Wasm format for execution in a Wasm runtime environment.</p>\n<p>Despite these differences, Wasm and JavaScript can be used together to provide a more powerful and versatile web development environment. For example, Wasm can be used to improve the performance of JavaScript applications by offloading computationally intensive tasks to Wasm modules.</p>\n<h2 id=\"wazero-and-wasm-for-go\">Wazero And Wasm For Go</h2>\n<p><a href=\"/external-link/\">wazero</a></p>",{headings:861,localImagePaths:902,remoteImagePaths:903,frontmatter:904,imagePaths:907},[862,865,868,871,874,877,878,881,884,887,890,893,896,899],{depth:29,slug:863,text:864},"history-of-wasm","History Of Wasm",{depth:29,slug:866,text:867},"how-wasm-works","How Wasm Works",{depth:29,slug:869,text:870},"benefits-of-wasm","Benefits Of Wasm",{depth:327,slug:872,text:873},"performance","Performance",{depth:327,slug:875,text:876},"portability","Portability",{depth:327,slug:285,text:286},{depth:327,slug:879,text:880},"interoperability","Interoperability",{depth:29,slug:882,text:883},"wasm-use-cases","Wasm Use Cases",{depth:327,slug:885,text:886},"web-applications","Web Applications",{depth:327,slug:888,text:889},"game-development","Game Development",{depth:327,slug:891,text:892},"serverless-computing","Serverless Computing ",{depth:327,slug:894,text:895},"desktop-and-mobile-applications","Desktop And Mobile Applications",{depth:29,slug:897,text:898},"whats-the-difference-between-wasm-and-javascript","What’s The Difference Between Wasm And Javascript?",{depth:29,slug:900,text:901},"wazero-and-wasm-for-go","Wazero And Wasm For Go",[],[],{title:850,slug:847,date:905,description:854,categories:906,excerpt:851},["Date","2024-09-15T03:52:11.000Z"],[17],[],"what-is-wasm/index.md","what-is-zero-trust-architecture",{id:909,data:911,body:917,filePath:918,digest:919,rendered:920,legacyId:949},{title:912,excerpt:913,categories:914,date:915,description:916,draft:20},"What Is Zero Trust Security?","Zero Trust Security—sometimes called Zero Trust Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information security model(/learn/kubernetes-security-best-practices/) that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by John Kindervag at Forrester in.",[17],["Date","2025-04-18T10:04:15.000Z"],"Explore Zero Trust Security: principles, implementation, and its transformative impact on cybersecurity with Tetrate's comprehensive guide.","## Overview\n\nZero Trust Security—sometimes called Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information [security model](/learn/kubernetes-security-best-practices) that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by [John Kindervag at Forrester in 2010](/external-link/)\n\nInstead, [Zero Trust](/zero-trust) requires continuous verification and authentication of users and devices before granting access to any network resources. This is typically done through multi-factor authentication, role-based access control and monitoring of user activity to detect and respond to any suspicious behavior.\n\nZero Trust also requires strict segmentation of network resources, so users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.\n\nZero Trust systems have a simple litmus test:  if exposed to a public network, they wouldn’t need to change. If properly implemented, a Zero Trust security architecture would be as secure on the public Internet as it is behind a firewall.\n\n## Principles of Zero Trust\n\nZero Trust is an approach—a way of thinking about network security—more than it is any particular topology, technology, or implementation. It starts from an assumption that there are no safe places on the network.\n\nIn the Zero Trust model, unlike traditional perimeter security, reachability does not imply authorization. Zero Trust seeks to shrink implicitly trusted zones around resources, ideally to zero. In a Zero Trust network, all access to resources should:\n\n**Assume a breach.** Zero Trust assumes attackers may already be present within the network, and therefore requires continuous monitoring and analysis of user and device behavior to detect and respond to any suspicious activity.\n\n**Authenticate and dynamically authorize access.** Authentication and authorization should be verified not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.\n\n**Bind trust in space.** The perimeter of trust around a resource should be as small as possible.\n\n**Bind trust in time.** Authentication and authorization should be bound to a short-lived session and re-established frequently.\n\n**Encrypt communication**. Network communication should be encrypted to prevent eavesdropping and to ensure messages are authentic and unaltered.\n\n**Use least privilege access.** Zero Trust requires granting users and devices only the minimum level of access required to perform their tasks, based on their roles and responsibilities within the organization.\n\n**Ensure observability.** A Zero Trust system should be observable so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.\n\n## Zero Trust vs Perimeter Security\n\nPerimeter security is based on the idea of creating a secure boundary around an organization’s network, typically using firewalls and other network security devices to block unauthorized access from external sources. Once a user or device is inside the perimeter, however, it is often assumed to be trusted and granted access to network resources without further authentication or verification.\n\nTraditional network security models that rely on a hardened perimeter have become less effective in the face of advanced persistent threats and targeted attacks. With the rise of cloud computing, mobile devices, and remote work, traditional network perimeters have become less relevant, making it easier for attackers to infiltrate networks and gain access to sensitive data.\n\nZero Trust, on the other hand, assumes that all users and devices,, both internal and external, may be potential threats and requires continuous verification and authentication of their identity and access permissions. This means that users and devices—and other system components—must be authenticated and authorized every time they attempt to access network resources, even if they are already inside the organization’s perimeter.\n\nAdditionally, Zero Trust requires strict segmentation of network resources, so that users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.\n\n## Benefits of Zero Trust Architecture\n\n- Accessibility is not authorization—unlike perimeter security, access to a resource is not granted solely because that resource is reachable; it must be explicitly authenticated and authorized as well.\n- Authenticated and authorized resources are protected from perimeter breaches.\n- Bounding trust in time limits the risk of compromised credentials.\n- Bounding trust in space allows for high granularity of policy enforcement.\n- Dynamic policy enforcement ensures authorization policy is up-to-date.\n- Encryption limits reconnaissance and ensures authenticity of communication.\n- Least privilege ensures that users, applications, and systems have only the access rights and permissions necessary to perform their specific functions. This approach helps prevent unauthorized access to sensitive data and resources and limits the damage that can be caused by a security incident\n- Fine-grained observability allows real-time assurance policy is being enforced and allows post-facto auditability of how policy has been enforced historically, plus the necessary data for troubleshooting and analysis.\n\n## Challenges of Implementing Zero Trust\n\nWhile Zero Trust architecture offers many benefits, there are also several challenges organizations may face when adopting this approach:\n\n**Complexity.** Implementing Zero Trust architecture requires a comprehensive understanding of an organization’s network environment, including all users, devices, applications and data. This can be a complex and time-consuming process, especially for organizations with large, distributed networks.\n\n**User experience.** Zero Trust requires continuous authentication and verification of users and devices, which can impact the user experience. If authentication processes are too cumbersome or time-consuming, users may resist using the system, leading to lower productivity and user satisfaction.\n\n**Cost.** Implementing Zero Trust architecture requires investment in new technologies, such as identity and access management systems, behavioral analytics tools, and network segmentation solutions. This can be costly, especially for smaller organizations with limited budgets.\n\n**Legacy systems.** Many organizations still rely on legacy systems and applications that may not be compatible with Zero Trust architecture. Upgrading these systems can be costly and time-consuming.\n\n**Culture and organizational change.** Implementing Zero Trust architecture requires a cultural shift within an organization, as it requires a change in the way users and devices are granted access to network resources. This can be challenging, especially in organizations with a long history of traditional perimeter security approaches.\n\n**Training and education.** Adopting Zero Trust architecture requires training and education for IT staff and end-users to understand the new security protocols and practices. This can be time-consuming and costly, especially for organizations with large and distributed workforces.\n\n## U.S. Government Standards for Zero Trust Security\n\nThe US government has published several standards and guidelines for Zero Trust Architecture. One of the most notable is the National Institute of Standards and Technology (NIST) [Special Publication 800-207](/blog/nist-sp-207-the-groundwork-for-zero-trust), which provides an overview of Zero Trust Architecture and its key components. NIST has also published the [SP 800-204 series](/blog/nist-standards-for-zero-trust-the-sp-800-204-series) on Zero Trust security recommendations for microservices applications.\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) has also published guidance on implementing Zero Trust, including a [Zero Trust Maturity Model](/external-link/)\n\nAdditionally, the Office of Management and Budget (OMB) [has issued a memorandum on Zero Trust Architecture implementation](/blog/us-government-endorses-zero-trust-architecture-for-security), outlining the government’s commitment to transitioning to a Zero Trust model and providing guidance for agencies to follow.\n\n## Service Mesh as a Critical Component of a Zero Trust Architecture\n\nIn SP 800-204B, NIST has established a reference platform consisting of Kubernetes for orchestration and resource management, with Istio service mesh providing the core security features.\n\nA service mesh is important because it closes certain gaps in the communication mechanism of Kubernetes. Kubernetes has some limitations the service mesh addresses, including insecure communications by default, lack of built-in certificate management mechanisms, lack of service identity and access management and firewall policy that operates at OSI L3, but not L7 and is, therefore, unable to peek into data packets or make metadata-driven decisions.\n\nKubernetes doesn’t address certain application-level development and operational needs unique to microservices the service mesh was designed to satisfy. A service mesh provides a unified way to address cross-cutting application concerns, standard plugins to quickly address those concerns and a framework for building custom plugins. It also manages operational complexity, enables easy governance of third-party developers and integrators, and can reduce costs for development and operations.\n\nIstio’s data plane of Envoy proxies provides authentication and authorization, secure service discovery via a dedicated service registry, secure communications including mTLS and encryption, network resilience and unified observability data. Istio’s ingress controller provides a common, external-facing API for all clients, protocol translation, composition of results, load balancing and public TLS termination. Istio’s egress controller provides a single, centralized set of whitelisted external workloads, credential exchange and protocol translation back to web-friendly protocols.\n\n- Learn more about the Istio service mesh ›\n- [Get started with Istio using Tetrate Istio Distro, Tetrate’s hardened, performant, and fully upstream Istio distribution ›](/external-link/)","src/content/learn/what-is-zero-trust-architecture/index.md","cbacfef5838e90d0",{html:921,metadata:922},"<h2 id=\"overview\">Overview</h2>\n<p>Zero Trust Security—sometimes called Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information <a href=\"/learn/kubernetes-security-best-practices\">security model</a> that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by <a href=\"/external-link/\">John Kindervag at Forrester in 2010</a></p>\n<p>Instead, <a href=\"/zero-trust\">Zero Trust</a> requires continuous verification and authentication of users and devices before granting access to any network resources. This is typically done through multi-factor authentication, role-based access control and monitoring of user activity to detect and respond to any suspicious behavior.</p>\n<p>Zero Trust also requires strict segmentation of network resources, so users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.</p>\n<p>Zero Trust systems have a simple litmus test:  if exposed to a public network, they wouldn’t need to change. If properly implemented, a Zero Trust security architecture would be as secure on the public Internet as it is behind a firewall.</p>\n<h2 id=\"principles-of-zero-trust\">Principles of Zero Trust</h2>\n<p>Zero Trust is an approach—a way of thinking about network security—more than it is any particular topology, technology, or implementation. It starts from an assumption that there are no safe places on the network.</p>\n<p>In the Zero Trust model, unlike traditional perimeter security, reachability does not imply authorization. Zero Trust seeks to shrink implicitly trusted zones around resources, ideally to zero. In a Zero Trust network, all access to resources should:</p>\n<p><strong>Assume a breach.</strong> Zero Trust assumes attackers may already be present within the network, and therefore requires continuous monitoring and analysis of user and device behavior to detect and respond to any suspicious activity.</p>\n<p><strong>Authenticate and dynamically authorize access.</strong> Authentication and authorization should be verified not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.</p>\n<p><strong>Bind trust in space.</strong> The perimeter of trust around a resource should be as small as possible.</p>\n<p><strong>Bind trust in time.</strong> Authentication and authorization should be bound to a short-lived session and re-established frequently.</p>\n<p><strong>Encrypt communication</strong>. Network communication should be encrypted to prevent eavesdropping and to ensure messages are authentic and unaltered.</p>\n<p><strong>Use least privilege access.</strong> Zero Trust requires granting users and devices only the minimum level of access required to perform their tasks, based on their roles and responsibilities within the organization.</p>\n<p><strong>Ensure observability.</strong> A Zero Trust system should be observable so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.</p>\n<h2 id=\"zero-trust-vs-perimeter-security\">Zero Trust vs Perimeter Security</h2>\n<p>Perimeter security is based on the idea of creating a secure boundary around an organization’s network, typically using firewalls and other network security devices to block unauthorized access from external sources. Once a user or device is inside the perimeter, however, it is often assumed to be trusted and granted access to network resources without further authentication or verification.</p>\n<p>Traditional network security models that rely on a hardened perimeter have become less effective in the face of advanced persistent threats and targeted attacks. With the rise of cloud computing, mobile devices, and remote work, traditional network perimeters have become less relevant, making it easier for attackers to infiltrate networks and gain access to sensitive data.</p>\n<p>Zero Trust, on the other hand, assumes that all users and devices,, both internal and external, may be potential threats and requires continuous verification and authentication of their identity and access permissions. This means that users and devices—and other system components—must be authenticated and authorized every time they attempt to access network resources, even if they are already inside the organization’s perimeter.</p>\n<p>Additionally, Zero Trust requires strict segmentation of network resources, so that users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.</p>\n<h2 id=\"benefits-of-zero-trust-architecture\">Benefits of Zero Trust Architecture</h2>\n<ul>\n<li>Accessibility is not authorization—unlike perimeter security, access to a resource is not granted solely because that resource is reachable; it must be explicitly authenticated and authorized as well.</li>\n<li>Authenticated and authorized resources are protected from perimeter breaches.</li>\n<li>Bounding trust in time limits the risk of compromised credentials.</li>\n<li>Bounding trust in space allows for high granularity of policy enforcement.</li>\n<li>Dynamic policy enforcement ensures authorization policy is up-to-date.</li>\n<li>Encryption limits reconnaissance and ensures authenticity of communication.</li>\n<li>Least privilege ensures that users, applications, and systems have only the access rights and permissions necessary to perform their specific functions. This approach helps prevent unauthorized access to sensitive data and resources and limits the damage that can be caused by a security incident</li>\n<li>Fine-grained observability allows real-time assurance policy is being enforced and allows post-facto auditability of how policy has been enforced historically, plus the necessary data for troubleshooting and analysis.</li>\n</ul>\n<h2 id=\"challenges-of-implementing-zero-trust\">Challenges of Implementing Zero Trust</h2>\n<p>While Zero Trust architecture offers many benefits, there are also several challenges organizations may face when adopting this approach:</p>\n<p><strong>Complexity.</strong> Implementing Zero Trust architecture requires a comprehensive understanding of an organization’s network environment, including all users, devices, applications and data. This can be a complex and time-consuming process, especially for organizations with large, distributed networks.</p>\n<p><strong>User experience.</strong> Zero Trust requires continuous authentication and verification of users and devices, which can impact the user experience. If authentication processes are too cumbersome or time-consuming, users may resist using the system, leading to lower productivity and user satisfaction.</p>\n<p><strong>Cost.</strong> Implementing Zero Trust architecture requires investment in new technologies, such as identity and access management systems, behavioral analytics tools, and network segmentation solutions. This can be costly, especially for smaller organizations with limited budgets.</p>\n<p><strong>Legacy systems.</strong> Many organizations still rely on legacy systems and applications that may not be compatible with Zero Trust architecture. Upgrading these systems can be costly and time-consuming.</p>\n<p><strong>Culture and organizational change.</strong> Implementing Zero Trust architecture requires a cultural shift within an organization, as it requires a change in the way users and devices are granted access to network resources. This can be challenging, especially in organizations with a long history of traditional perimeter security approaches.</p>\n<p><strong>Training and education.</strong> Adopting Zero Trust architecture requires training and education for IT staff and end-users to understand the new security protocols and practices. This can be time-consuming and costly, especially for organizations with large and distributed workforces.</p>\n<h2 id=\"us-government-standards-for-zero-trust-security\">U.S. Government Standards for Zero Trust Security</h2>\n<p>The US government has published several standards and guidelines for Zero Trust Architecture. One of the most notable is the National Institute of Standards and Technology (NIST) <a href=\"/blog/nist-sp-207-the-groundwork-for-zero-trust\">Special Publication 800-207</a>, which provides an overview of Zero Trust Architecture and its key components. NIST has also published the <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series\">SP 800-204 series</a> on Zero Trust security recommendations for microservices applications.</p>\n<p>The Cybersecurity and Infrastructure Security Agency (CISA) has also published guidance on implementing Zero Trust, including a <a href=\"/external-link/\">Zero Trust Maturity Model</a></p>\n<p>Additionally, the Office of Management and Budget (OMB) <a href=\"/blog/us-government-endorses-zero-trust-architecture-for-security\">has issued a memorandum on Zero Trust Architecture implementation</a>, outlining the government’s commitment to transitioning to a Zero Trust model and providing guidance for agencies to follow.</p>\n<h2 id=\"service-mesh-as-a-critical-component-of-a-zero-trust-architecture\">Service Mesh as a Critical Component of a Zero Trust Architecture</h2>\n<p>In SP 800-204B, NIST has established a reference platform consisting of Kubernetes for orchestration and resource management, with Istio service mesh providing the core security features.</p>\n<p>A service mesh is important because it closes certain gaps in the communication mechanism of Kubernetes. Kubernetes has some limitations the service mesh addresses, including insecure communications by default, lack of built-in certificate management mechanisms, lack of service identity and access management and firewall policy that operates at OSI L3, but not L7 and is, therefore, unable to peek into data packets or make metadata-driven decisions.</p>\n<p>Kubernetes doesn’t address certain application-level development and operational needs unique to microservices the service mesh was designed to satisfy. A service mesh provides a unified way to address cross-cutting application concerns, standard plugins to quickly address those concerns and a framework for building custom plugins. It also manages operational complexity, enables easy governance of third-party developers and integrators, and can reduce costs for development and operations.</p>\n<p>Istio’s data plane of Envoy proxies provides authentication and authorization, secure service discovery via a dedicated service registry, secure communications including mTLS and encryption, network resilience and unified observability data. Istio’s ingress controller provides a common, external-facing API for all clients, protocol translation, composition of results, load balancing and public TLS termination. Istio’s egress controller provides a single, centralized set of whitelisted external workloads, credential exchange and protocol translation back to web-friendly protocols.</p>\n<ul>\n<li>Learn more about the Istio service mesh ›</li>\n<li><a href=\"/external-link/\">Get started with Istio using Tetrate Istio Distro, Tetrate’s hardened, performant, and fully upstream Istio distribution ›</a></li>\n</ul>",{headings:923,localImagePaths:943,remoteImagePaths:944,frontmatter:945,imagePaths:948},[924,925,928,931,934,937,940],{depth:29,slug:158,text:159},{depth:29,slug:926,text:927},"principles-of-zero-trust","Principles of Zero Trust",{depth:29,slug:929,text:930},"zero-trust-vs-perimeter-security","Zero Trust vs Perimeter Security",{depth:29,slug:932,text:933},"benefits-of-zero-trust-architecture","Benefits of Zero Trust Architecture",{depth:29,slug:935,text:936},"challenges-of-implementing-zero-trust","Challenges of Implementing Zero Trust",{depth:29,slug:938,text:939},"us-government-standards-for-zero-trust-security","U.S. Government Standards for Zero Trust Security",{depth:29,slug:941,text:942},"service-mesh-as-a-critical-component-of-a-zero-trust-architecture","Service Mesh as a Critical Component of a Zero Trust Architecture",[],[],{title:912,slug:909,date:946,description:916,categories:947,excerpt:913},["Date","2025-04-18T10:04:15.000Z"],[17],[],"what-is-zero-trust-architecture/index.md","why-do-i-we-need-kubernetes",{id:950,data:952,body:959,filePath:960,digest:961,rendered:962,legacyId:981},{title:953,excerpt:954,categories:955,date:957,description:958,draft:20},"Why Do I/We Need Kubernetes?","Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their.",[956],"Why",["Date","2024-10-17T08:33:18.000Z"],"Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises…","Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their applications.\n\n## Benefits of Kubernetes\n\nUsing Kubernetes to manage your containerization infrastructure offers numerous benefits. It improves scalability, portability, resource efficiency and more. It is an invaluable tool for modern application deployment and management. Some of the key advantages of using Kubernetes include:\n\n### Container Orchestration\n\n- Automated Deployments and Rollbacks\n- Application Self-Healing\n- Service Discovery and Ingress Management\n- Load Balancing\n\n### Scalability\n\n- Scaling up and down again based on demand\n- Autoscaling as needs change without human intervention\n\n### Portability\n\n- Cloud platform agnostic\n- Technology stack neutral\n\n## Common Use Cases for Kubernetes\n\nKubernetes is a versatile tool that IT teams can use for many deployment scenarios. Its primary strength lies in its ability to manage complex, distributed applications through automation, making it an ideal solution for many scenarios. Here are some common real-world use cases where Kubernetes excels:\n\n- Microservices architecture deployments\n- Continuous integration and continuous deployment (CI/CD) workflows\n- DevSecOps and agile development\n- Cloud-native application deployments\n- Supporting [multi-cloud](/resource/simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh) and hybrid cloud strategies\n- Big data and machine learning projects\n- Internet of Things (IoT) and edge computing infrastructure management\n\nAdopting Kubernetes as the platform for deploying, managing, and decommissioning your application containers will significantly reduce the management overhead for your infrastructure.","src/content/learn/why-do-i-we-need-kubernetes/index.md","b8c77209010dc83d",{html:963,metadata:964},"<p>Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their applications.</p>\n<h2 id=\"benefits-of-kubernetes\">Benefits of Kubernetes</h2>\n<p>Using Kubernetes to manage your containerization infrastructure offers numerous benefits. It improves scalability, portability, resource efficiency and more. It is an invaluable tool for modern application deployment and management. Some of the key advantages of using Kubernetes include:</p>\n<h3 id=\"container-orchestration\">Container Orchestration</h3>\n<ul>\n<li>Automated Deployments and Rollbacks</li>\n<li>Application Self-Healing</li>\n<li>Service Discovery and Ingress Management</li>\n<li>Load Balancing</li>\n</ul>\n<h3 id=\"scalability\">Scalability</h3>\n<ul>\n<li>Scaling up and down again based on demand</li>\n<li>Autoscaling as needs change without human intervention</li>\n</ul>\n<h3 id=\"portability\">Portability</h3>\n<ul>\n<li>Cloud platform agnostic</li>\n<li>Technology stack neutral</li>\n</ul>\n<h2 id=\"common-use-cases-for-kubernetes\">Common Use Cases for Kubernetes</h2>\n<p>Kubernetes is a versatile tool that IT teams can use for many deployment scenarios. Its primary strength lies in its ability to manage complex, distributed applications through automation, making it an ideal solution for many scenarios. Here are some common real-world use cases where Kubernetes excels:</p>\n<ul>\n<li>Microservices architecture deployments</li>\n<li>Continuous integration and continuous deployment (CI/CD) workflows</li>\n<li>DevSecOps and agile development</li>\n<li>Cloud-native application deployments</li>\n<li>Supporting <a href=\"/resource/simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh\">multi-cloud</a> and hybrid cloud strategies</li>\n<li>Big data and machine learning projects</li>\n<li>Internet of Things (IoT) and edge computing infrastructure management</li>\n</ul>\n<p>Adopting Kubernetes as the platform for deploying, managing, and decommissioning your application containers will significantly reduce the management overhead for your infrastructure.</p>",{headings:965,localImagePaths:975,remoteImagePaths:976,frontmatter:977,imagePaths:980},[966,969,970,971,972],{depth:29,slug:967,text:968},"benefits-of-kubernetes","Benefits of Kubernetes",{depth:327,slug:495,text:496},{depth:327,slug:328,text:329},{depth:327,slug:875,text:876},{depth:29,slug:973,text:974},"common-use-cases-for-kubernetes","Common Use Cases for Kubernetes",[],[],{title:953,slug:950,date:978,description:958,categories:979,excerpt:954},["Date","2024-10-17T08:33:18.000Z"],[956],[],"why-do-i-we-need-kubernetes/index.md","what-is-observability",{id:982,data:984,body:990,filePath:991,digest:992,rendered:993,legacyId:1040},{title:985,excerpt:986,categories:987,date:988,description:989,draft:20},"What Is Observability?","Observability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal.",[17],["Date","2024-03-19T17:04:19.000Z"],"Discover observability basics with Tetrate. Learn to optimize performance and troubleshoot efficiently. Explore now!","## Overview\n\nObservability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal workings.\n\nIn software engineering, observability is the ability to monitor and understand the behavior of distributed systems, microservices, or applications through the collection, processing, and visualization of telemetry data such as logs, metrics, traces, and events.\n\nObservability is essential for engineers to maintain and operate complex systems, troubleshoot issues, and ensure high availability and performance. Without observability, it’s difficult to understand what’s happening inside a system, leading to longer resolution times and decreased reliability.\n\n## The Three Pillars of Observability\n\nObservability for distributed software applications is typically achieved through the collection and analysis of three types of data: logs, metrics, and traces. Let’s take a closer look at each of these pillars.\n\n### Logs\n\nLogs are essentially records of events that occur within a system, such as user requests, errors, or system events. They provide a detailed view of system behavior, allowing engineers and operators to identify patterns and diagnose issues quickly.\n\n### Metrics\n\nMetrics are numerical measurements of system performance, such as CPU usage or network traffic. They provide a high-level view of system behavior, enabling engineers and operators to identify trends and anomalies.\n\n### Traces\n\nTraces are a record of the path of a user request as it moves through a system. They enable engineers and operators to gain a deep understanding of how different components of a system are interacting, allowing for more efficient problem solving.\n\nTogether, logs, metrics, and traces provide a comprehensive view of system behavior, enabling engineers and operators to gain a deep understanding of how their systems operate.\n\n## Tools and Techniques for Implementing Observability\n\nImplementing observability requires the use of specialized tools and techniques that enable the collection and analysis of the data types discussed above. Here are some of the most commonly used tools and techniques in the observability space:\n\n### Logging Frameworks\n\nLogging frameworks enable the collection and analysis of logs generated by a system. They allow engineers and operators to define what types of events should be logged and how they should be formatted, as well as providing tools for searching and analyzing log data.\n\nSome popular logging frameworks include ELK Stack, Graylog, and Fluentd.\n\n### Metrics Collection Tools\n\nMetrics collection tools enable the collection and analysis of numerical measurements generated by a system. They typically provide real-time dashboards that display key metrics such as CPU usage, memory usage, and network traffic.\n\nSome popular metrics collection tools include Prometheus, Graphite, and InfluxDB.\n\n### Tracing Tools\n\nTracing tools enable the collection and analysis of traces generated by a system. They provide a detailed view of how a user request moves through a system, including any microservices or other components it interacts with.\n\nSome popular tracing tools include Jaeger, Zipkin, and OpenTelemetry.\n\n### Apache SkyWalking\n\nIn addition to point solutions mentioned above, broader observability solutions like Apache SkyWalking are also available. SkyWalking is designed to be a more comprehensive observability platform that includes tracing as one of its core features. SkyWalking also includes a broader range of features beyond tracing, such as metrics collection and log analysis.\n\n## What’s the Difference Between Monitoring and Observability?\n\nMonitoring involves tracking and measuring predefined metrics or events related to the performance or behavior of a system. For example, a monitoring system might track CPU usage, network traffic, or the number of requests being processed per second. The goal of monitoring is to provide a high-level overview of system behavior, identify trends or anomalies, and trigger alerts or notifications when certain thresholds are exceeded.\n\nObservability, on the other hand, is a more holistic approach that focuses on the ability to understand and analyze a system based on its external outputs. Rather than being limited to a predefined set of metrics or events, observability involves collecting and analyzing a wide range of data points, including those that may not have been previously considered important or relevant. The goal of observability is to gain a deep understanding of system behavior, identify the root cause of issues or anomalies, and provide actionable insights for improving system performance and reliability.\n\n## The Role of Apache Skywalking in Observability\n\nApache SkyWalking is an open-source application performance monitor (APM) that can play an important role in enabling observability in modern applications. SkyWalking provides a comprehensive set of features for monitoring the performance of distributed systems, including tracing, metrics collection, and log analysis.\n\nObservability is a holistic approach to understanding system behavior that involves collecting and analyzing a wide range of data points, including logs, metrics, and traces. Apache SkyWalking is designed to provide visibility into many of these data points, particularly those related to application performance.\n\nOne of the key features of Apache SkyWalking is its distributed tracing capability, which enables teams to track the flow of requests through their systems and identify bottlenecks or issues that may be impacting performance. SkyWalking also supports the collection of metrics related to application performance, such as response times, error rates, and resource utilization, as well as the analysis of log data to identify patterns or anomalies.\n\nIn addition to these features, Apache SkyWalking is highly configurable and supports a wide range of programming languages and frameworks, making it a versatile tool for monitoring applications across different technology stacks. It also supports integrations with other monitoring and observability tools, such as Prometheus and Grafana, to provide a more comprehensive view of system behavior.\n\n## The Role of Service Mesh in Observability\n\nA service mesh is a dedicated infrastructure layer that provides connectivity and security for microservices within a distributed system. It can play an important role in enabling observability for such systems.\n\nOne of the key benefits of a service mesh is that it can provide visibility into the interactions between microservices. By capturing data related to requests, responses, and other interactions between services, a service mesh can provide valuable insights into the behavior of a system. This information can be used to improve system performance, identify issues or anomalies, and troubleshoot problems as they arise.\n\nIn addition, a service mesh can provide a centralized location for collecting and analyzing data related to the performance of microservices. This can include metrics such as response times, error rates, and resource utilization, as well as log data related to specific transactions or events. By providing a comprehensive view of system behavior, a service mesh can enable effective observability and help ensure the reliability, scalability, and maintainability of distributed systems.\n\nSome popular service mesh platforms that offer observability features include Istio, Linkerd, and Consul. These platforms provide a range of tools and techniques for collecting and analyzing data related to system behavior, and can be integrated with other observability tools to provide a complete view of system performance.\n\nEnterprise service mesh offerings like Tetrate Service Bridge can help provide unified and consistent observability signals across a fleet of applications in multiple clusters, clouds, and on premises.","src/content/learn/what-is-observability/index.md","ce6b9815c083a267",{html:994,metadata:995},"<h2 id=\"overview\">Overview</h2>\n<p>Observability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal workings.</p>\n<p>In software engineering, observability is the ability to monitor and understand the behavior of distributed systems, microservices, or applications through the collection, processing, and visualization of telemetry data such as logs, metrics, traces, and events.</p>\n<p>Observability is essential for engineers to maintain and operate complex systems, troubleshoot issues, and ensure high availability and performance. Without observability, it’s difficult to understand what’s happening inside a system, leading to longer resolution times and decreased reliability.</p>\n<h2 id=\"the-three-pillars-of-observability\">The Three Pillars of Observability</h2>\n<p>Observability for distributed software applications is typically achieved through the collection and analysis of three types of data: logs, metrics, and traces. Let’s take a closer look at each of these pillars.</p>\n<h3 id=\"logs\">Logs</h3>\n<p>Logs are essentially records of events that occur within a system, such as user requests, errors, or system events. They provide a detailed view of system behavior, allowing engineers and operators to identify patterns and diagnose issues quickly.</p>\n<h3 id=\"metrics\">Metrics</h3>\n<p>Metrics are numerical measurements of system performance, such as CPU usage or network traffic. They provide a high-level view of system behavior, enabling engineers and operators to identify trends and anomalies.</p>\n<h3 id=\"traces\">Traces</h3>\n<p>Traces are a record of the path of a user request as it moves through a system. They enable engineers and operators to gain a deep understanding of how different components of a system are interacting, allowing for more efficient problem solving.</p>\n<p>Together, logs, metrics, and traces provide a comprehensive view of system behavior, enabling engineers and operators to gain a deep understanding of how their systems operate.</p>\n<h2 id=\"tools-and-techniques-for-implementing-observability\">Tools and Techniques for Implementing Observability</h2>\n<p>Implementing observability requires the use of specialized tools and techniques that enable the collection and analysis of the data types discussed above. Here are some of the most commonly used tools and techniques in the observability space:</p>\n<h3 id=\"logging-frameworks\">Logging Frameworks</h3>\n<p>Logging frameworks enable the collection and analysis of logs generated by a system. They allow engineers and operators to define what types of events should be logged and how they should be formatted, as well as providing tools for searching and analyzing log data.</p>\n<p>Some popular logging frameworks include ELK Stack, Graylog, and Fluentd.</p>\n<h3 id=\"metrics-collection-tools\">Metrics Collection Tools</h3>\n<p>Metrics collection tools enable the collection and analysis of numerical measurements generated by a system. They typically provide real-time dashboards that display key metrics such as CPU usage, memory usage, and network traffic.</p>\n<p>Some popular metrics collection tools include Prometheus, Graphite, and InfluxDB.</p>\n<h3 id=\"tracing-tools\">Tracing Tools</h3>\n<p>Tracing tools enable the collection and analysis of traces generated by a system. They provide a detailed view of how a user request moves through a system, including any microservices or other components it interacts with.</p>\n<p>Some popular tracing tools include Jaeger, Zipkin, and OpenTelemetry.</p>\n<h3 id=\"apache-skywalking\">Apache SkyWalking</h3>\n<p>In addition to point solutions mentioned above, broader observability solutions like Apache SkyWalking are also available. SkyWalking is designed to be a more comprehensive observability platform that includes tracing as one of its core features. SkyWalking also includes a broader range of features beyond tracing, such as metrics collection and log analysis.</p>\n<h2 id=\"whats-the-difference-between-monitoring-and-observability\">What’s the Difference Between Monitoring and Observability?</h2>\n<p>Monitoring involves tracking and measuring predefined metrics or events related to the performance or behavior of a system. For example, a monitoring system might track CPU usage, network traffic, or the number of requests being processed per second. The goal of monitoring is to provide a high-level overview of system behavior, identify trends or anomalies, and trigger alerts or notifications when certain thresholds are exceeded.</p>\n<p>Observability, on the other hand, is a more holistic approach that focuses on the ability to understand and analyze a system based on its external outputs. Rather than being limited to a predefined set of metrics or events, observability involves collecting and analyzing a wide range of data points, including those that may not have been previously considered important or relevant. The goal of observability is to gain a deep understanding of system behavior, identify the root cause of issues or anomalies, and provide actionable insights for improving system performance and reliability.</p>\n<h2 id=\"the-role-of-apache-skywalking-in-observability\">The Role of Apache Skywalking in Observability</h2>\n<p>Apache SkyWalking is an open-source application performance monitor (APM) that can play an important role in enabling observability in modern applications. SkyWalking provides a comprehensive set of features for monitoring the performance of distributed systems, including tracing, metrics collection, and log analysis.</p>\n<p>Observability is a holistic approach to understanding system behavior that involves collecting and analyzing a wide range of data points, including logs, metrics, and traces. Apache SkyWalking is designed to provide visibility into many of these data points, particularly those related to application performance.</p>\n<p>One of the key features of Apache SkyWalking is its distributed tracing capability, which enables teams to track the flow of requests through their systems and identify bottlenecks or issues that may be impacting performance. SkyWalking also supports the collection of metrics related to application performance, such as response times, error rates, and resource utilization, as well as the analysis of log data to identify patterns or anomalies.</p>\n<p>In addition to these features, Apache SkyWalking is highly configurable and supports a wide range of programming languages and frameworks, making it a versatile tool for monitoring applications across different technology stacks. It also supports integrations with other monitoring and observability tools, such as Prometheus and Grafana, to provide a more comprehensive view of system behavior.</p>\n<h2 id=\"the-role-of-service-mesh-in-observability\">The Role of Service Mesh in Observability</h2>\n<p>A service mesh is a dedicated infrastructure layer that provides connectivity and security for microservices within a distributed system. It can play an important role in enabling observability for such systems.</p>\n<p>One of the key benefits of a service mesh is that it can provide visibility into the interactions between microservices. By capturing data related to requests, responses, and other interactions between services, a service mesh can provide valuable insights into the behavior of a system. This information can be used to improve system performance, identify issues or anomalies, and troubleshoot problems as they arise.</p>\n<p>In addition, a service mesh can provide a centralized location for collecting and analyzing data related to the performance of microservices. This can include metrics such as response times, error rates, and resource utilization, as well as log data related to specific transactions or events. By providing a comprehensive view of system behavior, a service mesh can enable effective observability and help ensure the reliability, scalability, and maintainability of distributed systems.</p>\n<p>Some popular service mesh platforms that offer observability features include Istio, Linkerd, and Consul. These platforms provide a range of tools and techniques for collecting and analyzing data related to system behavior, and can be integrated with other observability tools to provide a complete view of system performance.</p>\n<p>Enterprise service mesh offerings like Tetrate Service Bridge can help provide unified and consistent observability signals across a fleet of applications in multiple clusters, clouds, and on premises.</p>",{headings:996,localImagePaths:1034,remoteImagePaths:1035,frontmatter:1036,imagePaths:1039},[997,998,1001,1004,1007,1010,1013,1016,1019,1022,1025,1028,1031],{depth:29,slug:158,text:159},{depth:29,slug:999,text:1000},"the-three-pillars-of-observability","The Three Pillars of Observability",{depth:327,slug:1002,text:1003},"logs","Logs",{depth:327,slug:1005,text:1006},"metrics","Metrics",{depth:327,slug:1008,text:1009},"traces","Traces",{depth:29,slug:1011,text:1012},"tools-and-techniques-for-implementing-observability","Tools and Techniques for Implementing Observability",{depth:327,slug:1014,text:1015},"logging-frameworks","Logging Frameworks",{depth:327,slug:1017,text:1018},"metrics-collection-tools","Metrics Collection Tools",{depth:327,slug:1020,text:1021},"tracing-tools","Tracing Tools",{depth:327,slug:1023,text:1024},"apache-skywalking","Apache SkyWalking",{depth:29,slug:1026,text:1027},"whats-the-difference-between-monitoring-and-observability","What’s the Difference Between Monitoring and Observability?",{depth:29,slug:1029,text:1030},"the-role-of-apache-skywalking-in-observability","The Role of Apache Skywalking in Observability",{depth:29,slug:1032,text:1033},"the-role-of-service-mesh-in-observability","The Role of Service Mesh in Observability",[],[],{title:985,slug:982,date:1037,description:989,categories:1038,excerpt:986},["Date","2024-03-19T17:04:19.000Z"],[17],[],"what-is-observability/index.md","resources",["Map",1043,1044,1070,1071,1091,1092,1112,1113,1133,1134,1157,1158,1194,1195,1213,1214,1234,1235,1255,1256,1282,1283,1306,1307,1330,1331,1351,1352,1370,1371,1388,1389,1410,1411,1431,1432],"common-vulnerabilities-and-exposures-cve-explained",{id:1043,data:1045,body:1055,filePath:1056,digest:1057,rendered:1058,legacyId:1069},{title:1046,categories:1047,featuredImage:1049,date:1050,hubspotFormId:1051,draft:20,downloadLink:1052,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1051},"Common Vulnerabilities and Exposures (CVE) Explained",[1048],"ebooks & reports","/images/resources/common-cover.png",["Date","2024-06-14T00:00:00.000Z"],"f88bed37-b582-4174-8791-9eb165b5c143","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/Common%20Vulnerabilities%20and%20Exposures%20Explained.pdf","Tetrate provides trusted connectivity and control for AI. Empower developers while safeguarding the business. Built atop the proven Envoy proxy & Envoy AI Gateway.","Get CVE Alerts and Patches","![Post Image](/images/resources/common-cover.png)\n\nBusinesses today face an unprecedented level of risk from cybersecurity attacks and data breaches, often resulting in substantial financial damages. These threats stem from vulnerabilities and exposures within computer systems, making it imperative for organizations to understand and address these risks.\n\nA [CVE (Common Vulnerabilities and Exposures)](/faq/what-is-a-cve-common-vulnerability-and-exposure/), is a standardized identifier assigned to a known security vulnerability in software, including open source software. The CVE system is maintained by the MITRE Corporation and provides a way to uniquely identify and track vulnerabilities across different information security databases and tools.\n\nA vulnerability can be described as a flaw or weakness in computer systems or software that unintentionally grants unauthorized access to users. Exploiting vulnerabilities allows attackers to execute destructive actions, such as installing malware or gaining unauthorized access to sensitive information.\n\nOn the other hand, exposure refers to a misconfiguration that provides attackers access to a computer system or its stored data. For instance, a loosely secured cloud storage system may permit unauthorized access to sensitive data, or an open network port on a server can be exploited by command and control malware.\n\nIt’s crucial to recognize exposures as vulnerabilities actively targeted and exploited by attackers.","src/content/resources/common-vulnerabilities-and-exposures-cve-explained/index.md","18d7359fc2c00caf",{html:1059,metadata:1060},"<p><img src=\"/images/resources/common-cover.png\" alt=\"Post Image\"></p>\n<p>Businesses today face an unprecedented level of risk from cybersecurity attacks and data breaches, often resulting in substantial financial damages. These threats stem from vulnerabilities and exposures within computer systems, making it imperative for organizations to understand and address these risks.</p>\n<p>A <a href=\"/faq/what-is-a-cve-common-vulnerability-and-exposure/\">CVE (Common Vulnerabilities and Exposures)</a>, is a standardized identifier assigned to a known security vulnerability in software, including open source software. The CVE system is maintained by the MITRE Corporation and provides a way to uniquely identify and track vulnerabilities across different information security databases and tools.</p>\n<p>A vulnerability can be described as a flaw or weakness in computer systems or software that unintentionally grants unauthorized access to users. Exploiting vulnerabilities allows attackers to execute destructive actions, such as installing malware or gaining unauthorized access to sensitive information.</p>\n<p>On the other hand, exposure refers to a misconfiguration that provides attackers access to a computer system or its stored data. For instance, a loosely secured cloud storage system may permit unauthorized access to sensitive data, or an open network port on a server can be exploited by command and control malware.</p>\n<p>It’s crucial to recognize exposures as vulnerabilities actively targeted and exploited by attackers.</p>",{headings:1061,localImagePaths:1062,remoteImagePaths:1063,frontmatter:1064,imagePaths:1068},[],[],[],{title:1046,featuredImage:1049,description:1053,date:1065,categories:1066,excerpt:1053,hubspotFormId:1051,modalFormId:1051,modalFormLinkText:1054,downloadLink:1052,useHubspotEmbed:1067},"2024-06-14T00:00:00.000Z",[1048],true,[],"common-vulnerabilities-and-exposures-cve-explained/index.md","essential-service-mesh-adoption-checklist",{id:1070,data:1072,body:1078,filePath:1079,digest:1080,rendered:1081,legacyId:1090},{title:1073,categories:1074,featuredImage:1075,hubspotFormId:1076,draft:20,downloadLink:1077,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1076},"The Essential Service Mesh Adoption Checklist",[1041],"/images/resources/service-mesh-adoption-checklist-cover-fixed.webp","d13a892b-af09-4610-bebb-6ad182230882","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/The%20Essential%20Service%20Mesh%20Adoption%20Checklist.pdf","![Post Image](/images/resources/service-mesh-adoption-checklist-cover-fixed.webp)\n\nBy offering connectivity, reliability, observability and security, the service mesh has evolved into a fundamental component of an organization’s application and infrastructure modernization initiatives. In general, your organization can reap substantial benefits from implementing a service mesh, especially when dealing with  distributed applications composed of many microservices. As application traffic grows, requests between these services can increase exponentially, requiring sophisticated routing capabilities to optimize the flow of data between the services and ensure the application continues to perform at a high level. From a secure communications standpoint, service meshes are essential for enabling the secure TLS (mTLS) connections between services.\n\nBecause service meshes manage the communication layer, they liberate developers from the complexities of managing how each service interacts with all the others. This allows developers to dedicate more time to adding business value with each service they create. For DevOps teams that have an established production CI/CD pipeline, a service mesh can be essential for programmatically deploying apps and application infrastructure (Kubernetes) to manage source code and test automation tools. Additionally, a service mesh enables DevOps teams to manage their networking and security policies through configuration, enhancing overall operational efficiency and ensuring consistent policy enforcement. Depending on the structure of your organization, one or more teams or roles may be responsible for installing and configuring a mesh. Based on our experience with customers, we’ve developed best practice guidelines for deploying the service mesh successfully. (Tetrate’s service mesh is based on the open source [Istio service mesh](https://istio.io/latest/).) This checklist, outlining the essential steps for a successful service mesh adoption journey, will ease the transition as you modernize applications with a microservices architecture and help you achieve ROI today. Addressing each one will not only smooth your adoption of Istio but will help transform your organizational culture and simplify workflows across the enterprise.","src/content/resources/essential-service-mesh-adoption-checklist/index.md","4a83271b926f5bbe",{html:1082,metadata:1083},"<p><img src=\"/images/resources/service-mesh-adoption-checklist-cover-fixed.webp\" alt=\"Post Image\"></p>\n<p>By offering connectivity, reliability, observability and security, the service mesh has evolved into a fundamental component of an organization’s application and infrastructure modernization initiatives. In general, your organization can reap substantial benefits from implementing a service mesh, especially when dealing with  distributed applications composed of many microservices. As application traffic grows, requests between these services can increase exponentially, requiring sophisticated routing capabilities to optimize the flow of data between the services and ensure the application continues to perform at a high level. From a secure communications standpoint, service meshes are essential for enabling the secure TLS (mTLS) connections between services.</p>\n<p>Because service meshes manage the communication layer, they liberate developers from the complexities of managing how each service interacts with all the others. This allows developers to dedicate more time to adding business value with each service they create. For DevOps teams that have an established production CI/CD pipeline, a service mesh can be essential for programmatically deploying apps and application infrastructure (Kubernetes) to manage source code and test automation tools. Additionally, a service mesh enables DevOps teams to manage their networking and security policies through configuration, enhancing overall operational efficiency and ensuring consistent policy enforcement. Depending on the structure of your organization, one or more teams or roles may be responsible for installing and configuring a mesh. Based on our experience with customers, we’ve developed best practice guidelines for deploying the service mesh successfully. (Tetrate’s service mesh is based on the open source <a href=\"https://istio.io/latest/\">Istio service mesh</a>.) This checklist, outlining the essential steps for a successful service mesh adoption journey, will ease the transition as you modernize applications with a microservices architecture and help you achieve ROI today. Addressing each one will not only smooth your adoption of Istio but will help transform your organizational culture and simplify workflows across the enterprise.</p>",{headings:1084,localImagePaths:1085,remoteImagePaths:1086,frontmatter:1087,imagePaths:1089},[],[],[],{title:1073,featuredImage:1075,description:1053,categories:1088,excerpt:1053,hubspotFormId:1076,modalFormId:1076,modalFormLinkText:1054,downloadLink:1077,useHubspotEmbed:1067},[1041],[],"essential-service-mesh-adoption-checklist/index.md","five-essential-amazon-eks-security-and-availability-strategies-with-a-service-mesh",{id:1091,data:1093,body:1099,filePath:1100,digest:1101,rendered:1102,legacyId:1111},{title:1094,categories:1095,featuredImage:1096,hubspotFormId:1097,draft:20,downloadLink:1098,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1097},"Five Essential Strategies for Deploying Service Mesh on Amazon EKS",[1041],"/images/resources/amazon-eks-service-mesh-cover.webp","30e70f6a-7027-4727-a27c-2c5798b226c5","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/Five%20Essential%20Amazon%20EKS%20Security%20and%20Availability%20Strategies%20with%20a%20Service%20Mesh.pdf","![Post Image](/images/resources/amazon-eks-service-mesh-cover.webp)\n\nVirtually all organizations are adopting cloud infrastructure, either augmenting their on- prem footprint or wholesale migrating away from on-prem in favor of more flexibility. Indeed many organizations are creating their entire infrastructure in the cloud from day 1! As of the time of this writing, Amazon Web Services (AWS) holds the largest market share amongst cloud providers. When running workloads in the cloud, many organizations\n\nare also adopting cloud-native infrastructure, which starts with running workloads in containers and adopting a container orchestration platform. Kubernetes is the most popular open source container orchestration engine for automating deployment, scaling, and management of containerized applications. Amazon Elastic Kubernetes Service ([EKS](/blog/tetrate-eks-anywhere/)) is a managed Kubernetes service to run Kubernetes. Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and other key tasks.\n\nEKS is undoubtedly one of the most popular ways to run Kubernetes, but are you getting the most out of it? Specifically, how do you translate small-scale success in limited-scale deployments into at-sale production infrastructure? Not only does running Kubernetes  \nat scale create new security and availability challenges for its operators. Crucially, Kubernetes teams rarely have the luxury of adding new headcount and budget as they scale out. In search for operational leverage is when Kubernetes operators find service mesh as the solution to achieve their security and availability requirements.\n\nService mesh is a proven architecture popularized recently that gives operators control and observability over Kubernetes traffic. [Istio](/) and Envoy have emerged as the de facto standard for implementing a service mesh, including Amazon EKS. With the right service mesh implementation on Amazon EKS, you can take advantage of all the performance, scale, reliability, and availability of AWS infrastructure, as well as integrations with AWS networking and security services. In this whitepaper, we present six service mesh strategies to help you enhance security and availability for your EKS workloads.","src/content/resources/five-essential-amazon-eks-security-and-availability-strategies-with-a-service-mesh/index.md","1a0ed81f1bf95960",{html:1103,metadata:1104},"<p><img src=\"/images/resources/amazon-eks-service-mesh-cover.webp\" alt=\"Post Image\"></p>\n<p>Virtually all organizations are adopting cloud infrastructure, either augmenting their on- prem footprint or wholesale migrating away from on-prem in favor of more flexibility. Indeed many organizations are creating their entire infrastructure in the cloud from day 1! As of the time of this writing, Amazon Web Services (AWS) holds the largest market share amongst cloud providers. When running workloads in the cloud, many organizations</p>\n<p>are also adopting cloud-native infrastructure, which starts with running workloads in containers and adopting a container orchestration platform. Kubernetes is the most popular open source container orchestration engine for automating deployment, scaling, and management of containerized applications. Amazon Elastic Kubernetes Service (<a href=\"/blog/tetrate-eks-anywhere/\">EKS</a>) is a managed Kubernetes service to run Kubernetes. Amazon EKS automatically manages the availability and scalability of the Kubernetes control plane nodes responsible for scheduling containers, managing application availability, storing cluster data, and other key tasks.</p>\n<p>EKS is undoubtedly one of the most popular ways to run Kubernetes, but are you getting the most out of it? Specifically, how do you translate small-scale success in limited-scale deployments into at-sale production infrastructure? Not only does running Kubernetes<br>\nat scale create new security and availability challenges for its operators. Crucially, Kubernetes teams rarely have the luxury of adding new headcount and budget as they scale out. In search for operational leverage is when Kubernetes operators find service mesh as the solution to achieve their security and availability requirements.</p>\n<p>Service mesh is a proven architecture popularized recently that gives operators control and observability over Kubernetes traffic. <a href=\"/\">Istio</a> and Envoy have emerged as the de facto standard for implementing a service mesh, including Amazon EKS. With the right service mesh implementation on Amazon EKS, you can take advantage of all the performance, scale, reliability, and availability of AWS infrastructure, as well as integrations with AWS networking and security services. In this whitepaper, we present six service mesh strategies to help you enhance security and availability for your EKS workloads.</p>",{headings:1105,localImagePaths:1106,remoteImagePaths:1107,frontmatter:1108,imagePaths:1110},[],[],[],{title:1094,featuredImage:1096,description:1053,categories:1109,excerpt:1053,hubspotFormId:1097,modalFormId:1097,modalFormLinkText:1054,downloadLink:1098,useHubspotEmbed:1067},[1041],[],"five-essential-amazon-eks-security-and-availability-strategies-with-a-service-mesh/index.md","hipaa-compliance-with-tetrate-service-bridge",{id:1112,data:1114,body:1120,filePath:1121,digest:1122,rendered:1123,legacyId:1132},{title:1115,categories:1116,featuredImage:1117,hubspotFormId:1118,draft:20,downloadLink:1119,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1118},"HIPAA Compliance with Tetrate Service Bridge",[1041],"/images/resources/hipaa-compliance-cover.webp","4e99453d-79b7-4b9d-9aa9-d6d95b7e66ea","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/White-Paper-HIPAA-Compliance.pdf","![Post Image](/images/resources/hipaa-compliance-cover.webp)\n\nThis white paper provides a brief overview of healthcare industry regulations for protecting patient data and describes how a service mesh solution can help health systems comply with those regulations.\n\nDownload to learn:\n\n- How to adhere to HIPAA regulations and rules while managing your systems efficiently\n- How to develop and deliver a more secure, agile, and reliable service mesh architecture\n- How service mesh and Istio are the reference standards for Zero Trust Architecture (ZTA) and why this is important to you","src/content/resources/hipaa-compliance-with-tetrate-service-bridge/index.md","492fe40f15a4410a",{html:1124,metadata:1125},"<p><img src=\"/images/resources/hipaa-compliance-cover.webp\" alt=\"Post Image\"></p>\n<p>This white paper provides a brief overview of healthcare industry regulations for protecting patient data and describes how a service mesh solution can help health systems comply with those regulations.</p>\n<p>Download to learn:</p>\n<ul>\n<li>How to adhere to HIPAA regulations and rules while managing your systems efficiently</li>\n<li>How to develop and deliver a more secure, agile, and reliable service mesh architecture</li>\n<li>How service mesh and Istio are the reference standards for Zero Trust Architecture (ZTA) and why this is important to you</li>\n</ul>",{headings:1126,localImagePaths:1127,remoteImagePaths:1128,frontmatter:1129,imagePaths:1131},[],[],[],{title:1115,featuredImage:1117,description:1053,categories:1130,excerpt:1053,hubspotFormId:1118,modalFormId:1118,modalFormLinkText:1054,downloadLink:1119,useHubspotEmbed:1067},[1041],[],"hipaa-compliance-with-tetrate-service-bridge/index.md","current-state-and-future-of-istio-service-mesh",{id:1133,data:1135,body:1141,filePath:1142,digest:1143,rendered:1144,legacyId:1156},{title:1136,categories:1137,featuredImage:1138,hubspotFormId:1139,draft:20,downloadLink:1140,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1139},"Current State and Future of the Istio Service Mesh",[1041],"/images/resources/istio-service-mesh-book-cover.jpg","4f9c50e9-feb4-423a-ad36-9dd109a43791","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Istio%20Book/The-Current-State-and-Future-of-the-Istio-Service-Mesh.pdf","### [](#insights-into-the-history-and-future-of-istio)Insights into the History and Future of Istio\n\n![Post Image](/images/resources/istio-service-mesh-book-cover.jpg)\n\nCloud native is rapidly gaining ground in the cloud and open source, and service mesh has become a critical part of the cloud native technology stack. Istio is one of the most popular service mesh today, and has been open source for over five years since 2017.\n\nThis book takes you through the historical motivation for the emergence of the service mesh, the evolution of Istio, and the Istio open source ecosystem. This book provides detailed information on\n\n- The rise of service mesh technology is due to the popularity of Kubernetes, microservices, DevOps, and cloud native architectures.\n- The emergence of Kubernetes and programmable proxies, which laid a solid foundation for Istio.\n- While eBPF can accelerate transparent traffic hijacking in Istio, it cannot replace the sidecar in the service mesh.\n- The future of Istio lies in building a zero trust network based on the hybrid cloud.\n\nWith the entry of Istio into CNCF and the introduction of the latest Ambient Mesh, we can expect that Istio will be easier to adopt, and its future will be more extensive.","src/content/resources/current-state-and-future-of-istio-service-mesh/index.md","822cc85124bbc74b",{html:1145,metadata:1146},"<h3 id=\"insights-into-the-history-and-future-of-istio\"><a href=\"#insights-into-the-history-and-future-of-istio\"></a>Insights into the History and Future of Istio</h3>\n<p><img src=\"/images/resources/istio-service-mesh-book-cover.jpg\" alt=\"Post Image\"></p>\n<p>Cloud native is rapidly gaining ground in the cloud and open source, and service mesh has become a critical part of the cloud native technology stack. Istio is one of the most popular service mesh today, and has been open source for over five years since 2017.</p>\n<p>This book takes you through the historical motivation for the emergence of the service mesh, the evolution of Istio, and the Istio open source ecosystem. This book provides detailed information on</p>\n<ul>\n<li>The rise of service mesh technology is due to the popularity of Kubernetes, microservices, DevOps, and cloud native architectures.</li>\n<li>The emergence of Kubernetes and programmable proxies, which laid a solid foundation for Istio.</li>\n<li>While eBPF can accelerate transparent traffic hijacking in Istio, it cannot replace the sidecar in the service mesh.</li>\n<li>The future of Istio lies in building a zero trust network based on the hybrid cloud.</li>\n</ul>\n<p>With the entry of Istio into CNCF and the introduction of the latest Ambient Mesh, we can expect that Istio will be easier to adopt, and its future will be more extensive.</p>",{headings:1147,localImagePaths:1151,remoteImagePaths:1152,frontmatter:1153,imagePaths:1155},[1148],{depth:327,slug:1149,text:1150},"insights-into-the-history-and-future-of-istio","Insights into the History and Future of Istio",[],[],{title:1136,featuredImage:1138,description:1053,categories:1154,excerpt:1053,hubspotFormId:1139,modalFormId:1139,modalFormLinkText:1054,downloadLink:1140,useHubspotEmbed:1067},[1041],[],"current-state-and-future-of-istio-service-mesh/index.md","informatica-gains-enterprise-level-support-with-tetrate-and-hardens-security",{id:1157,data:1159,body:1163,filePath:1164,digest:1165,rendered:1166,legacyId:1193},{title:1160,categories:1161,featuredImage:1162,draft:20,description:1053,excerpt:1053},"Informatica Gains Enterprise-level Support with Tetrate and Hardens Security",[1041],"/images/resources/aws-tetrate-case-study.webp","![Post Image](/images/resources/aws-tetrate-case-study.webp)\n\n## [](#executive-summary)Executive Summary\n\nInformatica, an enterprise cloud data management leader, empowers more than 5,000 customers to realize the transformative power of data. As a software provider, many of its applications are broken down into microservices. To manage them, Informatica uses Istio, an open-source service mesh platform for distributed applications. But, as Informatica scaled to roughly 300 clusters on Amazon Elastic Kubernetes Service (Amazon EKS), product teams were challenged with continuous update cycles and security certificates. Plus, Istio lacked the compliance necessary to build in AWS GovCloud. Tetrate provided Informatica with enterprise-level support to improve and streamline operations and harden security for AWS GovCloud.\n\n---\n\n## [](#about-informatica)About Informatica\n\nInformatica is an enterprise cloud data management company that brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. Learn more at [www.informatica.com](https://www.informatica.com)\n\n## [](#challenge-building-a-microservices-foundation-with-kubernetes)Challenge: Building a Microservices Foundation with Kubernetes\n\nLike many organizations, Informatica transitioned to a microservices architecture and Kubernetes for scalability, flexibility, and resiliency. This transition to decentralized microservices introduced challenges in monitoring and managing network communications and security, which led Informatica to adopt open source Istio. Istio is a service mesh project that helps organizations modernize their applications and thereby reduce complexity, improve security and reliability, and scale their applications to be more agile and flexible.\n\nAs Informatica continued to grow and the number of Kubernetes clusters on Amazon Elastic Kubernetes Service (Amazon EKS) grew to roughly 300, product teams were challenged with several operational inefficiencies. Continuous update cycles resulted in separate pipelines for deployment because different product teams were running different versions of Istio. Additionally, Istio could not support the Informatica customers that were required by law to meet FedRAMP compliance or its government customers that required compliance with the Federal Information Processing Standards (FIPS) in order to build in AWS GovCloud.\n\n## [](#solution-tetrate-istio-subscription-and-expert-enterprise-support)Solution: Tetrate Istio Subscription and Expert Enterprise Support\n\nTetrate Istio Subscription (TIS) is built on top of open source Istio and solves for these operational inefficiencies. Not only did Tetrate help Informatica accelerate and streamline service mesh adoption throughout the organization, its Zero Trust security model provided multi-layer security for microservices and eliminated the time and effort required to manage authentication certificates at scale while delivering end-to-end FIPS compliance.\n\n> “With Tetrate, we achieved FIPS compliance, elevated our security standards and streamlined the process of scaling on Amazon EKS. Informatica now benefits from enterprise-level support by Tetrate for our 300 clusters on Amazon EKS, which allows developers and engineers to focus on building and deploying applications rather than the complexities of network configurations and security.”\n>\n> — Deepak Deore, DevOps Architect, Informatica\n\nAs Informatica upgraded its Istio instances, Tetrate provided weekly calls with engineers to resolve technical issues and disruptions, ensuring a smooth implementation and top-notch support.\n\n## [](#benefits)Benefits\n\n- **Secured end-to-end FIPS compliance for AWS GovCloud**\n- **Decreased manual effort of building ARM Images that run on AWS Graviton from 3-4 days to a few seconds**\n- **Obtained customized Helm charts for its 300 Amazon EKS clusters, replacing the Istioctl command line tool that was incompatible with the team’s GitHub process and inconsistent with its Kubernetes configuration applications**\n- **Improved business agility, enhanced security, and FIPS compliance**\n\n## [](#results)Results\n\n- Informatica now receives enterprise-level support from Tetrate for 300+ clusters on Amazon EKS\n- Developers and engineers can focus on building and deploying applications, not network configuration and security\n- Streamlined operations and improved security posture for AWS GovCloud workloads\n\n---\n\n[Download the full case study (PDF)](https://tetrate.io/wp-content/uploads/2023/06/AWS-Tetrate-Informatica-Case-Study.pdf)","src/content/resources/informatica-gains-enterprise-level-support-with-tetrate-and-hardens-security/index.md","2922d75e5368bb86",{html:1167,metadata:1168},"<p><img src=\"/images/resources/aws-tetrate-case-study.webp\" alt=\"Post Image\"></p>\n<h2 id=\"executive-summary\"><a href=\"#executive-summary\"></a>Executive Summary</h2>\n<p>Informatica, an enterprise cloud data management leader, empowers more than 5,000 customers to realize the transformative power of data. As a software provider, many of its applications are broken down into microservices. To manage them, Informatica uses Istio, an open-source service mesh platform for distributed applications. But, as Informatica scaled to roughly 300 clusters on Amazon Elastic Kubernetes Service (Amazon EKS), product teams were challenged with continuous update cycles and security certificates. Plus, Istio lacked the compliance necessary to build in AWS GovCloud. Tetrate provided Informatica with enterprise-level support to improve and streamline operations and harden security for AWS GovCloud.</p>\n<hr>\n<h2 id=\"about-informatica\"><a href=\"#about-informatica\"></a>About Informatica</h2>\n<p>Informatica is an enterprise cloud data management company that brings data and AI to life by empowering businesses to realize the transformative power of their most critical assets. Learn more at <a href=\"https://www.informatica.com\">www.informatica.com</a></p>\n<h2 id=\"challenge-building-a-microservices-foundation-with-kubernetes\"><a href=\"#challenge-building-a-microservices-foundation-with-kubernetes\"></a>Challenge: Building a Microservices Foundation with Kubernetes</h2>\n<p>Like many organizations, Informatica transitioned to a microservices architecture and Kubernetes for scalability, flexibility, and resiliency. This transition to decentralized microservices introduced challenges in monitoring and managing network communications and security, which led Informatica to adopt open source Istio. Istio is a service mesh project that helps organizations modernize their applications and thereby reduce complexity, improve security and reliability, and scale their applications to be more agile and flexible.</p>\n<p>As Informatica continued to grow and the number of Kubernetes clusters on Amazon Elastic Kubernetes Service (Amazon EKS) grew to roughly 300, product teams were challenged with several operational inefficiencies. Continuous update cycles resulted in separate pipelines for deployment because different product teams were running different versions of Istio. Additionally, Istio could not support the Informatica customers that were required by law to meet FedRAMP compliance or its government customers that required compliance with the Federal Information Processing Standards (FIPS) in order to build in AWS GovCloud.</p>\n<h2 id=\"solution-tetrate-istio-subscription-and-expert-enterprise-support\"><a href=\"#solution-tetrate-istio-subscription-and-expert-enterprise-support\"></a>Solution: Tetrate Istio Subscription and Expert Enterprise Support</h2>\n<p>Tetrate Istio Subscription (TIS) is built on top of open source Istio and solves for these operational inefficiencies. Not only did Tetrate help Informatica accelerate and streamline service mesh adoption throughout the organization, its Zero Trust security model provided multi-layer security for microservices and eliminated the time and effort required to manage authentication certificates at scale while delivering end-to-end FIPS compliance.</p>\n<blockquote>\n<p>“With Tetrate, we achieved FIPS compliance, elevated our security standards and streamlined the process of scaling on Amazon EKS. Informatica now benefits from enterprise-level support by Tetrate for our 300 clusters on Amazon EKS, which allows developers and engineers to focus on building and deploying applications rather than the complexities of network configurations and security.”</p>\n<p>— Deepak Deore, DevOps Architect, Informatica</p>\n</blockquote>\n<p>As Informatica upgraded its Istio instances, Tetrate provided weekly calls with engineers to resolve technical issues and disruptions, ensuring a smooth implementation and top-notch support.</p>\n<h2 id=\"benefits\"><a href=\"#benefits\"></a>Benefits</h2>\n<ul>\n<li><strong>Secured end-to-end FIPS compliance for AWS GovCloud</strong></li>\n<li><strong>Decreased manual effort of building ARM Images that run on AWS Graviton from 3-4 days to a few seconds</strong></li>\n<li><strong>Obtained customized Helm charts for its 300 Amazon EKS clusters, replacing the Istioctl command line tool that was incompatible with the team’s GitHub process and inconsistent with its Kubernetes configuration applications</strong></li>\n<li><strong>Improved business agility, enhanced security, and FIPS compliance</strong></li>\n</ul>\n<h2 id=\"results\"><a href=\"#results\"></a>Results</h2>\n<ul>\n<li>Informatica now receives enterprise-level support from Tetrate for 300+ clusters on Amazon EKS</li>\n<li>Developers and engineers can focus on building and deploying applications, not network configuration and security</li>\n<li>Streamlined operations and improved security posture for AWS GovCloud workloads</li>\n</ul>\n<hr>\n<p><a href=\"https://tetrate.io/wp-content/uploads/2023/06/AWS-Tetrate-Informatica-Case-Study.pdf\">Download the full case study (PDF)</a></p>",{headings:1169,localImagePaths:1188,remoteImagePaths:1189,frontmatter:1190,imagePaths:1192},[1170,1173,1176,1179,1182,1185],{depth:29,slug:1171,text:1172},"executive-summary","Executive Summary",{depth:29,slug:1174,text:1175},"about-informatica","About Informatica",{depth:29,slug:1177,text:1178},"challenge-building-a-microservices-foundation-with-kubernetes","Challenge: Building a Microservices Foundation with Kubernetes",{depth:29,slug:1180,text:1181},"solution-tetrate-istio-subscription-and-expert-enterprise-support","Solution: Tetrate Istio Subscription and Expert Enterprise Support",{depth:29,slug:1183,text:1184},"benefits","Benefits",{depth:29,slug:1186,text:1187},"results","Results",[],[],{title:1160,featuredImage:1162,description:1053,categories:1191,excerpt:1053},[1041],[],"informatica-gains-enterprise-level-support-with-tetrate-and-hardens-security/index.md","teg-data-sheet",{id:1194,data:1196,body:1200,filePath:1201,digest:1202,rendered:1203,legacyId:1212},{title:1197,categories:1198,featuredImage:1199,draft:20,description:1053,excerpt:1053},"Seamlessly Add Envoy to Your Stack",[1041],"/images/resources/teg-data-sheet-feature.webp","![Post Image](/images/resources/teg-data-sheet-feature.webp)\n\n\n**Are you using Kubernetes?** Tetrate Enterprise Gateway for Envoy (TEG) is the easiest way to get started with Envoy Gateway for production use cases.\n\nGet the power of Envoy Proxy in an easy-to-consume package managed by the Kubernetes Gateway API.","src/content/resources/teg-data-sheet/index.md","a91a93e120c35425",{html:1204,metadata:1205},"<p><img src=\"/images/resources/teg-data-sheet-feature.webp\" alt=\"Post Image\"></p>\n<p><strong>Are you using Kubernetes?</strong> Tetrate Enterprise Gateway for Envoy (TEG) is the easiest way to get started with Envoy Gateway for production use cases.</p>\n<p>Get the power of Envoy Proxy in an easy-to-consume package managed by the Kubernetes Gateway API.</p>",{headings:1206,localImagePaths:1207,remoteImagePaths:1208,frontmatter:1209,imagePaths:1211},[],[],[],{title:1197,featuredImage:1199,description:1053,categories:1210,excerpt:1053},[1041],[],"teg-data-sheet/index.md","simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh",{id:1213,data:1215,body:1221,filePath:1222,digest:1223,rendered:1224,legacyId:1233},{title:1216,categories:1217,featuredImage:1218,hubspotFormId:1219,draft:20,downloadLink:1220,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1219},"Simplify Kubernetes and Multi-Cloud Complexity with the Service Mesh",[1041],"/images/resources/zero-trust-fips-cover.png","e986753a-68e1-4aec-8837-742310040589","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/Simplify%20Kubernetes%20and%20Multi-Cloud%20Complexity%20with%20the%20Service%20Mesh.pdf","![Post Image](/images/resources/zero-trust-fips-cover.png)\n\n\nAs today’s enterprises shift to the cloud, [Kubernetes](/faq/what-does-kubernetes-do/) has emerged as the de facto platform for running containerized microservices. And while Kubernetes operates as a single cluster in many deployments, [enterprises](/resource/service-mesh-handbook/) and federal agencies inevitably run their applications on a complex, often confusing, architecture of multiple clusters deployed to a hybrid of multiple cloud providers and private data centers. This approach creates a new set of challenges. How do your services find each other? How do they communicate securely? How do you enforce access and communication policies? How do you troubleshoot and monitor health? Even on a single cluster, these are not trivial concerns. In a multi or hybrid-cloud environment, the complexity can be overwhelming.\n\nFor federal agencies and enterprise organizations seeking the flexibility to deploy applications on cloud providers that align with their cost, compliance or strategic consideration, the service mesh is an essential technology that addresses the above challenges. It empowers the consistent and streamlined deployment of applications across diverse Kubernetes clusters spanning different clouds. At a high level, the service mesh simplifies Kubernetes complexity by:\n\n1.  Decoupling traffic management from Kubernetes by running proxies\n2.  Centralizing and standardizing the management of networking concerns\n3.  Improving overall security posture using [mTLS to encrypt traffic](/learn/what-is-mtls/) for secure communication and enabling zero trust security operations across any environment\n4.  Ensuring your system remains performant and efficient as it scales\n\nAs an additional benefit, the service mesh collects a trove of valuable data from logs, traces, and metrics related to your network traffic. This data can help you create a more robust and \u2028 reliable system.\n\nWhen used together, Kubernetes and the service mesh provide a powerful platform for building  \nand operating complex distributed applications efficiently and securely across multi-cloud environments with less specialized expertise in each cloud and less manual toil. Developers, platform engineers, and network and security professionals are able to perform their jobs better individually while collectively innovating faster. These benefits, especially developer productivity, are inordinately impactful given today’s reliance on digital technologies. Increasing developer productivity, removing complexity and reducing toil provides a faster path to production and reduce time to market, which reduces time to value.","src/content/resources/simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh/index.md","ac0e59239ddc2fe9",{html:1225,metadata:1226},"<p><img src=\"/images/resources/zero-trust-fips-cover.png\" alt=\"Post Image\"></p>\n<p>As today’s enterprises shift to the cloud, <a href=\"/faq/what-does-kubernetes-do/\">Kubernetes</a> has emerged as the de facto platform for running containerized microservices. And while Kubernetes operates as a single cluster in many deployments, <a href=\"/resource/service-mesh-handbook/\">enterprises</a> and federal agencies inevitably run their applications on a complex, often confusing, architecture of multiple clusters deployed to a hybrid of multiple cloud providers and private data centers. This approach creates a new set of challenges. How do your services find each other? How do they communicate securely? How do you enforce access and communication policies? How do you troubleshoot and monitor health? Even on a single cluster, these are not trivial concerns. In a multi or hybrid-cloud environment, the complexity can be overwhelming.</p>\n<p>For federal agencies and enterprise organizations seeking the flexibility to deploy applications on cloud providers that align with their cost, compliance or strategic consideration, the service mesh is an essential technology that addresses the above challenges. It empowers the consistent and streamlined deployment of applications across diverse Kubernetes clusters spanning different clouds. At a high level, the service mesh simplifies Kubernetes complexity by:</p>\n<ol>\n<li>Decoupling traffic management from Kubernetes by running proxies</li>\n<li>Centralizing and standardizing the management of networking concerns</li>\n<li>Improving overall security posture using <a href=\"/learn/what-is-mtls/\">mTLS to encrypt traffic</a> for secure communication and enabling zero trust security operations across any environment</li>\n<li>Ensuring your system remains performant and efficient as it scales</li>\n</ol>\n<p>As an additional benefit, the service mesh collects a trove of valuable data from logs, traces, and metrics related to your network traffic. This data can help you create a more robust and \u2028 reliable system.</p>\n<p>When used together, Kubernetes and the service mesh provide a powerful platform for building<br>\nand operating complex distributed applications efficiently and securely across multi-cloud environments with less specialized expertise in each cloud and less manual toil. Developers, platform engineers, and network and security professionals are able to perform their jobs better individually while collectively innovating faster. These benefits, especially developer productivity, are inordinately impactful given today’s reliance on digital technologies. Increasing developer productivity, removing complexity and reducing toil provides a faster path to production and reduce time to market, which reduces time to value.</p>",{headings:1227,localImagePaths:1228,remoteImagePaths:1229,frontmatter:1230,imagePaths:1232},[],[],[],{title:1216,featuredImage:1218,description:1053,categories:1231,excerpt:1053,hubspotFormId:1219,modalFormId:1219,modalFormLinkText:1054,downloadLink:1220,useHubspotEmbed:1067},[1041],[],"simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh/index.md","istio-production-success",{id:1234,data:1236,body:1242,filePath:1243,digest:1244,rendered:1245,legacyId:1254},{title:1237,categories:1238,featuredImage:1239,hubspotFormId:1240,draft:20,downloadLink:1241,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1240},"5 Stages to Istio Production Success",[1041],"/images/resources/istio-5-stages-success-cover.webp","c363979d-b1a5-496e-830d-de2476b27022","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/5%20stages%20to%20Istio%20production%20success.pdf","![Post Image](/images/resources/istio-5-stages-success-cover.webp)\n\nDeploying an Istio service mesh can help companies improve the security, reliability and observability of their microservices. But it can also be complex.\n\nThat’s why Venafi and Tetrate collaborated to produce this technical guide, which provides practical advice and recommendations on how to deploy Istio for production success.\n\nWhat this guide covers:\n\n- Straightforward, step-by-step recommendations to successfully get started with using Istio\n- How to implement identity-based policy controls for improved Kubernetes workload security\n- Recommended open source tooling along with effective solutions for cloud native security","src/content/resources/istio-production-success/index.md","e49446fc69907132",{html:1246,metadata:1247},"<p><img src=\"/images/resources/istio-5-stages-success-cover.webp\" alt=\"Post Image\"></p>\n<p>Deploying an Istio service mesh can help companies improve the security, reliability and observability of their microservices. But it can also be complex.</p>\n<p>That’s why Venafi and Tetrate collaborated to produce this technical guide, which provides practical advice and recommendations on how to deploy Istio for production success.</p>\n<p>What this guide covers:</p>\n<ul>\n<li>Straightforward, step-by-step recommendations to successfully get started with using Istio</li>\n<li>How to implement identity-based policy controls for improved Kubernetes workload security</li>\n<li>Recommended open source tooling along with effective solutions for cloud native security</li>\n</ul>",{headings:1248,localImagePaths:1249,remoteImagePaths:1250,frontmatter:1251,imagePaths:1253},[],[],[],{title:1237,featuredImage:1239,description:1053,categories:1252,excerpt:1053,hubspotFormId:1240,modalFormId:1240,modalFormLinkText:1054,downloadLink:1241,useHubspotEmbed:1067},[1041],[],"istio-production-success/index.md","service-mesh-handbook",{id:1255,data:1257,body:1265,filePath:1266,digest:1267,rendered:1268,legacyId:1281},{title:1258,categories:1259,featuredImage:1261,date:1262,hubspotFormId:1263,draft:20,downloadLink:1264,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1263},"Service Mesh Handbook: Tetrate’s Guide to Service Mesh for the Enterprise",[1260],"istio & service mesh","/images/resources/tetrate-service-mesh-handbook-cover.png",["Date","2024-10-11T00:00:00.000Z"],"f07a53ac-3e6a-465d-8479-6ae5ffddab9e","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/White%20Papers/Tetrate_Service_Mesh_Handbook.pdf","![Post Image](/images/resources/tetrate-service-mesh-handbook-cover.png)\n\n### [](#tetrates-guide-to-service-mesh-for-the-enterprise)Tetrate's Guide to Service Mesh for the Enterprise\n\n[Service mesh](/what-is-istio-service-mesh/) has become a critical infrastructure for modern, cloud-native applications. It emerged in response to the challenges of digital transformation and application modernization, as it offers the consistent observability, connectivity, security, and resilience capabilities that modern microservices applications require.\n\nIn the enterprise, service mesh plays a critical role in the areas of application modernization and cloud migration. It’s a necessary tool for organizations that need a zero trust security posture, are managing multi-platform or multi-cloud environments, or a combination of the above.\n\nThe frequently heard term [zero trust architecture](/what-is-istio-service-mesh/) (ZTA), when applied to microservices applications, most often refers to service mesh-based architectures. Applications using a zero-trust architecture are developed and managed in production using the DevSecOps approach, which adds security concerns to the well-known DevOps paradigm.\n\nAn enterprise service mesh extends the benefits of a service mesh to every workload, regardless of the application architecture or underlying compute platform or environment, with added operational and security benefits such as seamless traffic shifting, multitenancy, and support for failovers across multiple clusters and even multiple clouds.\n\nDownload this paper for an overview of what a service mesh is, what it’s for, and its major benefits to the enterprise.","src/content/resources/service-mesh-handbook/index.md","3e1c75f92c74ba29",{html:1269,metadata:1270},"<p><img src=\"/images/resources/tetrate-service-mesh-handbook-cover.png\" alt=\"Post Image\"></p>\n<h3 id=\"tetrates-guide-to-service-mesh-for-the-enterprise\"><a href=\"#tetrates-guide-to-service-mesh-for-the-enterprise\"></a>Tetrate’s Guide to Service Mesh for the Enterprise</h3>\n<p><a href=\"/what-is-istio-service-mesh/\">Service mesh</a> has become a critical infrastructure for modern, cloud-native applications. It emerged in response to the challenges of digital transformation and application modernization, as it offers the consistent observability, connectivity, security, and resilience capabilities that modern microservices applications require.</p>\n<p>In the enterprise, service mesh plays a critical role in the areas of application modernization and cloud migration. It’s a necessary tool for organizations that need a zero trust security posture, are managing multi-platform or multi-cloud environments, or a combination of the above.</p>\n<p>The frequently heard term <a href=\"/what-is-istio-service-mesh/\">zero trust architecture</a> (ZTA), when applied to microservices applications, most often refers to service mesh-based architectures. Applications using a zero-trust architecture are developed and managed in production using the DevSecOps approach, which adds security concerns to the well-known DevOps paradigm.</p>\n<p>An enterprise service mesh extends the benefits of a service mesh to every workload, regardless of the application architecture or underlying compute platform or environment, with added operational and security benefits such as seamless traffic shifting, multitenancy, and support for failovers across multiple clusters and even multiple clouds.</p>\n<p>Download this paper for an overview of what a service mesh is, what it’s for, and its major benefits to the enterprise.</p>",{headings:1271,localImagePaths:1275,remoteImagePaths:1276,frontmatter:1277,imagePaths:1280},[1272],{depth:327,slug:1273,text:1274},"tetrates-guide-to-service-mesh-for-the-enterprise","Tetrate’s Guide to Service Mesh for the Enterprise",[],[],{title:1258,featuredImage:1261,description:1053,date:1278,categories:1279,excerpt:1053,hubspotFormId:1263,modalFormId:1263,modalFormLinkText:1054,downloadLink:1264,useHubspotEmbed:1067},"2024-10-11T00:00:00.000Z",[1260],[],"service-mesh-handbook/index.md","tetrate-pci-dss-40-guide",{id:1282,data:1284,body:1292,filePath:1293,digest:1294,rendered:1295,legacyId:1305},{title:1285,categories:1286,featuredImage:1288,date:1289,hubspotFormId:1290,draft:20,downloadLink:1291,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1290},"Tetrate PCI DSS 4.0 Guide",[1287],"compliance","/images/resources/tetrate-pci-dss-guide-cover.webp",["Date","2025-04-16T00:00:00.000Z"],"70d39d9f-f428-4ac9-bc80-ef5e937575d1","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/Tetrate%20Service%20Bridge%20PCI%20DSS%20Guide.pdf","![Post Image](/images/resources/tetrate-pci-dss-guide-cover.webp)\n\n\nThe Payment Card Industry Data Security Standard (PCI DSS) establishes stringent requirements to protect cardholder data and ensure the integrity of payment systems. As businesses increasingly adopt cloud-native, distributed architectures, meeting [PCI](/blog/istio-for-pci-compliance-implementing-pci-dss-4-0-1-with-service-mesh-security/) DSS compliance becomes more complex, particularly in managing the Cardholder Data Environment (CDE) across hybrid and multi-cloud infrastructures.\n\nModern architectures demand innovative solutions to address these challenges while maintaining robust security and operational efficiency. This is where Tetrate’s enterprise-grade gateway and service mesh solutions—Tetrate Istio Subscription (TIS), Tetrate Enterprise Gateway (TEG), and Tetrate Service Bridge (TSB)—come into play. These products provide a unified platform for securing, managing, and observing application traffic, enabling organizations to implement PCI DSS requirements with precision and ease.\n\nTetrate’s products empower businesses to achieve network segmentation, end-to-end encryption, fine-grained access control, and real-time observability—all essential components of PCI DSS compliance. By integrating seamlessly into existing infrastructure, Tetrate solutions reduce the scope of compliance efforts while enhancing the overall security posture of payment environments.\n\nThis white paper explores how Tetrate’s solutions align with and support key PCI DSS requirements. It demonstrates how these tools simplify compliance, reduce operational complexity, and provide the flexibility to scale securely across modern, distributed systems. Whether you’re securing a legacy environment, transitioning to the cloud, or managing a hybrid infrastructure, Tetrate enables you to meet the highest security standards while delivering consistent, compliant payment services.\n\n[Contact us](/contact-us/) to learn more about how Tetrate can help you meet commercial and federal compliance standards for applications operating in a regulatory environment.","src/content/resources/tetrate-pci-dss-40-guide/index.md","e45bab40e63bb66e",{html:1296,metadata:1297},"<p><img src=\"/images/resources/tetrate-pci-dss-guide-cover.webp\" alt=\"Post Image\"></p>\n<p>The Payment Card Industry Data Security Standard (PCI DSS) establishes stringent requirements to protect cardholder data and ensure the integrity of payment systems. As businesses increasingly adopt cloud-native, distributed architectures, meeting <a href=\"/blog/istio-for-pci-compliance-implementing-pci-dss-4-0-1-with-service-mesh-security/\">PCI</a> DSS compliance becomes more complex, particularly in managing the Cardholder Data Environment (CDE) across hybrid and multi-cloud infrastructures.</p>\n<p>Modern architectures demand innovative solutions to address these challenges while maintaining robust security and operational efficiency. This is where Tetrate’s enterprise-grade gateway and service mesh solutions—Tetrate Istio Subscription (TIS), Tetrate Enterprise Gateway (TEG), and Tetrate Service Bridge (TSB)—come into play. These products provide a unified platform for securing, managing, and observing application traffic, enabling organizations to implement PCI DSS requirements with precision and ease.</p>\n<p>Tetrate’s products empower businesses to achieve network segmentation, end-to-end encryption, fine-grained access control, and real-time observability—all essential components of PCI DSS compliance. By integrating seamlessly into existing infrastructure, Tetrate solutions reduce the scope of compliance efforts while enhancing the overall security posture of payment environments.</p>\n<p>This white paper explores how Tetrate’s solutions align with and support key PCI DSS requirements. It demonstrates how these tools simplify compliance, reduce operational complexity, and provide the flexibility to scale securely across modern, distributed systems. Whether you’re securing a legacy environment, transitioning to the cloud, or managing a hybrid infrastructure, Tetrate enables you to meet the highest security standards while delivering consistent, compliant payment services.</p>\n<p><a href=\"/contact-us/\">Contact us</a> to learn more about how Tetrate can help you meet commercial and federal compliance standards for applications operating in a regulatory environment.</p>",{headings:1298,localImagePaths:1299,remoteImagePaths:1300,frontmatter:1301,imagePaths:1304},[],[],[],{title:1285,featuredImage:1288,description:1053,date:1302,categories:1303,excerpt:1053,hubspotFormId:1290,modalFormId:1290,modalFormLinkText:1054,downloadLink:1291,useHubspotEmbed:1067},"2025-04-16T00:00:00.000Z",[1287],[],"tetrate-pci-dss-40-guide/index.md","tetrate-guide-to-federal-security-requirements-for-microservices",{id:1306,data:1308,body:1314,filePath:1315,digest:1316,rendered:1317,legacyId:1329},{title:1309,categories:1310,featuredImage:1311,hubspotFormId:1312,draft:20,downloadLink:1313,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1312},"Tetrate’s Guide to Federal Security Requirements for Microservices",[1041],"/images/resources/nist-security-standards-cover.webp","255c3c71-af5c-466b-9ca3-9c7f85a44df4","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/NIST%20SP%20800-204%20Series%20Requirements%20Guide.pdf","![Post Image](/images/resources/nist-security-standards-cover.webp)\n\n### [](#nist-sp-800-204-series-requirements-guide)NIST SP 800-204 Series Requirements Guide\n\nThe National Institute of Standards and Technology (NIST) provides a comprehensive set of security strategies for [microservices](/resource/tetrate-guide-to-federal-security-requirements-for-microservices/) applications that is part of a critical set of documents defining the US government’s standards of zero trust. Co-authored with NIST by Tetrate founding engineer Zack Butcher, the SP 800-204 series focuses on strategies for implementing a zero trust network architecture and secure communications between microservices as well as between microservices and external systems and end users.\n\nThis document offers a concise representation of NIST’s specific recommendations paired with notes on how Tetrate’s flagship application connectivity platform, Tetrate Service Bridge, implements the standard.\n\nDownload this guide for a comprehensive overview of NIST’s security recommendations for microservices.","src/content/resources/tetrate-guide-to-federal-security-requirements-for-microservices/index.md","afee9212139bdc61",{html:1318,metadata:1319},"<p><img src=\"/images/resources/nist-security-standards-cover.webp\" alt=\"Post Image\"></p>\n<h3 id=\"nist-sp-800-204-series-requirements-guide\"><a href=\"#nist-sp-800-204-series-requirements-guide\"></a>NIST SP 800-204 Series Requirements Guide</h3>\n<p>The National Institute of Standards and Technology (NIST) provides a comprehensive set of security strategies for <a href=\"/resource/tetrate-guide-to-federal-security-requirements-for-microservices/\">microservices</a> applications that is part of a critical set of documents defining the US government’s standards of zero trust. Co-authored with NIST by Tetrate founding engineer Zack Butcher, the SP 800-204 series focuses on strategies for implementing a zero trust network architecture and secure communications between microservices as well as between microservices and external systems and end users.</p>\n<p>This document offers a concise representation of NIST’s specific recommendations paired with notes on how Tetrate’s flagship application connectivity platform, Tetrate Service Bridge, implements the standard.</p>\n<p>Download this guide for a comprehensive overview of NIST’s security recommendations for microservices.</p>",{headings:1320,localImagePaths:1324,remoteImagePaths:1325,frontmatter:1326,imagePaths:1328},[1321],{depth:327,slug:1322,text:1323},"nist-sp-800-204-series-requirements-guide","NIST SP 800-204 Series Requirements Guide",[],[],{title:1309,featuredImage:1311,description:1053,categories:1327,excerpt:1053,hubspotFormId:1312,modalFormId:1312,modalFormLinkText:1054,downloadLink:1313,useHubspotEmbed:1067},[1041],[],"tetrate-guide-to-federal-security-requirements-for-microservices/index.md","tetrate-service-bridge-application-security-architecture",{id:1330,data:1332,body:1338,filePath:1339,digest:1340,rendered:1341,legacyId:1350},{title:1333,categories:1334,featuredImage:1335,hubspotFormId:1336,draft:20,downloadLink:1337,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1336},"Tetrate Service Bridge Application Security Architecture",[1041],"/images/resources/tetrate-service-bridge-security-cover.webp","7f1d96ff-237b-4d1d-a917-a02f67894a1a","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/Tetrate%20Service%20Bridge%20Application%20Security%20Architecture.pdf","![Post Image](/images/resources/tetrate-service-bridge-security-cover.webp)\n\n\nMany enterprises and government agencies strive to achieve a zero trust architecture, recognizing that traditional perimeter-based security is no longer sufficient in today’s threat landscape. The zero trust mindset acknowledges that attackers could potentially infiltrate any network. Therefore, the focus shifts to containing and mitigating attacks both spatially and temporally. To achieve this at runtime, you need to be performing at minimum five checks on every hop between components in your infrastructure:\n\n1.  Encryption in transit\n2.  Service authentication\n3.  Service to service authorization\n4.  End-user authentication\n5.  End-user to resource authorization\n\nIn addition to these runtime checks, we need the ability to monitor the system continuously to ensure policy is being enforced and to respond to changes on demand by updating policy.\n\nTetrate Service Bridge (TSB) uses the Istio service mesh to manage your application’s traffic. As a dedicated infrastructure layer, the service mesh is an invaluable security tool for modern applications. The service mesh’s sidecar intercepts all traffic in and out of your applications, where it acts as a universal policy enforcement point. This allows the service mesh – which centrally manages a fleet of your applications’ sidecars – to become the modern cloud native security kernel (NIST SP 800-204B).\n\nThe sidecar is able to enforce security and traffic policies, as well as generate telemetry to allow operators to close the loop on policy changes: they can author a change, observe its effect on the runtime and make additional changes as needed – all in a real time feedback control loop. In other words, the mesh provides the capabilities to implement the runtime controls needed to achieve a zero trust posture.","src/content/resources/tetrate-service-bridge-application-security-architecture/index.md","6a28915a7ae61158",{html:1342,metadata:1343},"<p><img src=\"/images/resources/tetrate-service-bridge-security-cover.webp\" alt=\"Post Image\"></p>\n<p>Many enterprises and government agencies strive to achieve a zero trust architecture, recognizing that traditional perimeter-based security is no longer sufficient in today’s threat landscape. The zero trust mindset acknowledges that attackers could potentially infiltrate any network. Therefore, the focus shifts to containing and mitigating attacks both spatially and temporally. To achieve this at runtime, you need to be performing at minimum five checks on every hop between components in your infrastructure:</p>\n<ol>\n<li>Encryption in transit</li>\n<li>Service authentication</li>\n<li>Service to service authorization</li>\n<li>End-user authentication</li>\n<li>End-user to resource authorization</li>\n</ol>\n<p>In addition to these runtime checks, we need the ability to monitor the system continuously to ensure policy is being enforced and to respond to changes on demand by updating policy.</p>\n<p>Tetrate Service Bridge (TSB) uses the Istio service mesh to manage your application’s traffic. As a dedicated infrastructure layer, the service mesh is an invaluable security tool for modern applications. The service mesh’s sidecar intercepts all traffic in and out of your applications, where it acts as a universal policy enforcement point. This allows the service mesh – which centrally manages a fleet of your applications’ sidecars – to become the modern cloud native security kernel (NIST SP 800-204B).</p>\n<p>The sidecar is able to enforce security and traffic policies, as well as generate telemetry to allow operators to close the loop on policy changes: they can author a change, observe its effect on the runtime and make additional changes as needed – all in a real time feedback control loop. In other words, the mesh provides the capabilities to implement the runtime controls needed to achieve a zero trust posture.</p>",{headings:1344,localImagePaths:1345,remoteImagePaths:1346,frontmatter:1347,imagePaths:1349},[],[],[],{title:1333,featuredImage:1335,description:1053,categories:1348,excerpt:1053,hubspotFormId:1336,modalFormId:1336,modalFormLinkText:1054,downloadLink:1337,useHubspotEmbed:1067},[1041],[],"tetrate-service-bridge-application-security-architecture/index.md","tis-data-sheet",{id:1351,data:1353,body:1357,filePath:1358,digest:1359,rendered:1360,legacyId:1369},{title:1354,categories:1355,featuredImage:1356,draft:20,description:1053,excerpt:1053},"FIPS-Certified Istio and Enterprise Support from the Founders and Maintainers of Istio and Envoy",[1041],"/images/resources/tis-cover-image.webp","![Post Image](/images/resources/tis-cover-image.webp)\n\nThe first thing you need to know when using open source software in mission-critical applications is how you're going to get support and guidance if something goes wrong. For Istio, the most widely- deployed service mesh, Tetrate has you covered.\n\nTetrate Istio Subscription (TIS) delivers the confidence you need to use Istio and Envoy in production environments. TIS includes Tetrate Istio Distro (TID), Istio lifecycle management and production Istio support.\n\nTetrate Istio Distro, the industry’s most popular open source Istio distribution, is developed and supported by the creators and maintainers of Istio and Envoy. TID is a distribution of upstream Istio that provides vetted builds tested against all major cloud platforms. TID offers expanded Istio version support and maintenance beyond upstream Istio. It also includes lifecycle and change management tooling that makes Istio easy to install, manage, and upgrade. It is used by organizations worldwide to ensure compliance, avoid issues, and reduce downtime if issues do arise and is the surest way to get into production with enterprise-grade Istio, especially for mission-critical applications in regulatory environments.","src/content/resources/tis-data-sheet/index.md","605d88106f5c7ad8",{html:1361,metadata:1362},"<p><img src=\"/images/resources/tis-cover-image.webp\" alt=\"Post Image\"></p>\n<p>The first thing you need to know when using open source software in mission-critical applications is how you’re going to get support and guidance if something goes wrong. For Istio, the most widely- deployed service mesh, Tetrate has you covered.</p>\n<p>Tetrate Istio Subscription (TIS) delivers the confidence you need to use Istio and Envoy in production environments. TIS includes Tetrate Istio Distro (TID), Istio lifecycle management and production Istio support.</p>\n<p>Tetrate Istio Distro, the industry’s most popular open source Istio distribution, is developed and supported by the creators and maintainers of Istio and Envoy. TID is a distribution of upstream Istio that provides vetted builds tested against all major cloud platforms. TID offers expanded Istio version support and maintenance beyond upstream Istio. It also includes lifecycle and change management tooling that makes Istio easy to install, manage, and upgrade. It is used by organizations worldwide to ensure compliance, avoid issues, and reduce downtime if issues do arise and is the surest way to get into production with enterprise-grade Istio, especially for mission-critical applications in regulatory environments.</p>",{headings:1363,localImagePaths:1364,remoteImagePaths:1365,frontmatter:1366,imagePaths:1368},[],[],[],{title:1354,featuredImage:1356,description:1053,categories:1367,excerpt:1053},[1041],[],"tis-data-sheet/index.md","zero-trust-solutions-for-federal-agencies",{id:1370,data:1372,body:1375,filePath:1376,digest:1377,rendered:1378,legacyId:1387},{title:1373,categories:1374,featuredImage:1218,draft:20,description:1053,excerpt:1053},"Tetrate Zero Trust Solutions for Federal Agencies",[1041],"![Post Image](/images/resources/zero-trust-fips-cover.png)\n\nModernize and strengthen your cybersecurity posture, reduce risk and better control access, assets and users while meeting U.S. federal government mandates for Zero Trust architecture.\n\n**At a glance**:\n\n- A Zero Trust approach helps organizations modernize and strengthen their security environment in order to limit or prevent attacks\n- An Executive Order on improving the nation’s cybersecurity instructed the government to move towards Zero Trust by 2024\n- The service mesh provides essential capabilities for effective Zero Trust.","src/content/resources/zero-trust-solutions-for-federal-agencies/index.md","a2f608c51580c77b",{html:1379,metadata:1380},"<p><img src=\"/images/resources/zero-trust-fips-cover.png\" alt=\"Post Image\"></p>\n<p>Modernize and strengthen your cybersecurity posture, reduce risk and better control access, assets and users while meeting U.S. federal government mandates for Zero Trust architecture.</p>\n<p><strong>At a glance</strong>:</p>\n<ul>\n<li>A Zero Trust approach helps organizations modernize and strengthen their security environment in order to limit or prevent attacks</li>\n<li>An Executive Order on improving the nation’s cybersecurity instructed the government to move towards Zero Trust by 2024</li>\n<li>The service mesh provides essential capabilities for effective Zero Trust.</li>\n</ul>",{headings:1381,localImagePaths:1382,remoteImagePaths:1383,frontmatter:1384,imagePaths:1386},[],[],[],{title:1373,featuredImage:1218,description:1053,categories:1385,excerpt:1053},[1041],[],"zero-trust-solutions-for-federal-agencies/index.md","tetrate-service-bridge-data-sheet",{id:1388,data:1390,body:1394,filePath:1395,digest:1396,rendered:1397,legacyId:1409},{title:1391,categories:1392,featuredImage:1393,draft:20,description:1053,excerpt:1053},"Tetrate Service Bridge Data Sheet",[1041],"/images/resources/tsb-data-sheet-cover.webp","![Post Image](/images/resources/tsb-data-sheet-cover.webp)\n\n\n### [](#tetrate-service-bridge)Tetrate Service Bridge\n\nTetrate Service Bridge unifies and simplifies the connectivity, security, observability and reliability for your entire application fleet—across Kubernetes clusters, virtual machines, bare metal servers and across clouds and on-premises deployments.  \nTSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.","src/content/resources/tetrate-service-bridge-data-sheet/index.md","3c187c5abcc46aa1",{html:1398,metadata:1399},"<p><img src=\"/images/resources/tsb-data-sheet-cover.webp\" alt=\"Post Image\"></p>\n<h3 id=\"tetrate-service-bridge\"><a href=\"#tetrate-service-bridge\"></a>Tetrate Service Bridge</h3>\n<p>Tetrate Service Bridge unifies and simplifies the connectivity, security, observability and reliability for your entire application fleet—across Kubernetes clusters, virtual machines, bare metal servers and across clouds and on-premises deployments.<br>\nTSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.</p>",{headings:1400,localImagePaths:1404,remoteImagePaths:1405,frontmatter:1406,imagePaths:1408},[1401],{depth:327,slug:1402,text:1403},"tetrate-service-bridge","Tetrate Service Bridge",[],[],{title:1391,featuredImage:1393,description:1053,categories:1407,excerpt:1053},[1041],[],"tetrate-service-bridge-data-sheet/index.md","zero-trust-architecture-white-paper",{id:1410,data:1412,body:1418,filePath:1419,digest:1420,rendered:1421,legacyId:1430},{title:1413,categories:1414,featuredImage:1415,hubspotFormId:1416,draft:20,downloadLink:1417,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1416},"Zero Trust Architecture White Paper",[1041],"/images/resources/zta-white-paper-cover.webp","1455b6e5-5546-40a9-959a-8c37e2ea38ed","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Resources%20and%20PDFs/White%20Papers/Tetrate-Zero-Trust-Architecture-White-Paper.pdf","![Post Image](/images/resources/zta-white-paper-cover.webp)\n\n[Zero trust security](/learn/zero-trust/what-is-zero-trust-security/) is getting a lot of attention—and for good reason. It’s a paradigm shift in security architectures that addresses significant problems with protecting the highly dynamic, distributed systems driving today’s cloud-native applications.\n\nBut, zero trust is more than just authentication and encryption on the wire. In this white paper, Zack Butcher—Tetrate founding engineer and co-author of [NIST SP 800-204a, “Building Secure Microservices-based Applications Using Service-Mesh Architecture”](https://csrc.nist.gov/pubs/sp/800/204/a/final)— offers an overview of Zero Trust for microservices applications and why a service mesh is the best way to implement it.","src/content/resources/zero-trust-architecture-white-paper/index.md","18d9d883e6b029ec",{html:1422,metadata:1423},"<p><img src=\"/images/resources/zta-white-paper-cover.webp\" alt=\"Post Image\"></p>\n<p><a href=\"/learn/zero-trust/what-is-zero-trust-security/\">Zero trust security</a> is getting a lot of attention—and for good reason. It’s a paradigm shift in security architectures that addresses significant problems with protecting the highly dynamic, distributed systems driving today’s cloud-native applications.</p>\n<p>But, zero trust is more than just authentication and encryption on the wire. In this white paper, Zack Butcher—Tetrate founding engineer and co-author of <a href=\"https://csrc.nist.gov/pubs/sp/800/204/a/final\">NIST SP 800-204a, “Building Secure Microservices-based Applications Using Service-Mesh Architecture”</a>— offers an overview of Zero Trust for microservices applications and why a service mesh is the best way to implement it.</p>",{headings:1424,localImagePaths:1425,remoteImagePaths:1426,frontmatter:1427,imagePaths:1429},[],[],[],{title:1413,featuredImage:1415,description:1053,categories:1428,excerpt:1053,hubspotFormId:1416,modalFormId:1416,modalFormLinkText:1054,downloadLink:1417,useHubspotEmbed:1067},[1041],[],"zero-trust-architecture-white-paper/index.md","zero-trust-fips-and-fedramp-for-cloud-native-applications",{id:1431,data:1433,body:1440,filePath:1441,digest:1442,rendered:1443,legacyId:1490},{title:1434,categories:1435,featuredImage:1218,date:1437,hubspotFormId:1438,draft:20,downloadLink:1439,description:1053,excerpt:1053,modalFormLinkText:1054,modalFormId:1438},"Zero Trust, FIPS and FedRAMP for Kubernetes and Cloud Native Applications",[1436],"zero trust",["Date","2024-11-28T00:00:00.000Z"],"dc7e3dbc-8c2f-4855-873b-3072f4941816","https://7637559.fs1.hubspotusercontent-na1.net/hubfs/7637559/Primer%20on%20Zero%20Trust%20and%20FIPS%20for%20Cloud%20Native%20Applications/Tetrate%20Primer%20on%20Zero%20Trust%20and%20FIPS%20for%20Cloud%20Native%20Applications.pdf","![Post Image](/images/resources/Zero-Trust-FIPS-and-FedRAMP-for-Cloud-Native-Applications.ZNsUaInf.jpg)\n\n## [](#executive-summary)Executive Summary\n\n- Enterprise information security architecture has become increasingly important as information systems have evolved into critical business assets.\n- Zero trust network architecture is emerging as a **preferred approach** for enterprises to secure both their traditional and modern, cloud-native applications. A key component of zero trust architecture is **encryption in transit**.\n- The Istio service mesh acts as a **security kernel** for distributed applications and serves as the foundation of a zero trust architecture, including providing comprehensive encryption in transit between system components.\n- Tetrate offers the first **FIPS-verified distribution** of Istio specifically designed for organizations requiring [FedRAMP](/blog/new-guide-to-zero-trust-fips-and-fedramp-for-cloud-native-applications-from-tetrate/) authorization and other organizations in regulated environments where the stock builds of Istio and Envoy aren’t suitable.\n- The Federal Information Processing Standards (FIPS) are the **information security standards** for the U.S. federal government. Information systems built and run by federal agencies, contractors, and vendors are required to adhere to FIPS.\n- FIPS is also widely regarded as a set of robust and trustworthy security standards that is often **adopted by private sector organizations**.\n- The National Institute of Standards and Technology (NIST), the standards body responsible for defining FIPS, runs a program (CMVP) to validate that cryptographic modules adhere to FIPS standards and are suitable for use in U.S. federal agency information systems. Those modules are said to be **_FIPS validated_**. Software certified by a CMVP-accredited laboratory as using FIPS-validated modules correctly is said to be **_FIPS verified_**.\n- Tetrate offers a 100% upstream distribution of Istio and Envoy called [**Tetrate Istio Distro (TID)**](/tetrate-istio-subscription/) that is the first to be FIPS verified.\n\n## [](#why-information-security-architecture-is-important)Why Information Security Architecture Is Important\n\nInformation security architecture has become increasingly important as information systems have evolved into critical business assets. Cyber crime [has reached industrial scale](https://www.csoonline.com/article/572923/the-strange-business-of-cybercrime.html) at the same time that business-critical functionality is growing more  sophisticated and powerful.\n\nThat power comes with greater complexity: there are more pieces and parts that need to communicate with each other over networks and more places where those components and users can operate outside the traditional data center and fortified network perimeter. These pieces, parts, people, places—and their access to each other—must all be secured.\n\nTraditional security architecture has long followed the paradigm of a strong fortified perimeter with more permissive access to internal systems once a user has been authenticated, authorized, and let through the castle gates.The complexity of modern, cloud-native applications and associated risk to critical business assets and reputation has prompted many organizations (and [the U.S. federal government](/blog/us-government-endorses-zero-trust-architecture-for-security/)) to re-think their information security architecture from the ground up.\n\n## [](#zero-trust-architecture-is-the-future-of-enterprise-network-security)Zero Trust Architecture Is the Future of Enterprise Network Security\n\nTraditional network security relies on a strong defensive perimeter around a trusted internal network to keep bad actors out and sensitive data in. In an increasingly complex networking environment, maintaining a robust perimeter is increasingly difficult.\n\nZero trust network architecture [is emerging as a preferred approach](/blog/us-government-endorses-zero-trust-architecture-for-security/) for enterprises to secure both their traditional and modern, cloud-native applications. Zero trust network architecture inverts the assumptions of perimeter security. In a zero trust network, every resource is protected internally as if it were exposed to the open internet.\n\nZack Butcher, Tetrate founding engineer and co-author of [the NIST standards for microservices securit](/blog/nist-standards-for-zero-trust-the-sp-800-204-series/)[y](/blog/nist-standards-for-zero-trust-the-sp-800-204-series/), identifies the following minimum five core runtime requirements for a zero trust architecture:\n\n1.  Communication within the system, with end-users, and with external systems should be encrypted (also known as encryption in transit) to ensure authenticity, integrity, and privacy;\n2.  All service-to-service communication should be mutually authenticated;\n3.  All service-to-service communication should be mutually authorized;\n4.  All end-user communication should be authenticated;\n5.  All end-user communication should be authorized.\n\nAs a dedicated infrastructure layer, the Istio service mesh acts as a security kernel for distributed applications that satisfies all five of these requirements. When we’re talking about FIPS, we’re solely focused on the first requirement: encryption in transit.\n\n## [](#istio-and-zero-trust-in-a-fedramp-environment)Istio and Zero Trust in a FedRAMP Environment\n\n### [](#what-is-fedramp)What Is FedRAMP?\n\nThe Federal Risk and Authorization Management Program (FedRAMP)  is a U.S. government-wide program that standardizes the security assessment, authorization and continuous monitoring processes for cloud products and services used by federal agencies. FedRAMP was established to ensure that cloud solutions meet specific security requirements and standards to protect sensitive government data.\n\nCloud service providers (CSPs) seeking to work with federal agencies must go through a rigorous assessment and authorization process to achieve a FedRAMP Authorization. This process involves a comprehensive security evaluation and documentation of how the cloud service meets specific security controls and requirements.\n\nOnce a cloud service has been granted a FedRAMP Authorization, it means that it has met the security standards required to serve federal government agencies, making it easier for agencies to adopt and use these cloud services while maintaining data security and compliance. FedRAMP helps ensure the protection of sensitive government information while promoting the adoption of modern cloud technologies within the federal government.\n\n### [](#fedramp-and-nist-sp-800-53)FedRAMP and NIST SP 800-53\n\nFedRAMP builds upon the security standards established by the National Institute of Standards and Technology (NIST) in its Special Publication  (SP) 800-53. NIST SP 800-53 provides a comprehensive catalog of security controls and control enhancements that federal agencies can use to secure their information systems and protect sensitive data.\n\nFedRAMP takes the security controls from NIST SP 800-53 and provides a framework for how CSPs should implement them in the context of cloud services. When a cloud service provider achieves FedRAMP Authorization, it means that their cloud offering has been assessed and found to comply with the specific security requirements outlined in both NIST SP 800-53 and FedRAMP.\n\n### [](#fedramp-is-now-law)FedRAMP Is Now Law\n\nIn December 2022, the [FedRAMP Authorization Act](https://www.congress.gov/117/bills/hr7776/BILLS-117hr7776enr.pdf#page=1055) was signed as part of the FY23 National Defense Authorization Act (NDAA). The Act codifies the FedRAMP program as the authoritative standardized approach to security assessment and authorization for cloud computing products and services that process unclassified federal information.\n\n## [](#whats-new-in-fedramp-rev-5)What’s New in FedRAMP Rev. 5\n\nThere are different revisions of NIST SP 800-53, with each revision introducing updates and improvements to the security controls framework. The latest revision, Rev. 5, [finalized in 2020](https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final), applies to new FedRAMP authorizations starting May 30, 2023.\n\n_Note: FedRAMP authorizations already in the_ initiation _or_ continuous monitoring _phase prior to May 30, 2023_ [_may continue to use Rev. 4 baselines_](https://www.fedramp.gov/assets/resources/documents/FedRAMP_Baselines_Rev5_Transition_Guide.pdf)_, but must identify the delta between their current Rev. 4 implementation and the Rev. 5 requirements plus develop plans to address that delta._\n\nBroadly, here’s what’s new in Rev. 5:\n\n- **Expansion to 20 control families**  (from 18 in Rev. 4), with some controls being restructured and renumbered. The control families are also realigned to better match current security threats and technology trends.\n- **Expansion of scope to include privacy controls** in addition to security controls to reflect the growing importance of privacy protection in information systems.\n- **Greater emphasis on supply chain risk management** and includes controls related to software supply chain security, reflecting the increasing importance of securing the software development and distribution process.\n- **Better alignment with other cybersecurity and privacy frameworks** such as NIST’s [Cybersecurity Framework (CSF)](https://www.nist.gov/cyberframework) and  [Privacy Framework](https://www.nist.gov/privacy-framework).\n- **Increased Emphasis on continuous monitoring and improvement** of security and privacy controls, aligning with modern cybersecurity practices.\n\n## [](#tetrate-istio-is-the-fastest-way-to-fedramp-ato-including-rev-5)Tetrate Istio Is the Fastest Way to FedRAMP ATO (Including Rev. 5)\n\nFedRAMP Rev. 5 requires FIPS-validated encryption for data in transit. While Istio is the de facto standard [security kernel for microservices applications](/blog/nist-standards-for-zero-trust-the-sp-800-204-series/), only Tetrate offers [a FIPS-validated distribution of Istio suitable for FedRAMP environments](/blog/how-tetrate-istio-distro-became-the-first-fips-compliant-istio-distribution/). New in FedRAMP Rev. 5 is a requirement to document cryptographic modules in use to protect data in transit and at rest. Tetrate’s Istio distribution is built into the documentation  template ([SSP Appendix Q](https://www.fedramp.gov/assets/resources/templates/SSP-Appendix-Q-Cryptographic-Modules-Table.docx)) required for all System Security Plans (SSPs)—so, you can be sure when it’s time to pass the Full Security Assessment in the FedRAMP Authorization phase , Tetrate has you covered. Tetrate Istio  is also available via approved software factories like the AWS Marketplace for GovCloud and [Platform One](https://p1.dso.mil/).\n\n## [](#what-is-fips)What Is FIPS?\n\nFIPS is a set of standards for information processing systems that all U.S. federal agencies, contractors, and vendors must adhere to. FIPS is also widely regarded as a set of robust and trustworthy security standards that is often adopted by private sector organizations.\n\nA key part of FIPS governs cryptographic modules, the specialized [hardware, software, and/or firmware](https://csrc.nist.gov/glossary/term/Cryptographicmodule) that encrypt data to ensure privacy and authenticity. NIST offers a validation program for cryptographic modules to ensure that validated modules are safe and approved for use in federal information systems.\n\n**FIPS.** [Federal Information Processing Standards](https://www.nist.gov/standardsgov/compliance-faqs-federal-information-processing-standards-fips) are the information security standards for the federal government defined by the National Institute of Standards and Technology (NIST) in accordance with the Federal Information Security Management Act (FISMA). As part of FIPS, the standards for cryptography are evolving, with the [FIPS 140-2](https://csrc.nist.gov/pubs/fips/140-2/upd2/final) document currently in effect and [FIPS 140-3](https://csrc.nist.gov/pubs/fips/140-3/final) published but not yet required by [authorizing officials (AOs)](https://csrc.nist.gov/glossary/term/ao), the officials who grant [authorization to operate (ATO)](https://csrc.nist.gov/glossary/term/authorization_to_operate), which is required to run any software for government use.\n\n**CMVP.** The [Cryptographic Module Validation Program (CMVP)](https://csrc.nist.gov/projects/cryptographic-module-validation-program), a joint effort between NIST and the Canadian Centre for Cyber Security, promotes the use of validated cryptographic modules. CMVP tracks crypto implementations that have been validated by auditors to conform to FIPS 140-2 and/or 140-3.\n\n**FedRAMP.** [FedRAMP](https://www.fedramp.gov/), the most common ATO in the U.S. government, requires the use of FIPS 140-2 validated modules for encrypting data in transit and at rest.\n\n## [](#what-is-fips-validated-vs-verified-vs-certified)What Is FIPS _Validated_ vs _Verified_ vs _Certified_?\n\n**FIPS validation**. As part of CMVP, NIST authorizes independent labs to audit cryptographic modules submitted for review. Modules that pass this review are said to be **_FIPS validated_**. The validation status of all modules submitted to CMVP is published via a [publicly searchable database](https://csrc.nist.gov/projects/cryptographic-module-validation-program/validated-modules/search).\n\n**FIPS verification.** Software that uses FIPS-validated cryptographic modules may need additional verification from an accredited testing lab that those cryptographic modules are used correctly in order to be authorized by a program like FedRAMP. Such software is said to be **_FIPS verified_**.\n\nThis approach to achieving federal authorization is a safer alternative to forking a module for independent FIPS validation. The forking approach has the sole advantage of listing the vendor of the forked module in the CMVP database. In contrast, the verification approach (what Tetrate does for Tetrate Istio Distro) offers the **smallest possible footprint of sensitive code** that must be FIPS validated and avoids the inevitable risk that a fork will drift from the more well-maintained upstream version of the module.\n\n**Applicability of validated modules.** Currently validated modules under FIPS 140-2 are [acceptable for use in new systems](https://csrc.nist.gov/projects/cryptographic-module-validation-program) until Sept. 21, 2026, after which they will be placed on the “Historical” list. At that point, their use will be allowed only for existing systems. Agencies should continue to use FIPS 140-2 validated modules until a FIPS 140-3 validated module becomes available.**FIPS certification.**_Certification_ is an industry term used to apply more generally to programs like CMVP that seek to provide some kind of provable compliance with a standard. In the context of FIPS 140, _certified_ essentially means _validated_.\n\n## [](#tetrate-istio-distro-tid-and-fips-validation)Tetrate Istio Distro (TID) and FIPS Validation\n\n[Tetrate Istio Distro](https://docs.tetrate.io/istio-distro/) is Tetrate’s hardened, performant, and fully upstream Istio distribution. It is also the first distribution of Istio to be FIPS verified for use in FedRAMP environments.\n\nThe Istio and Envoy binaries published by their respective project sites ([istio.io](https://istio.io/latest/docs/setup/getting-started/#download) and [envoyproxy.io](https://www.envoyproxy.io/docs/envoy/latest/start/install))  are not built using FIPS-validated crypto libraries. Those binaries are not approved for use by federal authorization programs such as FedRAMP.\n\nTetrate solves this problem by offering Istio and Envoy binaries that _are_ built with FIPS-validated crypto modules and independently verified by an accredited third-party testing laboratory.\n\n**Boring Crypto**. Istio—and its data plane of Envoy proxies—use [BoringSSL](https://boringssl.googlesource.com/boringssl/) which, in turn, [uses a core module called Boring Crypto](https://boringssl.googlesource.com/boringssl/+/master/crypto/fipsmodule/FIPS.md). Boring Crypto is FIPS 140-2 validated ([Certificate #3678](https://csrc.nist.gov/projects/cryptographic-module-validation-program/certificate/3678)). Boring Crypto’s FIPS 140-2 validation status will be active until Sept. 21, 2026, and the Boring Crypto team is actively working towards FIPS 140-3 validation.\n\n**Tetrate Istio Distro FIPS builds.** When pursuing FIPS validation for Istio and Envoy in TID, we used an existing crypto module that has already been validated (BoringSSL’s Boring Crypto). We then engaged an [NVLAP-accredited testing lab](https://www.nist.gov/nvlap) to verify that our distribution uses the CMVP-validated crypto module correctly. This lets us deliver **100% upstream Istio and Envoy** in TID, with no need for proprietary forks. And, when Boring Crypto achieves FIPS 140-3, we will update TID FIPS build certification accordingly.\n\nA less desirable option would have been to fork a crypto library, independently maintain it, and get it validated and listed in the CMVP database, then validate that the resulting distribution uses the CMVP validated crypto module correctly.\n\nAlthough our approach to getting FIPS validation for Istio and Envoy means Tetrate and TID do not have a unique entry in the CMVP database, we believe it is obviously better for users of TID and the Istio and Envoy communities since it does not require forking the highly sensitive functionality in cryptographic libraries.\n\n## [](#tetrate-istio-distro-is-the-fastest-way-to-get-to-production-with-istio)Tetrate Istio Distro Is the Fastest Way to Get to Production with Istio\n\nWhen you want to deploy Istio in production, the first question is where to get your Istio distribution. Tetrate Istio Distro is Tetrate’s hardened, performant, and fully upstream Istio distribution. Teams often choose to run TID because it’s simple to use and is built and supported by Tetrate’s Istio experts (in addition to being co-creators of Istio, we also [built the official CNCF course on Istio](https://tetr8.io/3UkuTyN)).\n\nTID support and FIPS-validated builds are available as a paid subscription service, [Tetrate Istio Subscription](/tetrate-istio-subscription/). It’s a great way to get started with Istio knowing you have a trusted distribution to begin with, have an expert team supporting you, and also have the option to get to FIPS compliance quickly if you need to. [Reach out to us](/contact-us/) to start a conversation.","src/content/resources/zero-trust-fips-and-fedramp-for-cloud-native-applications/index.md","118448463d1e59a7",{html:1444,metadata:1445},"<p><img src=\"/images/resources/Zero-Trust-FIPS-and-FedRAMP-for-Cloud-Native-Applications.ZNsUaInf.jpg\" alt=\"Post Image\"></p>\n<h2 id=\"executive-summary\"><a href=\"#executive-summary\"></a>Executive Summary</h2>\n<ul>\n<li>Enterprise information security architecture has become increasingly important as information systems have evolved into critical business assets.</li>\n<li>Zero trust network architecture is emerging as a <strong>preferred approach</strong> for enterprises to secure both their traditional and modern, cloud-native applications. A key component of zero trust architecture is <strong>encryption in transit</strong>.</li>\n<li>The Istio service mesh acts as a <strong>security kernel</strong> for distributed applications and serves as the foundation of a zero trust architecture, including providing comprehensive encryption in transit between system components.</li>\n<li>Tetrate offers the first <strong>FIPS-verified distribution</strong> of Istio specifically designed for organizations requiring <a href=\"/blog/new-guide-to-zero-trust-fips-and-fedramp-for-cloud-native-applications-from-tetrate/\">FedRAMP</a> authorization and other organizations in regulated environments where the stock builds of Istio and Envoy aren’t suitable.</li>\n<li>The Federal Information Processing Standards (FIPS) are the <strong>information security standards</strong> for the U.S. federal government. Information systems built and run by federal agencies, contractors, and vendors are required to adhere to FIPS.</li>\n<li>FIPS is also widely regarded as a set of robust and trustworthy security standards that is often <strong>adopted by private sector organizations</strong>.</li>\n<li>The National Institute of Standards and Technology (NIST), the standards body responsible for defining FIPS, runs a program (CMVP) to validate that cryptographic modules adhere to FIPS standards and are suitable for use in U.S. federal agency information systems. Those modules are said to be <strong><em>FIPS validated</em></strong>. Software certified by a CMVP-accredited laboratory as using FIPS-validated modules correctly is said to be <strong><em>FIPS verified</em></strong>.</li>\n<li>Tetrate offers a 100% upstream distribution of Istio and Envoy called <a href=\"/tetrate-istio-subscription/\"><strong>Tetrate Istio Distro (TID)</strong></a> that is the first to be FIPS verified.</li>\n</ul>\n<h2 id=\"why-information-security-architecture-is-important\"><a href=\"#why-information-security-architecture-is-important\"></a>Why Information Security Architecture Is Important</h2>\n<p>Information security architecture has become increasingly important as information systems have evolved into critical business assets. Cyber crime <a href=\"https://www.csoonline.com/article/572923/the-strange-business-of-cybercrime.html\">has reached industrial scale</a> at the same time that business-critical functionality is growing more  sophisticated and powerful.</p>\n<p>That power comes with greater complexity: there are more pieces and parts that need to communicate with each other over networks and more places where those components and users can operate outside the traditional data center and fortified network perimeter. These pieces, parts, people, places—and their access to each other—must all be secured.</p>\n<p>Traditional security architecture has long followed the paradigm of a strong fortified perimeter with more permissive access to internal systems once a user has been authenticated, authorized, and let through the castle gates.The complexity of modern, cloud-native applications and associated risk to critical business assets and reputation has prompted many organizations (and <a href=\"/blog/us-government-endorses-zero-trust-architecture-for-security/\">the U.S. federal government</a>) to re-think their information security architecture from the ground up.</p>\n<h2 id=\"zero-trust-architecture-is-the-future-of-enterprise-network-security\"><a href=\"#zero-trust-architecture-is-the-future-of-enterprise-network-security\"></a>Zero Trust Architecture Is the Future of Enterprise Network Security</h2>\n<p>Traditional network security relies on a strong defensive perimeter around a trusted internal network to keep bad actors out and sensitive data in. In an increasingly complex networking environment, maintaining a robust perimeter is increasingly difficult.</p>\n<p>Zero trust network architecture <a href=\"/blog/us-government-endorses-zero-trust-architecture-for-security/\">is emerging as a preferred approach</a> for enterprises to secure both their traditional and modern, cloud-native applications. Zero trust network architecture inverts the assumptions of perimeter security. In a zero trust network, every resource is protected internally as if it were exposed to the open internet.</p>\n<p>Zack Butcher, Tetrate founding engineer and co-author of <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series/\">the NIST standards for microservices securit</a><a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series/\">y</a>, identifies the following minimum five core runtime requirements for a zero trust architecture:</p>\n<ol>\n<li>Communication within the system, with end-users, and with external systems should be encrypted (also known as encryption in transit) to ensure authenticity, integrity, and privacy;</li>\n<li>All service-to-service communication should be mutually authenticated;</li>\n<li>All service-to-service communication should be mutually authorized;</li>\n<li>All end-user communication should be authenticated;</li>\n<li>All end-user communication should be authorized.</li>\n</ol>\n<p>As a dedicated infrastructure layer, the Istio service mesh acts as a security kernel for distributed applications that satisfies all five of these requirements. When we’re talking about FIPS, we’re solely focused on the first requirement: encryption in transit.</p>\n<h2 id=\"istio-and-zero-trust-in-a-fedramp-environment\"><a href=\"#istio-and-zero-trust-in-a-fedramp-environment\"></a>Istio and Zero Trust in a FedRAMP Environment</h2>\n<h3 id=\"what-is-fedramp\"><a href=\"#what-is-fedramp\"></a>What Is FedRAMP?</h3>\n<p>The Federal Risk and Authorization Management Program (FedRAMP)  is a U.S. government-wide program that standardizes the security assessment, authorization and continuous monitoring processes for cloud products and services used by federal agencies. FedRAMP was established to ensure that cloud solutions meet specific security requirements and standards to protect sensitive government data.</p>\n<p>Cloud service providers (CSPs) seeking to work with federal agencies must go through a rigorous assessment and authorization process to achieve a FedRAMP Authorization. This process involves a comprehensive security evaluation and documentation of how the cloud service meets specific security controls and requirements.</p>\n<p>Once a cloud service has been granted a FedRAMP Authorization, it means that it has met the security standards required to serve federal government agencies, making it easier for agencies to adopt and use these cloud services while maintaining data security and compliance. FedRAMP helps ensure the protection of sensitive government information while promoting the adoption of modern cloud technologies within the federal government.</p>\n<h3 id=\"fedramp-and-nist-sp-800-53\"><a href=\"#fedramp-and-nist-sp-800-53\"></a>FedRAMP and NIST SP 800-53</h3>\n<p>FedRAMP builds upon the security standards established by the National Institute of Standards and Technology (NIST) in its Special Publication  (SP) 800-53. NIST SP 800-53 provides a comprehensive catalog of security controls and control enhancements that federal agencies can use to secure their information systems and protect sensitive data.</p>\n<p>FedRAMP takes the security controls from NIST SP 800-53 and provides a framework for how CSPs should implement them in the context of cloud services. When a cloud service provider achieves FedRAMP Authorization, it means that their cloud offering has been assessed and found to comply with the specific security requirements outlined in both NIST SP 800-53 and FedRAMP.</p>\n<h3 id=\"fedramp-is-now-law\"><a href=\"#fedramp-is-now-law\"></a>FedRAMP Is Now Law</h3>\n<p>In December 2022, the <a href=\"https://www.congress.gov/117/bills/hr7776/BILLS-117hr7776enr.pdf#page=1055\">FedRAMP Authorization Act</a> was signed as part of the FY23 National Defense Authorization Act (NDAA). The Act codifies the FedRAMP program as the authoritative standardized approach to security assessment and authorization for cloud computing products and services that process unclassified federal information.</p>\n<h2 id=\"whats-new-in-fedramp-rev-5\"><a href=\"#whats-new-in-fedramp-rev-5\"></a>What’s New in FedRAMP Rev. 5</h2>\n<p>There are different revisions of NIST SP 800-53, with each revision introducing updates and improvements to the security controls framework. The latest revision, Rev. 5, <a href=\"https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final\">finalized in 2020</a>, applies to new FedRAMP authorizations starting May 30, 2023.</p>\n<p><em>Note: FedRAMP authorizations already in the</em> initiation <em>or</em> continuous monitoring <em>phase prior to May 30, 2023</em> <a href=\"https://www.fedramp.gov/assets/resources/documents/FedRAMP_Baselines_Rev5_Transition_Guide.pdf\"><em>may continue to use Rev. 4 baselines</em></a><em>, but must identify the delta between their current Rev. 4 implementation and the Rev. 5 requirements plus develop plans to address that delta.</em></p>\n<p>Broadly, here’s what’s new in Rev. 5:</p>\n<ul>\n<li><strong>Expansion to 20 control families</strong>  (from 18 in Rev. 4), with some controls being restructured and renumbered. The control families are also realigned to better match current security threats and technology trends.</li>\n<li><strong>Expansion of scope to include privacy controls</strong> in addition to security controls to reflect the growing importance of privacy protection in information systems.</li>\n<li><strong>Greater emphasis on supply chain risk management</strong> and includes controls related to software supply chain security, reflecting the increasing importance of securing the software development and distribution process.</li>\n<li><strong>Better alignment with other cybersecurity and privacy frameworks</strong> such as NIST’s <a href=\"https://www.nist.gov/cyberframework\">Cybersecurity Framework (CSF)</a> and  <a href=\"https://www.nist.gov/privacy-framework\">Privacy Framework</a>.</li>\n<li><strong>Increased Emphasis on continuous monitoring and improvement</strong> of security and privacy controls, aligning with modern cybersecurity practices.</li>\n</ul>\n<h2 id=\"tetrate-istio-is-the-fastest-way-to-fedramp-ato-including-rev-5\"><a href=\"#tetrate-istio-is-the-fastest-way-to-fedramp-ato-including-rev-5\"></a>Tetrate Istio Is the Fastest Way to FedRAMP ATO (Including Rev. 5)</h2>\n<p>FedRAMP Rev. 5 requires FIPS-validated encryption for data in transit. While Istio is the de facto standard <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series/\">security kernel for microservices applications</a>, only Tetrate offers <a href=\"/blog/how-tetrate-istio-distro-became-the-first-fips-compliant-istio-distribution/\">a FIPS-validated distribution of Istio suitable for FedRAMP environments</a>. New in FedRAMP Rev. 5 is a requirement to document cryptographic modules in use to protect data in transit and at rest. Tetrate’s Istio distribution is built into the documentation  template (<a href=\"https://www.fedramp.gov/assets/resources/templates/SSP-Appendix-Q-Cryptographic-Modules-Table.docx\">SSP Appendix Q</a>) required for all System Security Plans (SSPs)—so, you can be sure when it’s time to pass the Full Security Assessment in the FedRAMP Authorization phase , Tetrate has you covered. Tetrate Istio  is also available via approved software factories like the AWS Marketplace for GovCloud and <a href=\"https://p1.dso.mil/\">Platform One</a>.</p>\n<h2 id=\"what-is-fips\"><a href=\"#what-is-fips\"></a>What Is FIPS?</h2>\n<p>FIPS is a set of standards for information processing systems that all U.S. federal agencies, contractors, and vendors must adhere to. FIPS is also widely regarded as a set of robust and trustworthy security standards that is often adopted by private sector organizations.</p>\n<p>A key part of FIPS governs cryptographic modules, the specialized <a href=\"https://csrc.nist.gov/glossary/term/Cryptographicmodule\">hardware, software, and/or firmware</a> that encrypt data to ensure privacy and authenticity. NIST offers a validation program for cryptographic modules to ensure that validated modules are safe and approved for use in federal information systems.</p>\n<p><strong>FIPS.</strong> <a href=\"https://www.nist.gov/standardsgov/compliance-faqs-federal-information-processing-standards-fips\">Federal Information Processing Standards</a> are the information security standards for the federal government defined by the National Institute of Standards and Technology (NIST) in accordance with the Federal Information Security Management Act (FISMA). As part of FIPS, the standards for cryptography are evolving, with the <a href=\"https://csrc.nist.gov/pubs/fips/140-2/upd2/final\">FIPS 140-2</a> document currently in effect and <a href=\"https://csrc.nist.gov/pubs/fips/140-3/final\">FIPS 140-3</a> published but not yet required by <a href=\"https://csrc.nist.gov/glossary/term/ao\">authorizing officials (AOs)</a>, the officials who grant <a href=\"https://csrc.nist.gov/glossary/term/authorization_to_operate\">authorization to operate (ATO)</a>, which is required to run any software for government use.</p>\n<p><strong>CMVP.</strong> The <a href=\"https://csrc.nist.gov/projects/cryptographic-module-validation-program\">Cryptographic Module Validation Program (CMVP)</a>, a joint effort between NIST and the Canadian Centre for Cyber Security, promotes the use of validated cryptographic modules. CMVP tracks crypto implementations that have been validated by auditors to conform to FIPS 140-2 and/or 140-3.</p>\n<p><strong>FedRAMP.</strong> <a href=\"https://www.fedramp.gov/\">FedRAMP</a>, the most common ATO in the U.S. government, requires the use of FIPS 140-2 validated modules for encrypting data in transit and at rest.</p>\n<h2 id=\"what-is-fips-validated-vs-verified-vs-certified\"><a href=\"#what-is-fips-validated-vs-verified-vs-certified\"></a>What Is FIPS <em>Validated</em> vs <em>Verified</em> vs <em>Certified</em>?</h2>\n<p><strong>FIPS validation</strong>. As part of CMVP, NIST authorizes independent labs to audit cryptographic modules submitted for review. Modules that pass this review are said to be <strong><em>FIPS validated</em></strong>. The validation status of all modules submitted to CMVP is published via a <a href=\"https://csrc.nist.gov/projects/cryptographic-module-validation-program/validated-modules/search\">publicly searchable database</a>.</p>\n<p><strong>FIPS verification.</strong> Software that uses FIPS-validated cryptographic modules may need additional verification from an accredited testing lab that those cryptographic modules are used correctly in order to be authorized by a program like FedRAMP. Such software is said to be <strong><em>FIPS verified</em></strong>.</p>\n<p>This approach to achieving federal authorization is a safer alternative to forking a module for independent FIPS validation. The forking approach has the sole advantage of listing the vendor of the forked module in the CMVP database. In contrast, the verification approach (what Tetrate does for Tetrate Istio Distro) offers the <strong>smallest possible footprint of sensitive code</strong> that must be FIPS validated and avoids the inevitable risk that a fork will drift from the more well-maintained upstream version of the module.</p>\n<p><strong>Applicability of validated modules.</strong> Currently validated modules under FIPS 140-2 are <a href=\"https://csrc.nist.gov/projects/cryptographic-module-validation-program\">acceptable for use in new systems</a> until Sept. 21, 2026, after which they will be placed on the “Historical” list. At that point, their use will be allowed only for existing systems. Agencies should continue to use FIPS 140-2 validated modules until a FIPS 140-3 validated module becomes available.<strong>FIPS certification.</strong><em>Certification</em> is an industry term used to apply more generally to programs like CMVP that seek to provide some kind of provable compliance with a standard. In the context of FIPS 140, <em>certified</em> essentially means <em>validated</em>.</p>\n<h2 id=\"tetrate-istio-distro-tid-and-fips-validation\"><a href=\"#tetrate-istio-distro-tid-and-fips-validation\"></a>Tetrate Istio Distro (TID) and FIPS Validation</h2>\n<p><a href=\"https://docs.tetrate.io/istio-distro/\">Tetrate Istio Distro</a> is Tetrate’s hardened, performant, and fully upstream Istio distribution. It is also the first distribution of Istio to be FIPS verified for use in FedRAMP environments.</p>\n<p>The Istio and Envoy binaries published by their respective project sites (<a href=\"https://istio.io/latest/docs/setup/getting-started/#download\">istio.io</a> and <a href=\"https://www.envoyproxy.io/docs/envoy/latest/start/install\">envoyproxy.io</a>)  are not built using FIPS-validated crypto libraries. Those binaries are not approved for use by federal authorization programs such as FedRAMP.</p>\n<p>Tetrate solves this problem by offering Istio and Envoy binaries that <em>are</em> built with FIPS-validated crypto modules and independently verified by an accredited third-party testing laboratory.</p>\n<p><strong>Boring Crypto</strong>. Istio—and its data plane of Envoy proxies—use <a href=\"https://boringssl.googlesource.com/boringssl/\">BoringSSL</a> which, in turn, <a href=\"https://boringssl.googlesource.com/boringssl/+/master/crypto/fipsmodule/FIPS.md\">uses a core module called Boring Crypto</a>. Boring Crypto is FIPS 140-2 validated (<a href=\"https://csrc.nist.gov/projects/cryptographic-module-validation-program/certificate/3678\">Certificate #3678</a>). Boring Crypto’s FIPS 140-2 validation status will be active until Sept. 21, 2026, and the Boring Crypto team is actively working towards FIPS 140-3 validation.</p>\n<p><strong>Tetrate Istio Distro FIPS builds.</strong> When pursuing FIPS validation for Istio and Envoy in TID, we used an existing crypto module that has already been validated (BoringSSL’s Boring Crypto). We then engaged an <a href=\"https://www.nist.gov/nvlap\">NVLAP-accredited testing lab</a> to verify that our distribution uses the CMVP-validated crypto module correctly. This lets us deliver <strong>100% upstream Istio and Envoy</strong> in TID, with no need for proprietary forks. And, when Boring Crypto achieves FIPS 140-3, we will update TID FIPS build certification accordingly.</p>\n<p>A less desirable option would have been to fork a crypto library, independently maintain it, and get it validated and listed in the CMVP database, then validate that the resulting distribution uses the CMVP validated crypto module correctly.</p>\n<p>Although our approach to getting FIPS validation for Istio and Envoy means Tetrate and TID do not have a unique entry in the CMVP database, we believe it is obviously better for users of TID and the Istio and Envoy communities since it does not require forking the highly sensitive functionality in cryptographic libraries.</p>\n<h2 id=\"tetrate-istio-distro-is-the-fastest-way-to-get-to-production-with-istio\"><a href=\"#tetrate-istio-distro-is-the-fastest-way-to-get-to-production-with-istio\"></a>Tetrate Istio Distro Is the Fastest Way to Get to Production with Istio</h2>\n<p>When you want to deploy Istio in production, the first question is where to get your Istio distribution. Tetrate Istio Distro is Tetrate’s hardened, performant, and fully upstream Istio distribution. Teams often choose to run TID because it’s simple to use and is built and supported by Tetrate’s Istio experts (in addition to being co-creators of Istio, we also <a href=\"https://tetr8.io/3UkuTyN\">built the official CNCF course on Istio</a>).</p>\n<p>TID support and FIPS-validated builds are available as a paid subscription service, <a href=\"/tetrate-istio-subscription/\">Tetrate Istio Subscription</a>. It’s a great way to get started with Istio knowing you have a trusted distribution to begin with, have an expert team supporting you, and also have the option to get to FIPS compliance quickly if you need to. <a href=\"/contact-us/\">Reach out to us</a> to start a conversation.</p>",{headings:1446,localImagePaths:1484,remoteImagePaths:1485,frontmatter:1486,imagePaths:1489},[1447,1448,1451,1454,1457,1460,1463,1466,1469,1472,1475,1478,1481],{depth:29,slug:1171,text:1172},{depth:29,slug:1449,text:1450},"why-information-security-architecture-is-important","Why Information Security Architecture Is Important",{depth:29,slug:1452,text:1453},"zero-trust-architecture-is-the-future-of-enterprise-network-security","Zero Trust Architecture Is the Future of Enterprise Network Security",{depth:29,slug:1455,text:1456},"istio-and-zero-trust-in-a-fedramp-environment","Istio and Zero Trust in a FedRAMP Environment",{depth:327,slug:1458,text:1459},"what-is-fedramp","What Is FedRAMP?",{depth:327,slug:1461,text:1462},"fedramp-and-nist-sp-800-53","FedRAMP and NIST SP 800-53",{depth:327,slug:1464,text:1465},"fedramp-is-now-law","FedRAMP Is Now Law",{depth:29,slug:1467,text:1468},"whats-new-in-fedramp-rev-5","What’s New in FedRAMP Rev. 5",{depth:29,slug:1470,text:1471},"tetrate-istio-is-the-fastest-way-to-fedramp-ato-including-rev-5","Tetrate Istio Is the Fastest Way to FedRAMP ATO (Including Rev. 5)",{depth:29,slug:1473,text:1474},"what-is-fips","What Is FIPS?",{depth:29,slug:1476,text:1477},"what-is-fips-validated-vs-verified-vs-certified","What Is FIPS Validated vs Verified vs Certified?",{depth:29,slug:1479,text:1480},"tetrate-istio-distro-tid-and-fips-validation","Tetrate Istio Distro (TID) and FIPS Validation",{depth:29,slug:1482,text:1483},"tetrate-istio-distro-is-the-fastest-way-to-get-to-production-with-istio","Tetrate Istio Distro Is the Fastest Way to Get to Production with Istio",[],[],{title:1434,featuredImage:1218,description:1053,date:1487,categories:1488,excerpt:1053,hubspotFormId:1438,modalFormId:1438,modalFormLinkText:1054,downloadLink:1439,useHubspotEmbed:1067},"2024-11-28T00:00:00.000Z",[1436],[],"zero-trust-fips-and-fedramp-for-cloud-native-applications/index.md"];

export { _astro_dataLayerContent as default };
