const _astro_dataLayerContent = [["Map",1,2,9,10,1043,1044],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.9.2","content-config-digest","f36dcf9d3d14a462","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://astro-my.vercel.app/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"never\",\"output\":\"server\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":false,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"experimentalDefaultStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"nord\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"csp\":false},\"legacy\":{\"collections\":false}}","learn",["Map",11,12,51,52,76,77,108,109,139,140,170,171,195,196,220,221,254,255,300,301,331,332,369,370,407,408,458,459,493,494,603,604,637,638,743,744,781,782,840,841,908,909,970,971,1011,1012],"difference-between-ingress-and-service-in-kubernetes",{id:11,data:13,body:21,filePath:22,digest:23,rendered:24,legacyId:50},{title:14,excerpt:15,categories:16,date:18,description:19,draft:20},"Difference Between Ingress and Service in Kubernetes","Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A Service provides internal networking within a cluster, while Ingress manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network.",[17],"Kubernetes",["Date","2025-05-05T17:05:27.000Z"],"Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A Service provides internal networking within a…",false,"Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A **Service** provides internal networking within a cluster, while **Ingress** manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network architecture.\n\n## What is a Kubernetes Service?\n\nA Service defines a logical abstraction that describes how users, applications, or other systems can access a group of Pods within a cluster. Since Pods are ephemeral and may be restarted or moved across nodes, the Service ensures stable application connectivity and prevents disruptions. There are several types of Kubernetes Services:\n\n*   **ClusterIP (Default)** – Exposes the Service on a cluster-internal IP. This makes the Service reachable only from within the cluster. It is the default type if no other service type is specified.\n*   **NodePort** – Exposes the Service on each Node’s IP at a static port (the NodePort). Kubernetes also sets up a cluster IP, making the Service accessible as if it were a ClusterIP.  \n    **LoadBalancer** – Exposes the Service externally using an external load balancer. Kubernetes does not include a built-in load balancer, so you must provide one or integrate with a cloud provider that supports it.\n*   **ExternalName** – Maps the Service to the value of the externalName field (e.g., a DNS hostname). This configures the cluster’s DNS to return a CNAME record pointing to the external hostname. No proxying is used. \n\n## What is Kubernetes Ingress?\n\nIngress is a higher-level method that provides an API for HTTP/HTTPS access to externally exposed services. Rather than assigning external IPs to each Service (via LoadBalancer or NodePort), Ingress consolidates access through a single entry point and routes traffic to the appropriate Service based on defined rules. Ingress understands web concepts such as hostnames, URIs, and paths. It allows you to map traffic to different backends using rules defined through the Kubernetes API. \n\n## Key Differences Between Ingress and Service\n\nThe table below compares Kubernetes Service and Ingress for traffic management:\n\n**Kubernetes Service**\n\n**Kubernetes Ingress**\n\n**Purpose**\n\nInternal service discovery and traffic routing\n\nExternal traffic routing to services\n\n**Scope**\n\nManages Traffic inside the cluster\n\nManages traffic from outside the cluster\n\n**Types**\n\nClusterIP, NodePort, LoadBalancer\n\nUses Ingress Controllers (e.g. Envoy Gateway)\n\n**Load Balancing**\n\nInternal load balancing across Pods\n\nHTTPS based traffic routing\n\n**TLS Support**\n\nNot built in – requires manual addition\n\nSupports TLS termination for secure traffic\n\n## How Service and Ingress Work Together\n\nKubernetes deployments typically use both Services and Ingress to manage traffic flow. Use a Service to enable stable internal access to a set of Pods. Use Ingress as a gateway to direct external traffic to the appropriate application or microservice based on hostname, path, or other routing rules. Modern deployments often combine Ingress with a service mesh like Istio to add advanced traffic control, security, and observability. \n\n## Optimize Kubernetes Networking with Tetrate\n\nDesigning Ingress and Service configurations can be complex. Tetrate [Consulting](/kubernetes-consulting) helps organizations implement secure, scalable, and optimized Kubernetes networking architectures using Istio and advanced traffic management. Our experts can help you refine your Ingress, service mesh, and overall traffic management strategies. Connect with Tetrate’s consulting services for expert guidance on Kubernetes networking.","src/content/learn/difference-between-ingress-and-service-in-kubernetes/index.md","1c4981fd4d0fbc76",{html:25,metadata:26},"<p>Ingress and Service are two methods used to manage network traffic flow in Kubernetes, each serving a distinct purpose. A <strong>Service</strong> provides internal networking within a cluster, while <strong>Ingress</strong> manages external access to services from clients and other IT systems outside the cluster. Understanding their roles and how they complement each other is essential for optimizing your Kubernetes network architecture.</p>\n<h2 id=\"what-is-a-kubernetes-service\">What is a Kubernetes Service?</h2>\n<p>A Service defines a logical abstraction that describes how users, applications, or other systems can access a group of Pods within a cluster. Since Pods are ephemeral and may be restarted or moved across nodes, the Service ensures stable application connectivity and prevents disruptions. There are several types of Kubernetes Services:</p>\n<ul>\n<li><strong>ClusterIP (Default)</strong> – Exposes the Service on a cluster-internal IP. This makes the Service reachable only from within the cluster. It is the default type if no other service type is specified.</li>\n<li><strong>NodePort</strong> – Exposes the Service on each Node’s IP at a static port (the NodePort). Kubernetes also sets up a cluster IP, making the Service accessible as if it were a ClusterIP.<br>\n<strong>LoadBalancer</strong> – Exposes the Service externally using an external load balancer. Kubernetes does not include a built-in load balancer, so you must provide one or integrate with a cloud provider that supports it.</li>\n<li><strong>ExternalName</strong> – Maps the Service to the value of the externalName field (e.g., a DNS hostname). This configures the cluster’s DNS to return a CNAME record pointing to the external hostname. No proxying is used. </li>\n</ul>\n<h2 id=\"what-is-kubernetes-ingress\">What is Kubernetes Ingress?</h2>\n<p>Ingress is a higher-level method that provides an API for HTTP/HTTPS access to externally exposed services. Rather than assigning external IPs to each Service (via LoadBalancer or NodePort), Ingress consolidates access through a single entry point and routes traffic to the appropriate Service based on defined rules. Ingress understands web concepts such as hostnames, URIs, and paths. It allows you to map traffic to different backends using rules defined through the Kubernetes API. </p>\n<h2 id=\"key-differences-between-ingress-and-service\">Key Differences Between Ingress and Service</h2>\n<p>The table below compares Kubernetes Service and Ingress for traffic management:</p>\n<p><strong>Kubernetes Service</strong></p>\n<p><strong>Kubernetes Ingress</strong></p>\n<p><strong>Purpose</strong></p>\n<p>Internal service discovery and traffic routing</p>\n<p>External traffic routing to services</p>\n<p><strong>Scope</strong></p>\n<p>Manages Traffic inside the cluster</p>\n<p>Manages traffic from outside the cluster</p>\n<p><strong>Types</strong></p>\n<p>ClusterIP, NodePort, LoadBalancer</p>\n<p>Uses Ingress Controllers (e.g. Envoy Gateway)</p>\n<p><strong>Load Balancing</strong></p>\n<p>Internal load balancing across Pods</p>\n<p>HTTPS based traffic routing</p>\n<p><strong>TLS Support</strong></p>\n<p>Not built in – requires manual addition</p>\n<p>Supports TLS termination for secure traffic</p>\n<h2 id=\"how-service-and-ingress-work-together\">How Service and Ingress Work Together</h2>\n<p>Kubernetes deployments typically use both Services and Ingress to manage traffic flow. Use a Service to enable stable internal access to a set of Pods. Use Ingress as a gateway to direct external traffic to the appropriate application or microservice based on hostname, path, or other routing rules. Modern deployments often combine Ingress with a service mesh like Istio to add advanced traffic control, security, and observability. </p>\n<h2 id=\"optimize-kubernetes-networking-with-tetrate\">Optimize Kubernetes Networking with Tetrate</h2>\n<p>Designing Ingress and Service configurations can be complex. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> helps organizations implement secure, scalable, and optimized Kubernetes networking architectures using Istio and advanced traffic management. Our experts can help you refine your Ingress, service mesh, and overall traffic management strategies. Connect with Tetrate’s consulting services for expert guidance on Kubernetes networking.</p>",{headings:27,localImagePaths:44,remoteImagePaths:45,frontmatter:46,imagePaths:49},[28,32,35,38,41],{depth:29,slug:30,text:31},2,"what-is-a-kubernetes-service","What is a Kubernetes Service?",{depth:29,slug:33,text:34},"what-is-kubernetes-ingress","What is Kubernetes Ingress?",{depth:29,slug:36,text:37},"key-differences-between-ingress-and-service","Key Differences Between Ingress and Service",{depth:29,slug:39,text:40},"how-service-and-ingress-work-together","How Service and Ingress Work Together",{depth:29,slug:42,text:43},"optimize-kubernetes-networking-with-tetrate","Optimize Kubernetes Networking with Tetrate",[],[],{title:14,slug:11,date:47,description:19,categories:48,excerpt:15},["Date","2025-05-05T17:05:27.000Z"],[17],[],"difference-between-ingress-and-service-in-kubernetes/index.md","envoy-proxy",{id:51,data:53,body:59,filePath:60,digest:61,rendered:62,legacyId:75},{title:54,excerpt:55,categories:56,date:57,description:58,draft:20},"Envoy Proxy","Envoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. Envoy(/what-is-envoy-proxy/) is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, distributed application.",[17],["Date","2024-09-30T02:58:20.000Z"],"Discover the power of Envoy Proxy with Tetrate. Learn how this modern proxy enhances your network's security, scalability, and performance.","## What does Envoy mean?\n\nEnvoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. [Envoy](/what-is-envoy-proxy) is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, [distributed application architectures.](/blog/nist-sp-800-207a-explained-zero-trust-architecture-model-for-access-control)\n\n[Learn more ›](/what-is-envoy-proxy)","src/content/learn/envoy-proxy/index.md","c351b35c1ee273f9",{html:63,metadata:64},"<h2 id=\"what-does-envoy-mean\">What does Envoy mean?</h2>\n<p>Envoy is a high-performance, open-source proxy server designed for cloud-native applications. It is often used in conjunction with service mesh platforms like Istio to provide advanced traffic management, load balancing, and observability capabilities for microservices-based applications. <a href=\"/what-is-envoy-proxy\">Envoy</a> is built with a modular architecture that allows it to be easily extended with new functionality, and it supports a wide range of protocols and features, including HTTP/2, gRPC, WebSocket, and more. Envoy is designed to be scalable, resilient, and easy to operate, making it a popular choice for modern, <a href=\"/blog/nist-sp-800-207a-explained-zero-trust-architecture-model-for-access-control\">distributed application architectures.</a></p>\n<p><a href=\"/what-is-envoy-proxy\">Learn more ›</a></p>",{headings:65,localImagePaths:69,remoteImagePaths:70,frontmatter:71,imagePaths:74},[66],{depth:29,slug:67,text:68},"what-does-envoy-mean","What does Envoy mean?",[],[],{title:54,slug:51,date:72,description:58,categories:73,excerpt:55},["Date","2024-09-30T02:58:20.000Z"],[17],[],"envoy-proxy/index.md","container-ingress-traffic-management-capabilities",{id:76,data:78,body:85,filePath:86,digest:87,rendered:88,legacyId:107},{title:79,excerpt:80,categories:81,date:83,description:84,draft:20},"Container Ingress Traffic Management Capabilities","Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and.",[82],"Ingress",["Date","2025-05-05T17:04:33.000Z"],"Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress…","Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and security.\n\n## How Container Ingress Traffic Management Works\n\nKubernetes manages Ingress traffic using three main components: the Ingress resource, the Ingress controller, and supporting services:\n\n- **Ingress Resource** – Defines rules for routing HTTP/HTTPS traffic to internal services.\n- **Ingress Controller** – Processes the Ingress resource rules and directs traffic accordingly. Common examples include Envoy Gateway, NGINX, and HAProxy.\n\nA service mesh like Istio enhances Ingress traffic management by adding fine-grained control, mTLS encryption, and observability for monitoring traffic flows.\n\n## Why Proper Configuration is Crucial\n\nIncorrect Ingress traffic management can lead to serious problems, including:\n\n- **Application Downtime & Latency** – Poorly configured Ingress can cause misrouted traffic, overloaded services, or inefficient failover strategies, resulting in degraded performance or outages.\n- **Security Vulnerabilities** – Misconfigured TLS can expose sensitive data, creating compliance risks and potential data breaches.\n- **Traffic Bottlenecks** – Inadequate load balancing and improper rate limiting can overload some Pods while underutilizing others.\n- **Observability Gaps** – Without proper logging and monitoring, diagnosing traffic issues becomes difficult, increasing the time required to resolve incidents.\n\n## How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management\n\nManaging container Ingress traffic requires deep expertise in networking, security, and Kubernetes architecture. Tetrate [Consulting](/kubernetes-consulting) helps organizations configure Ingress controllers, service meshes, and TLS security to ensure optimal performance and protection. Our experts can help you fine-tune your Kubernetes Ingress, service mesh, and traffic management strategies.\n\nTetrate’s enterprise-grade Istio platform also simplifies and optimizes Ingress traffic management, making it easier to deploy secure, scalable applications in Kubernetes environments.","src/content/learn/container-ingress-traffic-management-capabilities/index.md","debf7542db4a22ef",{html:89,metadata:90},"<p>Managing Ingress traffic for containerized applications in Kubernetes is essential for ensuring smooth, secure, and reliable application performance. Container Ingress traffic management refers to how external requests are handled, routed, and secured as they enter a Kubernetes cluster. Properly configuring Ingress controllers ensures high availability, scalability, and security.</p>\n<h2 id=\"how-container-ingress-traffic-management-works\">How Container Ingress Traffic Management Works</h2>\n<p>Kubernetes manages Ingress traffic using three main components: the Ingress resource, the Ingress controller, and supporting services:</p>\n<ul>\n<li><strong>Ingress Resource</strong> – Defines rules for routing HTTP/HTTPS traffic to internal services.</li>\n<li><strong>Ingress Controller</strong> – Processes the Ingress resource rules and directs traffic accordingly. Common examples include Envoy Gateway, NGINX, and HAProxy.</li>\n</ul>\n<p>A service mesh like Istio enhances Ingress traffic management by adding fine-grained control, mTLS encryption, and observability for monitoring traffic flows.</p>\n<h2 id=\"why-proper-configuration-is-crucial\">Why Proper Configuration is Crucial</h2>\n<p>Incorrect Ingress traffic management can lead to serious problems, including:</p>\n<ul>\n<li><strong>Application Downtime &#x26; Latency</strong> – Poorly configured Ingress can cause misrouted traffic, overloaded services, or inefficient failover strategies, resulting in degraded performance or outages.</li>\n<li><strong>Security Vulnerabilities</strong> – Misconfigured TLS can expose sensitive data, creating compliance risks and potential data breaches.</li>\n<li><strong>Traffic Bottlenecks</strong> – Inadequate load balancing and improper rate limiting can overload some Pods while underutilizing others.</li>\n<li><strong>Observability Gaps</strong> – Without proper logging and monitoring, diagnosing traffic issues becomes difficult, increasing the time required to resolve incidents.</li>\n</ul>\n<h2 id=\"how-tetrates-istio-based-solutions-improve-ingress-traffic-management\">How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management</h2>\n<p>Managing container Ingress traffic requires deep expertise in networking, security, and Kubernetes architecture. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> helps organizations configure Ingress controllers, service meshes, and TLS security to ensure optimal performance and protection. Our experts can help you fine-tune your Kubernetes Ingress, service mesh, and traffic management strategies.</p>\n<p>Tetrate’s enterprise-grade Istio platform also simplifies and optimizes Ingress traffic management, making it easier to deploy secure, scalable applications in Kubernetes environments.</p>",{headings:91,localImagePaths:101,remoteImagePaths:102,frontmatter:103,imagePaths:106},[92,95,98],{depth:29,slug:93,text:94},"how-container-ingress-traffic-management-works","How Container Ingress Traffic Management Works",{depth:29,slug:96,text:97},"why-proper-configuration-is-crucial","Why Proper Configuration is Crucial",{depth:29,slug:99,text:100},"how-tetrates-istio-based-solutions-improve-ingress-traffic-management","How Tetrate’s Istio-Based Solutions Improve Ingress Traffic Management",[],[],{title:79,slug:76,date:104,description:84,categories:105,excerpt:80},["Date","2025-05-05T17:04:33.000Z"],[82],[],"container-ingress-traffic-management-capabilities/index.md","ingress-controller-vs-load-balancer",{id:108,data:110,body:116,filePath:117,digest:118,rendered:119,legacyId:138},{title:111,excerpt:112,categories:113,date:114,description:115,draft:20},"Ingress Controller vs Load Balancer","When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and.",[82],["Date","2025-04-14T17:07:11.000Z"],"When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress…","When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and performance.\n\n## Load Balancers in Kubernetes?\n\nA load balancer distributes incoming traffic across multiple instances of a service to ensure high availability and reliability. Kubernetes supports external load balancers, often provided by cloud platforms like AWS, Azure, and GCP, or a dedicated load balancer solution running in a virtual machine.  These load balancers can all allocate a public IP and direct traffic to backend services running in a Kubernetes cluster.  It’s typical to use a Load Balancer when:\n\n- You need direct external access to a service without additional routing logic.\n- You’re running Kubernetes on a cloud platform with managed load balancing.\n- You need global traffic distribution across multiple clusters or regions.\n\n## What is an Ingress Controller?\n\nAn Ingress Controller acts as a traffic gateway, managing HTTPS requests and routing them to the appropriate Kubernetes Services based on domain names, paths, or other routing criteria. Unlike a load balancer, an Ingress Controller consolidates external access through a single entry point, reducing the number of external IPs required. It’s appropriate to use an Ingress Controller when:\n\n- You have multiple services that need controlled access under a single domain.\n- You require advanced traffic routing (e.g., path-based, host-based, or header-based routing).\n- You need built-in TLS termination for secure HTTPS communication.\n- You want to integrate authentication, rate limiting, and observability.\n\nIngress Controllers and Load Balancers complement each other. A typical best practice is to use a Load Balancer at the infrastructure level to direct traffic to an Ingress Controller, which then manages fine-grained traffic routing to internal services.\n\n## Tetrate Simplifies Ingress and Load Balancing\n\nCorrectly configuring Ingress Controllers and Load Balancers delivers optimal scalability, security, and efficiency. The Tetrate Consulting Services team provides expert guidance on designing and deploying optimized, secure, and resilient Kubernetes environments. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.","src/content/learn/ingress-controller-vs-load-balancer/index.md","916f8acaa817df23",{html:120,metadata:121},"<p>When managing traffic flow in Kubernetes, various ways exist to control routing to pods and other microservices running in a cluster. For example, using Ingress Controllers and Load Balancers. While both direct traffic to applications and microservices, they use different methods. Understanding their differences and when to use each is essential for optimizing Kubernetes networking, security, and performance.</p>\n<h2 id=\"load-balancers-in-kubernetes\">Load Balancers in Kubernetes?</h2>\n<p>A load balancer distributes incoming traffic across multiple instances of a service to ensure high availability and reliability. Kubernetes supports external load balancers, often provided by cloud platforms like AWS, Azure, and GCP, or a dedicated load balancer solution running in a virtual machine.  These load balancers can all allocate a public IP and direct traffic to backend services running in a Kubernetes cluster.  It’s typical to use a Load Balancer when:</p>\n<ul>\n<li>You need direct external access to a service without additional routing logic.</li>\n<li>You’re running Kubernetes on a cloud platform with managed load balancing.</li>\n<li>You need global traffic distribution across multiple clusters or regions.</li>\n</ul>\n<h2 id=\"what-is-an-ingress-controller\">What is an Ingress Controller?</h2>\n<p>An Ingress Controller acts as a traffic gateway, managing HTTPS requests and routing them to the appropriate Kubernetes Services based on domain names, paths, or other routing criteria. Unlike a load balancer, an Ingress Controller consolidates external access through a single entry point, reducing the number of external IPs required. It’s appropriate to use an Ingress Controller when:</p>\n<ul>\n<li>You have multiple services that need controlled access under a single domain.</li>\n<li>You require advanced traffic routing (e.g., path-based, host-based, or header-based routing).</li>\n<li>You need built-in TLS termination for secure HTTPS communication.</li>\n<li>You want to integrate authentication, rate limiting, and observability.</li>\n</ul>\n<p>Ingress Controllers and Load Balancers complement each other. A typical best practice is to use a Load Balancer at the infrastructure level to direct traffic to an Ingress Controller, which then manages fine-grained traffic routing to internal services.</p>\n<h2 id=\"tetrate-simplifies-ingress-and-load-balancing\">Tetrate Simplifies Ingress and Load Balancing</h2>\n<p>Correctly configuring Ingress Controllers and Load Balancers delivers optimal scalability, security, and efficiency. The Tetrate Consulting Services team provides expert guidance on designing and deploying optimized, secure, and resilient Kubernetes environments. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.</p>",{headings:122,localImagePaths:132,remoteImagePaths:133,frontmatter:134,imagePaths:137},[123,126,129],{depth:29,slug:124,text:125},"load-balancers-in-kubernetes","Load Balancers in Kubernetes?",{depth:29,slug:127,text:128},"what-is-an-ingress-controller","What is an Ingress Controller?",{depth:29,slug:130,text:131},"tetrate-simplifies-ingress-and-load-balancing","Tetrate Simplifies Ingress and Load Balancing",[],[],{title:111,slug:108,date:135,description:115,categories:136,excerpt:112},["Date","2025-04-14T17:07:11.000Z"],[82],[],"ingress-controller-vs-load-balancer/index.md","how-to-secure-kubernetes",{id:139,data:141,body:147,filePath:148,digest:149,rendered:150,legacyId:169},{title:142,excerpt:143,categories:144,date:145,description:146,draft:20},"How To Secure Kubernetes","Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes.",[17],["Date","2024-10-24T03:53:52.000Z"],"Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments…","Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes security.\n\n## Securing Kubernetes\n\nTo secure Kubernetes deployments with manageable overhead, it’s best to follow industry best practices that have been devised and honed over time. No single tool or process can be adopted to deliver easier management and enhanced security for Kubernetes-based infrastructure. Like other IT management and cybersecurity areas, securing Kubernetes requires a multilayered approach using built-in and external functionality. Here is a list of best practices that should be on your radar when planning and implementing Kubernetes security.\n\n1.  Use namespaces and access control—[(RBAC (role-based access control), ABAC (attribute-based access control), or NGAC (next-generation access control)](/blog/rbac-vs-abac-vs-ngac)—to isolate workloads and minimize the potential impact of a security breach. Each application should run in its own namespace with granular access control policy to enforce least privilege access.\n2.  Secure Kubernetes control plane components, such as the API server. Implement strong authentication and authorization policies, encrypt data in transit, and limit administrator (and regular user) access to the control plane.\n3.  Secure the container runtime and images. Scan images for vulnerabilities, enforce least privilege access to containers, and monitor containers in production for suspicious activity. \n4.  Network security is crucial. Employ [a service mesh](/what-is-istio-service-mesh) to encrypt traffic between services, enforce authentication and authorization policies, and segment applications. Implement network policies to restrict traffic between pods and namespaces.\n5.  Protect application secrets using secrets management tools and encrypt them at rest. Change secrets frequently to minimize the risk of their compromise.\n6.  Implement logging and monitoring to detect threats. Use sophisticated monitoring and alerting tools to flag suspicious or anomalous activity.\n7.  Develop an incident response plan and practice it regularly. Having such a plan and knowing how to use it is critical for quickly containing cyber-attacks and recovering from incidents.\n\n## How Tetrate Simplifies Kubernetes Security\n\nImplementing these security best practices across multiple microservices and containers deployed via Kubernetes is complex and time-consuming. The solutions available from Tetrate can reduce this complexity and the time needed to configure and monitor a Kubernetes-based landscape.\n\nTetrate offers a comprehensive gateway and service mesh platform designed to secure service communication within Kubernetes environments. With Tetrate, you can:\n\n*   Effortlessly encrypt all service-to-service communication.\n*   Implement detailed access policies based on identity and context.\n*   Monitor service behavior to identify potential threats.\n*   Apply consistent high-level security and access policies.\n\nTetrate integrates with authentication/authorization services, certificate management, workload identity, and other essential security tools for comprehensive cloud-native security. Organizations using Tetrate simplify their Kubernetes management experience and enhance their overall security posture.\n\n## Final Thoughts\n\nIt is crucial to secure Kubernetes, but doing so can be complex. Organizations can mitigate risks effectively by prioritizing basic best practices such as isolation, access control, runtime security, secrets management, and monitoring. Implementing these best practices consistently and at scale is greatly simplified by using Tetrate.","src/content/learn/how-to-secure-kubernetes/index.md","f0c0d02fab0099e7",{html:151,metadata:152},"<p>Kubernetes is now the leading orchestration platform for deploying and managing containerized applications. However, the distributed nature of Kubernetes environments also introduces security challenges. In this article, we’ll look at best practices for securing Kubernetes and touch on how solutions from Tetrate can help simplify management while bolstering Kubernetes security.</p>\n<h2 id=\"securing-kubernetes\">Securing Kubernetes</h2>\n<p>To secure Kubernetes deployments with manageable overhead, it’s best to follow industry best practices that have been devised and honed over time. No single tool or process can be adopted to deliver easier management and enhanced security for Kubernetes-based infrastructure. Like other IT management and cybersecurity areas, securing Kubernetes requires a multilayered approach using built-in and external functionality. Here is a list of best practices that should be on your radar when planning and implementing Kubernetes security.</p>\n<ol>\n<li>Use namespaces and access control—<a href=\"/blog/rbac-vs-abac-vs-ngac\">(RBAC (role-based access control), ABAC (attribute-based access control), or NGAC (next-generation access control)</a>—to isolate workloads and minimize the potential impact of a security breach. Each application should run in its own namespace with granular access control policy to enforce least privilege access.</li>\n<li>Secure Kubernetes control plane components, such as the API server. Implement strong authentication and authorization policies, encrypt data in transit, and limit administrator (and regular user) access to the control plane.</li>\n<li>Secure the container runtime and images. Scan images for vulnerabilities, enforce least privilege access to containers, and monitor containers in production for suspicious activity. </li>\n<li>Network security is crucial. Employ <a href=\"/what-is-istio-service-mesh\">a service mesh</a> to encrypt traffic between services, enforce authentication and authorization policies, and segment applications. Implement network policies to restrict traffic between pods and namespaces.</li>\n<li>Protect application secrets using secrets management tools and encrypt them at rest. Change secrets frequently to minimize the risk of their compromise.</li>\n<li>Implement logging and monitoring to detect threats. Use sophisticated monitoring and alerting tools to flag suspicious or anomalous activity.</li>\n<li>Develop an incident response plan and practice it regularly. Having such a plan and knowing how to use it is critical for quickly containing cyber-attacks and recovering from incidents.</li>\n</ol>\n<h2 id=\"how-tetrate-simplifies-kubernetes-security\">How Tetrate Simplifies Kubernetes Security</h2>\n<p>Implementing these security best practices across multiple microservices and containers deployed via Kubernetes is complex and time-consuming. The solutions available from Tetrate can reduce this complexity and the time needed to configure and monitor a Kubernetes-based landscape.</p>\n<p>Tetrate offers a comprehensive gateway and service mesh platform designed to secure service communication within Kubernetes environments. With Tetrate, you can:</p>\n<ul>\n<li>Effortlessly encrypt all service-to-service communication.</li>\n<li>Implement detailed access policies based on identity and context.</li>\n<li>Monitor service behavior to identify potential threats.</li>\n<li>Apply consistent high-level security and access policies.</li>\n</ul>\n<p>Tetrate integrates with authentication/authorization services, certificate management, workload identity, and other essential security tools for comprehensive cloud-native security. Organizations using Tetrate simplify their Kubernetes management experience and enhance their overall security posture.</p>\n<h2 id=\"final-thoughts\">Final Thoughts</h2>\n<p>It is crucial to secure Kubernetes, but doing so can be complex. Organizations can mitigate risks effectively by prioritizing basic best practices such as isolation, access control, runtime security, secrets management, and monitoring. Implementing these best practices consistently and at scale is greatly simplified by using Tetrate.</p>",{headings:153,localImagePaths:163,remoteImagePaths:164,frontmatter:165,imagePaths:168},[154,157,160],{depth:29,slug:155,text:156},"securing-kubernetes","Securing Kubernetes",{depth:29,slug:158,text:159},"how-tetrate-simplifies-kubernetes-security","How Tetrate Simplifies Kubernetes Security",{depth:29,slug:161,text:162},"final-thoughts","Final Thoughts",[],[],{title:142,slug:139,date:166,description:146,categories:167,excerpt:143},["Date","2024-10-24T03:53:52.000Z"],[17],[],"how-to-secure-kubernetes/index.md","istio-service-mesh",{id:170,data:172,body:178,filePath:179,digest:180,rendered:181,legacyId:194},{title:173,excerpt:174,categories:175,date:176,description:177,draft:20},"Istio Service Mesh","Learn more.",[17],["Date","2024-10-17T08:15:11.000Z"],"Explore Tetrate's comprehensive Istio service mesh guide. Discover how Istio enhances microservices with robust traffic management, security, and observability.","## Overview\n\n[Istio](/external-link/)\n\n[Learn more ›](/what-is-istio-service-mesh)","src/content/learn/istio-service-mesh/index.md","6514eb68e7c98cdb",{html:182,metadata:183},"<h2 id=\"overview\">Overview</h2>\n<p><a href=\"/external-link/\">Istio</a></p>\n<p><a href=\"/what-is-istio-service-mesh\">Learn more ›</a></p>",{headings:184,localImagePaths:188,remoteImagePaths:189,frontmatter:190,imagePaths:193},[185],{depth:29,slug:186,text:187},"overview","Overview",[],[],{title:173,slug:170,date:191,description:177,categories:192,excerpt:174},["Date","2024-10-17T08:15:11.000Z"],[17],[],"istio-service-mesh/index.md","kubernetes-traffic-routing-and-control",{id:195,data:197,body:203,filePath:204,digest:205,rendered:206,legacyId:219},{title:198,excerpt:199,categories:200,date:201,description:202,draft:20},"Kubernetes Traffic Routing and Control","Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include Services, Ingress Controllers, and Service Meshes. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and.",[17],["Date","2025-04-16T11:18:04.000Z"],"Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes…","Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include **Services**, **Ingress Controllers**, and **Service Meshes**. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and stability.\n\nThe core component of Kubernetes traffic management is the **Kubernetes Service**. It creates a stable endpoint to direct network traffic to pods. In more complex environments, **Ingress** and **Service Meshes** like Istio add layers of control. Ingress manages external access, while service meshes offer more sophisticated routing features, like retries, failover, and load balancing. It is important to note that Kubernetes Ingress gateways can use Kubernetes’ Ingress resource and Ingress controllers, but this approach is no longer recommended. Instead, using the **Kubernetes Gateway API** with **Gateway** resources is now the preferred method. Effective routing strategies ensure reduced latency, improved scalability, and a better overall user experience.\n\n## Kubernetes Ingress and Egress Networking: Architecture Simplified\n\nKubernetes clusters must handle both incoming (ingress) and outgoing (egress) traffic to communicate effectively with the outside world. **Ingress** in Kubernetes is managed using **Ingress Controllers**, which map external HTTP/HTTPS requests to internal services. These controllers are essential for directing user traffic from outside the cluster, often used for load balancing and exposing services. It is now recommended to use the **Kubernetes Gateway API** and **Gateway** resources instead of traditional Ingress resources for better control and scalability. Choosing the right Ingress Controller, whether it’s NGINX, Traefik, or Envoy Gateway, can significantly impact traffic flow and scalability.\n\n**Egress** handles outbound communication from pods to external resources, like APIs or databases. Configuring **Network Policies** and Egress Gateways is key for security and performance when accessing external networks. Egress also depends on controllers deployed in Kubernetes to manage outbound traffic. These components ensure that only authorized services can communicate outside of the cluster, adding an additional layer of control to the architecture.\n\nIf you need to [implement both Ingress and Egress gateway control](/learn/ingress-and-egress-architecture), it is recommended to use **Istio**. Istio provides a comprehensive solution for managing both incoming and outgoing traffic, with enhanced features for security, observability, and traffic control.\n\nTogether, Ingress and Egress form a comprehensive architecture that secures and optimizes the way Kubernetes clusters interact with both internal and external environments. Proper planning of ingress and egress strategies will improve reliability, security, and the efficiency of your Kubernetes applications.","src/content/learn/kubernetes-traffic-routing-and-control/index.md","432c995710982592",{html:207,metadata:208},"<p>Kubernetes is the go-to choice for deploying containerized applications, but managing traffic within and between services is crucial for optimal performance. Kubernetes traffic routing encompasses how requests move between different services, ensuring that applications respond reliably and efficiently. Key mechanisms include <strong>Services</strong>, <strong>Ingress Controllers</strong>, and <strong>Service Meshes</strong>. Each provides unique ways to control and direct internal and external traffic, maintaining scalability and stability.</p>\n<p>The core component of Kubernetes traffic management is the <strong>Kubernetes Service</strong>. It creates a stable endpoint to direct network traffic to pods. In more complex environments, <strong>Ingress</strong> and <strong>Service Meshes</strong> like Istio add layers of control. Ingress manages external access, while service meshes offer more sophisticated routing features, like retries, failover, and load balancing. It is important to note that Kubernetes Ingress gateways can use Kubernetes’ Ingress resource and Ingress controllers, but this approach is no longer recommended. Instead, using the <strong>Kubernetes Gateway API</strong> with <strong>Gateway</strong> resources is now the preferred method. Effective routing strategies ensure reduced latency, improved scalability, and a better overall user experience.</p>\n<h2 id=\"kubernetes-ingress-and-egress-networking-architecture-simplified\">Kubernetes Ingress and Egress Networking: Architecture Simplified</h2>\n<p>Kubernetes clusters must handle both incoming (ingress) and outgoing (egress) traffic to communicate effectively with the outside world. <strong>Ingress</strong> in Kubernetes is managed using <strong>Ingress Controllers</strong>, which map external HTTP/HTTPS requests to internal services. These controllers are essential for directing user traffic from outside the cluster, often used for load balancing and exposing services. It is now recommended to use the <strong>Kubernetes Gateway API</strong> and <strong>Gateway</strong> resources instead of traditional Ingress resources for better control and scalability. Choosing the right Ingress Controller, whether it’s NGINX, Traefik, or Envoy Gateway, can significantly impact traffic flow and scalability.</p>\n<p><strong>Egress</strong> handles outbound communication from pods to external resources, like APIs or databases. Configuring <strong>Network Policies</strong> and Egress Gateways is key for security and performance when accessing external networks. Egress also depends on controllers deployed in Kubernetes to manage outbound traffic. These components ensure that only authorized services can communicate outside of the cluster, adding an additional layer of control to the architecture.</p>\n<p>If you need to <a href=\"/learn/ingress-and-egress-architecture\">implement both Ingress and Egress gateway control</a>, it is recommended to use <strong>Istio</strong>. Istio provides a comprehensive solution for managing both incoming and outgoing traffic, with enhanced features for security, observability, and traffic control.</p>\n<p>Together, Ingress and Egress form a comprehensive architecture that secures and optimizes the way Kubernetes clusters interact with both internal and external environments. Proper planning of ingress and egress strategies will improve reliability, security, and the efficiency of your Kubernetes applications.</p>",{headings:209,localImagePaths:213,remoteImagePaths:214,frontmatter:215,imagePaths:218},[210],{depth:29,slug:211,text:212},"kubernetes-ingress-and-egress-networking-architecture-simplified","Kubernetes Ingress and Egress Networking: Architecture Simplified",[],[],{title:198,slug:195,date:216,description:202,categories:217,excerpt:199},["Date","2025-04-16T11:18:04.000Z"],[17],[],"kubernetes-traffic-routing-and-control/index.md","ingress-and-egress-architecture",{id:220,data:222,body:228,filePath:229,digest:230,rendered:231,legacyId:253},{title:223,excerpt:224,categories:225,date:226,description:227,draft:20},"Ingress and Egress Architecture","Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and.",[82],["Date","2025-05-05T17:03:16.000Z"],"Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic…","Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and performance.\n\n## Managing Ingress Traffic in Kubernetes\n\nIngress and Egress work together to deliver the inward and outward traffic flow to and from a Kubernetes cluster. Ingress brings external requests into the cluster, while Egress ensures secure outbound communication. Together, they create a well-structured traffic flow, enhancing performance, security, and compliance.\n\nIngress in Kubernetes uses several components:\n\n- **Ingress Controllers** – Manages external access by routing traffic based on domain names, paths, and protocols. Envoy Gateway is the modern standard for Kubernetes Ingress, providing advanced security, observability, and multi-tenancy support.\n- **TLS Termination** – Ensures secure traffic by handling TLS/SSL encryption at the Ingress layer.\n- **Load Balancing & Traffic Shaping** – Distributes incoming requests efficiently, supporting QA deployments and canary releases.\n\n## Managing Egress Traffic in Kubernetes\n\nEgress in Kubernetes is built on several components:\n\n- **Egress Gateways** – Enforce outbound traffic policies, ensuring controlled communication with external APIs or cloud services.\n- **Network Policies** – Restrict Egress traffic to limit unauthorized data flows and prevent leaks.\n- **Service Mesh Integration** – Istio’s Egress gateway provides fine-grained control over external connections, enforcing authentication and mutual TLS (mTLS).\n\n## Ingress vs. Egress – Understanding the Differences\n\nThe table below compares and contrasts Kubernetes Ingress and Egress for traffic routing.\n\n**Ingress Traffic**\n\n**Egress Traffic**\n\n**Definition**\n\nTraffic coming **into** the cluster from **External**Sources\n\nTraffic **leaving** the cluster to **External**destinations\n\n**Primary Purpose**\n\nRoutes external HTTPS traffic to Kubernetes Services\n\nControls outbound communication from Pods to External clients and systems\n\n**Managed By**\n\nIngress Controllers (such as Envoy Gateway or HAProxy)\n\nEgress gateways, network policies, Kubernetes NAT configurations\n\n**Security Features**\n\nTLS termination, authentication, and access control\n\nPreventing unauthorized data exfiltration, securing external dependencies\n\n## Optimize Ingress & Egress Traffic with Tetrate\n\nMisconfigurations in Ingress and Egress traffic can lead to security risks, performance bottlenecks, and compliance issues. Tetrate [Consulting](/kubernetes-consulting) offers expert guidance on Kubernetes’ Ingress controllers, Egress gateways, and service mesh configurations. Tetrate’s Istio-based service mesh optimizes Ingress and Egress traffic, providing centralized control, monitoring, and security. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.","src/content/learn/ingress-and-egress-architecture/index.md","393795197aa68bef",{html:232,metadata:233},"<p>Managing Ingress and Egress traffic is essential for secure, efficient, and reliable communication within Kubernetes deployments. Ingress controls how external traffic enters the cluster, while Egress manages how internal services communicate with external resources. Together, they form the foundation of Kubernetes traffic flow, ensuring connectivity, security, and performance.</p>\n<h2 id=\"managing-ingress-traffic-in-kubernetes\">Managing Ingress Traffic in Kubernetes</h2>\n<p>Ingress and Egress work together to deliver the inward and outward traffic flow to and from a Kubernetes cluster. Ingress brings external requests into the cluster, while Egress ensures secure outbound communication. Together, they create a well-structured traffic flow, enhancing performance, security, and compliance.</p>\n<p>Ingress in Kubernetes uses several components:</p>\n<ul>\n<li><strong>Ingress Controllers</strong> – Manages external access by routing traffic based on domain names, paths, and protocols. Envoy Gateway is the modern standard for Kubernetes Ingress, providing advanced security, observability, and multi-tenancy support.</li>\n<li><strong>TLS Termination</strong> – Ensures secure traffic by handling TLS/SSL encryption at the Ingress layer.</li>\n<li><strong>Load Balancing &#x26; Traffic Shaping</strong> – Distributes incoming requests efficiently, supporting QA deployments and canary releases.</li>\n</ul>\n<h2 id=\"managing-egress-traffic-in-kubernetes\">Managing Egress Traffic in Kubernetes</h2>\n<p>Egress in Kubernetes is built on several components:</p>\n<ul>\n<li><strong>Egress Gateways</strong> – Enforce outbound traffic policies, ensuring controlled communication with external APIs or cloud services.</li>\n<li><strong>Network Policies</strong> – Restrict Egress traffic to limit unauthorized data flows and prevent leaks.</li>\n<li><strong>Service Mesh Integration</strong> – Istio’s Egress gateway provides fine-grained control over external connections, enforcing authentication and mutual TLS (mTLS).</li>\n</ul>\n<h2 id=\"ingress-vs-egress--understanding-the-differences\">Ingress vs. Egress – Understanding the Differences</h2>\n<p>The table below compares and contrasts Kubernetes Ingress and Egress for traffic routing.</p>\n<p><strong>Ingress Traffic</strong></p>\n<p><strong>Egress Traffic</strong></p>\n<p><strong>Definition</strong></p>\n<p>Traffic coming <strong>into</strong> the cluster from <strong>External</strong>Sources</p>\n<p>Traffic <strong>leaving</strong> the cluster to <strong>External</strong>destinations</p>\n<p><strong>Primary Purpose</strong></p>\n<p>Routes external HTTPS traffic to Kubernetes Services</p>\n<p>Controls outbound communication from Pods to External clients and systems</p>\n<p><strong>Managed By</strong></p>\n<p>Ingress Controllers (such as Envoy Gateway or HAProxy)</p>\n<p>Egress gateways, network policies, Kubernetes NAT configurations</p>\n<p><strong>Security Features</strong></p>\n<p>TLS termination, authentication, and access control</p>\n<p>Preventing unauthorized data exfiltration, securing external dependencies</p>\n<h2 id=\"optimize-ingress--egress-traffic-with-tetrate\">Optimize Ingress &#x26; Egress Traffic with Tetrate</h2>\n<p>Misconfigurations in Ingress and Egress traffic can lead to security risks, performance bottlenecks, and compliance issues. Tetrate <a href=\"/kubernetes-consulting\">Consulting</a> offers expert guidance on Kubernetes’ Ingress controllers, Egress gateways, and service mesh configurations. Tetrate’s Istio-based service mesh optimizes Ingress and Egress traffic, providing centralized control, monitoring, and security. Our experts can help you optimize your Kubernetes Ingress, service mesh, and traffic management strategies. Get expert guidance on all things Kubernetes-related by connecting with Tetrate’s consulting services.</p>",{headings:234,localImagePaths:247,remoteImagePaths:248,frontmatter:249,imagePaths:252},[235,238,241,244],{depth:29,slug:236,text:237},"managing-ingress-traffic-in-kubernetes","Managing Ingress Traffic in Kubernetes",{depth:29,slug:239,text:240},"managing-egress-traffic-in-kubernetes","Managing Egress Traffic in Kubernetes",{depth:29,slug:242,text:243},"ingress-vs-egress--understanding-the-differences","Ingress vs. Egress – Understanding the Differences",{depth:29,slug:245,text:246},"optimize-ingress--egress-traffic-with-tetrate","Optimize Ingress & Egress Traffic with Tetrate",[],[],{title:223,slug:220,date:250,description:227,categories:251,excerpt:224},["Date","2025-05-05T17:03:16.000Z"],[82],[],"ingress-and-egress-architecture/index.md","use-case-kubernetes-architecture",{id:254,data:256,body:262,filePath:263,digest:264,rendered:265,legacyId:299},{title:257,excerpt:258,categories:259,date:260,description:261,draft:20},"Use Case and Architecture for Tetrate with Kubernetes and Istio","How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like AWS(/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide/) and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and.",[17],["Date","2025-05-05T19:18:41.000Z"],"How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like AWS…","How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like [AWS](/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide) and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and security:\n\n## Use Cases\n\n- **Large-Scale Microservices Management:** eBay’s use case demonstrates how Istio can be used to manage traffic in a complex, multi-datacenter environment with multiple Kubernetes clusters. This showcases Istio’s ability to handle large-scale microservices architectures.\n- **Zero Trust Security Implementation:** Tetrate provides a consistent way to implement Zero Trust security across thousands of microservices in various environments. This is particularly crucial for organizations dealing with sensitive data or those under regulatory compliance like PCI, HIPAA, GDPR, and FIPS/FedRAMP.\n- **Multi-Cloud and Hybrid Cloud Deployments:** Tetrate Service Bridge (TSB) enables management of services across multiple clusters, clouds, and hybrid environments. This is valuable for organizations operating in complex, distributed infrastructures.\n- **Traffic Management and Routing:** Tetrate’s service mesh provides advanced traffic management capabilities, including traffic routing, splitting, and canary deployments. This is useful for organizations looking to implement sophisticated deployment strategies.\n- **Observability and Troubleshooting:** The service mesh offers improved visibility into service health and performance, making it easier to troubleshoot application issues and investigate security incidents.\n- **Kubernetes and VM Integration:** Tetrate has extended Istio’s capabilities to integrate with legacy platforms, allowing organizations to manage both modern Kubernetes microservices and traditional VM applications under a single service mesh.\n- **Compliance and Policy Enforcement:** Tetrate automates the deployment of security policies and enables continuous compliance monitoring, which is particularly valuable for highly regulated industries.\n- **Military and Defense Applications:** The U.S. Air Force has contracted Tetrate for implementing Istio-based resilient communications and enhancing DevSecOps security. This demonstrates the platform’s applicability in high-security, mission-critical environments.\n- **Ambient Mode for Resource Optimization:** Tetrate is implementing ambient mode for the U.S. Air Force, allowing for more efficient resource utilization based on security risk profiles.\n- **Cross-Platform Compatibility:** Tetrate Service Bridge supports multiple cloud platforms like Amazon EKS, Azure AKS, and Red Hat OpenShift, making it versatile for organizations with diverse infrastructure.\n\n## Architecture\n\nTetrate Service Bridge (TSB) provides a comprehensive [service mesh](/blog/service-mesh-architecture) solution that seamlessly integrates with Kubernetes, offering:\n\n- **Unified Control Plane:** TSB extends Istio’s capabilities, providing a single pane of glass for managing multiple clusters across different cloud providers and on-premises environments.\n- **Enhanced Proxy Model in sidecar or ambient mode:** We optimize Envoy’s sidecar proxy implementation, ensuring efficient and secure service-to-service communication.\n- **Multi-Cloud Support:** TSB enables consistent policies and management across AWS EKS, Azure AKS, and other Kubernetes platforms, simplifying multi-cloud deployments.\n\n## Observability\n\nTetrate’s solutions significantly enhance observability in microservices environments:\n\n- **Advanced Metrics Collection:** TSBprovides native dashboards for in-depth performance analysis and integrates with popular tools like Prometheus.\n- **Distributed Tracing:** We offer seamless tracing, providing end-to-end visibility of request flows.\n- **Centralized Logging:** TSB aggregates and analyzes logs from across your service mesh, enabling quick troubleshooting and pattern recognition.\n- **Visualization:** TSB offers native network visualization and real-time metrics insights.\n\n## Security\n\nTetrate puts a strong emphasis on zero trust security:\n\n- **Zero Trust Implementation:** TSB provides out-of-the-box zero trust security, aligning with CISA and NIST recommendations.\n- **Automated mTLS:** We ensure all service-to-service communication is encrypted and authenticated, with automatic certificate management.\n- **Fine-grained Access Control:** TSB enables detailed RBAC and ABAC policies, allowing precise control over service interactions.\n- **Identity Management:** We support integration with external identity providers and implement strong service identity verification.\n- **Policy Enforcement:** TSB integrates with Open Policy Agent (OPA) for advanced authorization policies.\n\n## Advanced Load Balancing\n\nTetrate’s approach to load balancing goes beyond basic Kubernetes capabilities:\n\n- **Global Load Balancing:** Tier-1 gateways enable global load balancing across multiple clusters, allowing for traffic distribution based on weighted percentages.\n- **Cross-Cluster Load Balancing:** TSB facilitates load balancing across different Kubernetes clusters, supporting multi-cluster and multi-cloud deployments.\n- **L7 Load Balancing:** TSB implements Layer 7 load balancing across one or more ingress gateways in different clusters over Istio-controlled mTLS\n\n## Cloud Provider Integration\n\nTetrate’s solutions are designed to work seamlessly with major cloud providers:\n\n- **AWS Integration:** TSB integrates with AWS services like ACM for certificate management and supports deployment on EKS.\n- **Azure Support:** We provide full support for Azure AKS, enabling seamless service mesh implementation in Azure environments.\n- **Multi-Cloud Management:** TSB offers a consistent management layer across different cloud providers, simplifying hybrid and multi-cloud deployments.\n\n## Enterprise-Ready Features\n\nTetrate Service Bridge goes beyond open-source Istio to provide enterprise-grade features:\n\n- **Simplified Configuration:** TSB offers an intuitive interface for managing complex service mesh configurations across multiple clusters.\n- **Scalability:** Our solution is built to handle enterprise-scale deployments, managing thousands of services efficiently.\n- **Compliance and Governance:** TSB helps enforce organizational policies and compliance requirements across the entire service mesh.\n- **Expert Support:** We provide enterprise-grade support and professional services to ensure successful implementation and operation of your service mesh.\n\n## Enterprise consulting and support services\n\nTetrate’s [consulting](/kubernetes-consulting) and enterprise support services provide comprehensive assistance for organizations implementing service mesh solutions across AWS, Azure, and hybrid environments.\n\nOur expert consultants offer tailored guidance on architecting, deploying, and optimizing Istio and Envoy-based service meshes, ensuring seamless integration with cloud-native technologies and existing infrastructure. We specialize in addressing the unique challenges of multi-cloud deployments, helping clients leverage the strengths of both AWS and Azure while maintaining consistent security and observability policies. Our enterprise support goes beyond traditional offerings, providing 24/7 access to service mesh experts, proactive monitoring, and rapid incident response.\n\nWhether it’s optimizing performance on AWS EKS, enhancing security on Azure AKS, or implementing cross-cloud traffic management, Tetrate’s consulting and support services enable organizations to maximize the value of their service mesh investments while minimizing operational complexity and risk.\n\nBy choosing Tetrate, organizations can confidently implement a secure, observable, and scalable microservices architecture. Our solutions simplify the complexities of service mesh technology, allowing teams to focus on delivering business value while ensuring robust security and operational excellence across their Kubernetes environments.","src/content/learn/use-case-kubernetes-architecture/index.md","5d885b22895d75d2",{html:266,metadata:267},"<p>How our solutions help organizations implement a robust zero trust security model using Istio and Envoy in Kubernetes environments, particularly on platforms like <a href=\"/blog/migrating-from-aws-app-mesh-to-istio-a-comprehensive-guide\">AWS</a> and Azure. Here’s how Tetrate’s offerings address key aspects of microservices architecture, observability, and security:</p>\n<h2 id=\"use-cases\">Use Cases</h2>\n<ul>\n<li><strong>Large-Scale Microservices Management:</strong> eBay’s use case demonstrates how Istio can be used to manage traffic in a complex, multi-datacenter environment with multiple Kubernetes clusters. This showcases Istio’s ability to handle large-scale microservices architectures.</li>\n<li><strong>Zero Trust Security Implementation:</strong> Tetrate provides a consistent way to implement Zero Trust security across thousands of microservices in various environments. This is particularly crucial for organizations dealing with sensitive data or those under regulatory compliance like PCI, HIPAA, GDPR, and FIPS/FedRAMP.</li>\n<li><strong>Multi-Cloud and Hybrid Cloud Deployments:</strong> Tetrate Service Bridge (TSB) enables management of services across multiple clusters, clouds, and hybrid environments. This is valuable for organizations operating in complex, distributed infrastructures.</li>\n<li><strong>Traffic Management and Routing:</strong> Tetrate’s service mesh provides advanced traffic management capabilities, including traffic routing, splitting, and canary deployments. This is useful for organizations looking to implement sophisticated deployment strategies.</li>\n<li><strong>Observability and Troubleshooting:</strong> The service mesh offers improved visibility into service health and performance, making it easier to troubleshoot application issues and investigate security incidents.</li>\n<li><strong>Kubernetes and VM Integration:</strong> Tetrate has extended Istio’s capabilities to integrate with legacy platforms, allowing organizations to manage both modern Kubernetes microservices and traditional VM applications under a single service mesh.</li>\n<li><strong>Compliance and Policy Enforcement:</strong> Tetrate automates the deployment of security policies and enables continuous compliance monitoring, which is particularly valuable for highly regulated industries.</li>\n<li><strong>Military and Defense Applications:</strong> The U.S. Air Force has contracted Tetrate for implementing Istio-based resilient communications and enhancing DevSecOps security. This demonstrates the platform’s applicability in high-security, mission-critical environments.</li>\n<li><strong>Ambient Mode for Resource Optimization:</strong> Tetrate is implementing ambient mode for the U.S. Air Force, allowing for more efficient resource utilization based on security risk profiles.</li>\n<li><strong>Cross-Platform Compatibility:</strong> Tetrate Service Bridge supports multiple cloud platforms like Amazon EKS, Azure AKS, and Red Hat OpenShift, making it versatile for organizations with diverse infrastructure.</li>\n</ul>\n<h2 id=\"architecture\">Architecture</h2>\n<p>Tetrate Service Bridge (TSB) provides a comprehensive <a href=\"/blog/service-mesh-architecture\">service mesh</a> solution that seamlessly integrates with Kubernetes, offering:</p>\n<ul>\n<li><strong>Unified Control Plane:</strong> TSB extends Istio’s capabilities, providing a single pane of glass for managing multiple clusters across different cloud providers and on-premises environments.</li>\n<li><strong>Enhanced Proxy Model in sidecar or ambient mode:</strong> We optimize Envoy’s sidecar proxy implementation, ensuring efficient and secure service-to-service communication.</li>\n<li><strong>Multi-Cloud Support:</strong> TSB enables consistent policies and management across AWS EKS, Azure AKS, and other Kubernetes platforms, simplifying multi-cloud deployments.</li>\n</ul>\n<h2 id=\"observability\">Observability</h2>\n<p>Tetrate’s solutions significantly enhance observability in microservices environments:</p>\n<ul>\n<li><strong>Advanced Metrics Collection:</strong> TSBprovides native dashboards for in-depth performance analysis and integrates with popular tools like Prometheus.</li>\n<li><strong>Distributed Tracing:</strong> We offer seamless tracing, providing end-to-end visibility of request flows.</li>\n<li><strong>Centralized Logging:</strong> TSB aggregates and analyzes logs from across your service mesh, enabling quick troubleshooting and pattern recognition.</li>\n<li><strong>Visualization:</strong> TSB offers native network visualization and real-time metrics insights.</li>\n</ul>\n<h2 id=\"security\">Security</h2>\n<p>Tetrate puts a strong emphasis on zero trust security:</p>\n<ul>\n<li><strong>Zero Trust Implementation:</strong> TSB provides out-of-the-box zero trust security, aligning with CISA and NIST recommendations.</li>\n<li><strong>Automated mTLS:</strong> We ensure all service-to-service communication is encrypted and authenticated, with automatic certificate management.</li>\n<li><strong>Fine-grained Access Control:</strong> TSB enables detailed RBAC and ABAC policies, allowing precise control over service interactions.</li>\n<li><strong>Identity Management:</strong> We support integration with external identity providers and implement strong service identity verification.</li>\n<li><strong>Policy Enforcement:</strong> TSB integrates with Open Policy Agent (OPA) for advanced authorization policies.</li>\n</ul>\n<h2 id=\"advanced-load-balancing\">Advanced Load Balancing</h2>\n<p>Tetrate’s approach to load balancing goes beyond basic Kubernetes capabilities:</p>\n<ul>\n<li><strong>Global Load Balancing:</strong> Tier-1 gateways enable global load balancing across multiple clusters, allowing for traffic distribution based on weighted percentages.</li>\n<li><strong>Cross-Cluster Load Balancing:</strong> TSB facilitates load balancing across different Kubernetes clusters, supporting multi-cluster and multi-cloud deployments.</li>\n<li><strong>L7 Load Balancing:</strong> TSB implements Layer 7 load balancing across one or more ingress gateways in different clusters over Istio-controlled mTLS</li>\n</ul>\n<h2 id=\"cloud-provider-integration\">Cloud Provider Integration</h2>\n<p>Tetrate’s solutions are designed to work seamlessly with major cloud providers:</p>\n<ul>\n<li><strong>AWS Integration:</strong> TSB integrates with AWS services like ACM for certificate management and supports deployment on EKS.</li>\n<li><strong>Azure Support:</strong> We provide full support for Azure AKS, enabling seamless service mesh implementation in Azure environments.</li>\n<li><strong>Multi-Cloud Management:</strong> TSB offers a consistent management layer across different cloud providers, simplifying hybrid and multi-cloud deployments.</li>\n</ul>\n<h2 id=\"enterprise-ready-features\">Enterprise-Ready Features</h2>\n<p>Tetrate Service Bridge goes beyond open-source Istio to provide enterprise-grade features:</p>\n<ul>\n<li><strong>Simplified Configuration:</strong> TSB offers an intuitive interface for managing complex service mesh configurations across multiple clusters.</li>\n<li><strong>Scalability:</strong> Our solution is built to handle enterprise-scale deployments, managing thousands of services efficiently.</li>\n<li><strong>Compliance and Governance:</strong> TSB helps enforce organizational policies and compliance requirements across the entire service mesh.</li>\n<li><strong>Expert Support:</strong> We provide enterprise-grade support and professional services to ensure successful implementation and operation of your service mesh.</li>\n</ul>\n<h2 id=\"enterprise-consulting-and-support-services\">Enterprise consulting and support services</h2>\n<p>Tetrate’s <a href=\"/kubernetes-consulting\">consulting</a> and enterprise support services provide comprehensive assistance for organizations implementing service mesh solutions across AWS, Azure, and hybrid environments.</p>\n<p>Our expert consultants offer tailored guidance on architecting, deploying, and optimizing Istio and Envoy-based service meshes, ensuring seamless integration with cloud-native technologies and existing infrastructure. We specialize in addressing the unique challenges of multi-cloud deployments, helping clients leverage the strengths of both AWS and Azure while maintaining consistent security and observability policies. Our enterprise support goes beyond traditional offerings, providing 24/7 access to service mesh experts, proactive monitoring, and rapid incident response.</p>\n<p>Whether it’s optimizing performance on AWS EKS, enhancing security on Azure AKS, or implementing cross-cloud traffic management, Tetrate’s consulting and support services enable organizations to maximize the value of their service mesh investments while minimizing operational complexity and risk.</p>\n<p>By choosing Tetrate, organizations can confidently implement a secure, observable, and scalable microservices architecture. Our solutions simplify the complexities of service mesh technology, allowing teams to focus on delivering business value while ensuring robust security and operational excellence across their Kubernetes environments.</p>",{headings:268,localImagePaths:293,remoteImagePaths:294,frontmatter:295,imagePaths:298},[269,272,275,278,281,284,287,290],{depth:29,slug:270,text:271},"use-cases","Use Cases",{depth:29,slug:273,text:274},"architecture","Architecture",{depth:29,slug:276,text:277},"observability","Observability",{depth:29,slug:279,text:280},"security","Security",{depth:29,slug:282,text:283},"advanced-load-balancing","Advanced Load Balancing",{depth:29,slug:285,text:286},"cloud-provider-integration","Cloud Provider Integration",{depth:29,slug:288,text:289},"enterprise-ready-features","Enterprise-Ready Features",{depth:29,slug:291,text:292},"enterprise-consulting-and-support-services","Enterprise consulting and support services",[],[],{title:257,slug:254,date:296,description:261,categories:297,excerpt:258},["Date","2025-05-05T19:18:41.000Z"],[17],[],"use-case-kubernetes-architecture/index.md","kubernetes-security-architecture",{id:300,data:302,body:308,filePath:309,digest:310,rendered:311,legacyId:330},{title:303,excerpt:304,categories:305,date:306,description:307,draft:20},"Kubernetes Security Architecture","Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security.",[17],["Date","2025-01-21T03:37:22.000Z"],"Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the…","Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security techniques:\n\n## Secure Storage\n\nThe security of the Etcd storage backend, where all cluster data is stored, requires top-tier hardware and software protection.\n\n## Use a Service Mesh\n\nA service mesh adds a layer of security across the Kubernetes platform deployment by adding features like traffic encryption and monitoring to enhance the underlying network security.\n\n## Use Namespaces\n\nDividing Kubernetes clusters into logical segments using namespaces improves resource isolation and limits the impact of any breaches to a subset of the deployed resources. This is part of the broader segmentation recommended for Kubernetes deployments.","src/content/learn/kubernetes-security-architecture/index.md","c643ed2febe02e69",{html:312,metadata:313},"<p>Kubernetes has a security architecture based on the principle of defense in depth. This means that it incorporates multiple layers of security controls throughout the platform to deliver robust security. The security controls needed include the items mentioned in the previous section. At the architectural level, the flooring items also need to be protected with suitable security techniques:</p>\n<h2 id=\"secure-storage\">Secure Storage</h2>\n<p>The security of the Etcd storage backend, where all cluster data is stored, requires top-tier hardware and software protection.</p>\n<h2 id=\"use-a-service-mesh\">Use a Service Mesh</h2>\n<p>A service mesh adds a layer of security across the Kubernetes platform deployment by adding features like traffic encryption and monitoring to enhance the underlying network security.</p>\n<h2 id=\"use-namespaces\">Use Namespaces</h2>\n<p>Dividing Kubernetes clusters into logical segments using namespaces improves resource isolation and limits the impact of any breaches to a subset of the deployed resources. This is part of the broader segmentation recommended for Kubernetes deployments.</p>",{headings:314,localImagePaths:324,remoteImagePaths:325,frontmatter:326,imagePaths:329},[315,318,321],{depth:29,slug:316,text:317},"secure-storage","Secure Storage",{depth:29,slug:319,text:320},"use-a-service-mesh","Use a Service Mesh",{depth:29,slug:322,text:323},"use-namespaces","Use Namespaces",[],[],{title:303,slug:300,date:327,description:307,categories:328,excerpt:304},["Date","2025-01-21T03:37:22.000Z"],[17],[],"kubernetes-security-architecture/index.md","what-does-kubernetes-do",{id:331,data:333,body:340,filePath:341,digest:342,rendered:343,legacyId:368},{title:334,excerpt:335,categories:336,date:338,description:339,draft:20},"What Does Kubernetes Do?","Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized.",[337],"What",["Date","2025-05-05T18:06:35.000Z"],"Containerization Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized…","## Containerization\n\nKubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized applications.\n\nContainerization is a popular method for packaging applications for deployment. Software containers are self-contained and include everything an application needs to run in a single package called a container.\n\nDue to the benefits, containers have become a popular deployment option for development teams. Docker is one of the most widely used container platforms and runtime engines, but others exist, such as CRI-O, Podman, and more.\n\nThe ease of creating containers and their growing popularity has led to a management challenge for many organizations. With a large number of containers deployed in DevSecOps and Production environments, container sprawl has become a problem, similar to virtual server sprawl in virtualization platforms. The open-source [Kubernetes](/learn/what-does-kubernetes-do) platform has become a popular solution for managing containers and controlling sprawl.\n\n## Container Orchestration\n\nContainer orchestration is an essential process for IT Operations Teams when managing large numbers of containers to ensure that deployed applications are available and performing well 24×7. This is especially true for microservices-based applications that use many separately deployed modules working in unison across containers. The Kubernetes platform provides the tools to manage container deployments.\n\nKubernetes has gained popularity with developers and DevSecOps teams due to its features, toolset, and support from major cloud service providers. Kubernetes has transformed application deployment by delivering enhanced container management for organizations.\n\n## Kubernetes Components\n\nKubernetes groups software containers into pods, the smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, networking, and a configuration specification outlining how the containers should run.\n\nKubernetes has multiple components to deliver container orchestration:\n\n**Clusters** – The Kubernetes architecture uses clusters consisting of multiple worker nodes (see below) for running and managing containers and a Control Plane node for controlling and monitoring the worker nodes.\n\n**Nodes** – A compute host that can be a physical, virtual, or cloud instance. These nodes exist within a cluster, where they can act as workers or a Control Plane. The worker nodes are responsible for hosting and running the deployed containers. In contrast, the Control Plane node in each cluster manages the worker nodes in the same cluster. Each worker node runs an agent, which the master node uses to monitor and manage it.\n\n**Pods** – Groups of containers that share compute resources and a network. Kubernetes scales at the pod level. If additional capacity is needed to support an application running on containers in a pod, then Kubernetes can replicate the complete pod to add capacity via additional containers.\n\n**Deployments** – Kubernetes Deployments control the creation and deployment of containerized applications. They use declarative configuration files that specify the desired state of an application and its containers. The deployment file also specifies how many replicas of a pod should run on a cluster. Deployments monitor running container pods, and if a pod fails, they recreate it.\n\n## Kubernetes Services\n\nWithin these components, Kubernetes provides the following services and functionality:\n\n**Control Plane-Node Architecture** – Kubernetes uses a Control Plane-Node architecture. A Control Plane node is responsible for managing the state of the cluster. It schedules pods to run on nodes, maintains the desired state of the application, scales applications, and rolls out new updates.\n\n**Rollouts** – These describe the target container landscape needed for an application and let Kubernetes handle the process deployment to get there. This includes new deployments, changing existing deployed containers, and rollbacks to remove obsolete deployments. The Kubernetes rollout scheduler automatically places containers in the available infrastructure based on their resource requirements.\n\n**Self-healing** – This monitors containers for issues and restarts them automatically if necessary. The restart can be within the same pod or on another pod.\n\n**Service discovery** – Automatically expose a container to the broader network or other containers via a DNS name or an IP Address.\n\n**Load Balancing** – Manage the load across multiple containers delivering the same application to ensure consistent performance.\n\n**Storage orchestration** – Mount storage from the cloud or local resources as needed and for as long as necessary. Applications can request the storage they need, and the Kubernetes platform will allocate it dynamically from local or cloud storage resources without system administrators needing to be involved.\n\n**Secret and configuration management** – Securely manage sensitive information like passwords, tokens, and keys. Deploy and update secrets and app configurations without exposing secrets on the network.\n\n## Conclusion\n\nIn summary, Kubernetes provides the tools to automate the deployment, scaling, and management of application container clusters and pods across hosts. By abstracting the hardware infrastructure with a controlling software platform, Kubernetes presents the available IT infrastructure as a single deployment platform.\n\nKubernetes promotes a more collaborative approach between developers and operations teams, enabling them to work together more effectively and encouraging a more DevSecOps-centric approach to application deployment.","src/content/learn/what-does-kubernetes-do/index.md","5697f2801d65ecb2",{html:344,metadata:345},"<h2 id=\"containerization\">Containerization</h2>\n<p>Kubernetes has become essential to modern application deployments by providing flexibility, scalability, and efficiency when managing containerized applications.</p>\n<p>Containerization is a popular method for packaging applications for deployment. Software containers are self-contained and include everything an application needs to run in a single package called a container.</p>\n<p>Due to the benefits, containers have become a popular deployment option for development teams. Docker is one of the most widely used container platforms and runtime engines, but others exist, such as CRI-O, Podman, and more.</p>\n<p>The ease of creating containers and their growing popularity has led to a management challenge for many organizations. With a large number of containers deployed in DevSecOps and Production environments, container sprawl has become a problem, similar to virtual server sprawl in virtualization platforms. The open-source <a href=\"/learn/what-does-kubernetes-do\">Kubernetes</a> platform has become a popular solution for managing containers and controlling sprawl.</p>\n<h2 id=\"container-orchestration\">Container Orchestration</h2>\n<p>Container orchestration is an essential process for IT Operations Teams when managing large numbers of containers to ensure that deployed applications are available and performing well 24×7. This is especially true for microservices-based applications that use many separately deployed modules working in unison across containers. The Kubernetes platform provides the tools to manage container deployments.</p>\n<p>Kubernetes has gained popularity with developers and DevSecOps teams due to its features, toolset, and support from major cloud service providers. Kubernetes has transformed application deployment by delivering enhanced container management for organizations.</p>\n<h2 id=\"kubernetes-components\">Kubernetes Components</h2>\n<p>Kubernetes groups software containers into pods, the smallest deployable units in Kubernetes. A pod can contain one or more containers that share storage, networking, and a configuration specification outlining how the containers should run.</p>\n<p>Kubernetes has multiple components to deliver container orchestration:</p>\n<p><strong>Clusters</strong> – The Kubernetes architecture uses clusters consisting of multiple worker nodes (see below) for running and managing containers and a Control Plane node for controlling and monitoring the worker nodes.</p>\n<p><strong>Nodes</strong> – A compute host that can be a physical, virtual, or cloud instance. These nodes exist within a cluster, where they can act as workers or a Control Plane. The worker nodes are responsible for hosting and running the deployed containers. In contrast, the Control Plane node in each cluster manages the worker nodes in the same cluster. Each worker node runs an agent, which the master node uses to monitor and manage it.</p>\n<p><strong>Pods</strong> – Groups of containers that share compute resources and a network. Kubernetes scales at the pod level. If additional capacity is needed to support an application running on containers in a pod, then Kubernetes can replicate the complete pod to add capacity via additional containers.</p>\n<p><strong>Deployments</strong> – Kubernetes Deployments control the creation and deployment of containerized applications. They use declarative configuration files that specify the desired state of an application and its containers. The deployment file also specifies how many replicas of a pod should run on a cluster. Deployments monitor running container pods, and if a pod fails, they recreate it.</p>\n<h2 id=\"kubernetes-services\">Kubernetes Services</h2>\n<p>Within these components, Kubernetes provides the following services and functionality:</p>\n<p><strong>Control Plane-Node Architecture</strong> – Kubernetes uses a Control Plane-Node architecture. A Control Plane node is responsible for managing the state of the cluster. It schedules pods to run on nodes, maintains the desired state of the application, scales applications, and rolls out new updates.</p>\n<p><strong>Rollouts</strong> – These describe the target container landscape needed for an application and let Kubernetes handle the process deployment to get there. This includes new deployments, changing existing deployed containers, and rollbacks to remove obsolete deployments. The Kubernetes rollout scheduler automatically places containers in the available infrastructure based on their resource requirements.</p>\n<p><strong>Self-healing</strong> – This monitors containers for issues and restarts them automatically if necessary. The restart can be within the same pod or on another pod.</p>\n<p><strong>Service discovery</strong> – Automatically expose a container to the broader network or other containers via a DNS name or an IP Address.</p>\n<p><strong>Load Balancing</strong> – Manage the load across multiple containers delivering the same application to ensure consistent performance.</p>\n<p><strong>Storage orchestration</strong> – Mount storage from the cloud or local resources as needed and for as long as necessary. Applications can request the storage they need, and the Kubernetes platform will allocate it dynamically from local or cloud storage resources without system administrators needing to be involved.</p>\n<p><strong>Secret and configuration management</strong> – Securely manage sensitive information like passwords, tokens, and keys. Deploy and update secrets and app configurations without exposing secrets on the network.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, Kubernetes provides the tools to automate the deployment, scaling, and management of application container clusters and pods across hosts. By abstracting the hardware infrastructure with a controlling software platform, Kubernetes presents the available IT infrastructure as a single deployment platform.</p>\n<p>Kubernetes promotes a more collaborative approach between developers and operations teams, enabling them to work together more effectively and encouraging a more DevSecOps-centric approach to application deployment.</p>",{headings:346,localImagePaths:362,remoteImagePaths:363,frontmatter:364,imagePaths:367},[347,350,353,356,359],{depth:29,slug:348,text:349},"containerization","Containerization",{depth:29,slug:351,text:352},"container-orchestration","Container Orchestration",{depth:29,slug:354,text:355},"kubernetes-components","Kubernetes Components",{depth:29,slug:357,text:358},"kubernetes-services","Kubernetes Services",{depth:29,slug:360,text:361},"conclusion","Conclusion",[],[],{title:334,slug:331,date:365,description:339,categories:366,excerpt:335},["Date","2025-05-05T18:06:35.000Z"],[337],[],"what-does-kubernetes-do/index.md","what-is-envoy-gateway",{id:369,data:371,body:377,filePath:378,digest:379,rendered:380,legacyId:406},{title:372,excerpt:373,categories:374,date:375,description:376,draft:20},"What Is Envoy Gateway?","> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, Tetrate Enterprise Gateway for Envoy (TEG)(/tetrate-enterprise-gateway-for-envoy/). TEG is the easiest way to get started with Envoy for production use cases. Get access now.",[337],["Date","2024-10-08T07:50:50.000Z"],"Overview Envoy Gateway is an open source project that aims to make it simple to use Envoy Proxy as an API gateway by delivering a simplified deployment model and API…","## Overview\n\n[Envoy Gateway](/external-link/)\n\n> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy). TEG is the easiest way to get started with Envoy for production use cases. [Get access now ›](/demo-request)\n\n\n\nEnvoy Proxy is a high-performance open-source proxy server designed for cloud-native applications. It was initially created by Lyft in 2016, where it was used as an API gateway and edge proxy, offering observability insights [that aided Lyft’s transition](/external-link/)\n\nWhile Envoy Proxy is typically used in conjunction with other technologies like [the Istio service mesh](/what-is-istio-service-mesh), Envoy Gateway is a great way to get started with Envoy on its own as a cloud-native application gateway—especially for those coming from traditional API gateway technologies looking to adopt a more modern approach.\n\nEnvoy Gateway provides a suite of services and features including:\n\n- An [xDS](/external-link/)\n- An expressive API based on the Kubernetes Gateway API, with reasonable settings to simplify the Envoy user experience.\n- Support for heterogeneous environments (although, early versions focus on deployment in Kubernetes).\n- Extensibility to support a multitude of application gateway use cases.\n- Envoy infrastructure provisioning and management.\n- High-quality documentation, tooling, and a diverse group of project maintainers for support.\n\nEnvoy Gateway makes it easy for organizations to leverage the power of Envoy for “north-south” traffic. With its support for multiple user personas, organizations can leave their existing operational models unchanged. For example, **infrastructure admins** can use Envoy Gateway to provision and manage fleets of Envoys while **application developers** can simply route application traffic to their backend services. In addition, Envoy Gateway implements the Kubernetes Gateway API which aims to standardize and improve all the Kubernetes APIs that are currently used for ingress.\n\n## Benefits of Envoy Gateway\n\nUsing Envoy Gateway provides several benefits for microservices and cloud-native applications, including:\n\n**High performance.** Envoy Gateway is built on top of the high-performance Envoy proxy, which can handle millions of requests per second. This makes it an ideal choice for high-traffic APIs.\n\n**Extensibility.** Envoy has a flexible architecture and supports custom filters, allowing developers to extend its functionality according to their specific needs.\n\n**Dynamic configuration.** Envoy supports dynamic service discovery and configuration updates, which allows it to adapt to changes in the system without the need for manual intervention or downtime.\n\n**Security.** Envoy Gateway supports a variety of security features, such as Transport Layer Security (TLS), TLS pass-through, and secure gRPC. It also provides features for authentication and rate-limiting to protect backend services from unauthorized access and potential attacks.\n\n**Protocol support.** Envoy Gateway supports a wide range of protocols, including HTTP/1.x, HTTP/2, gRPC, and TCP. This makes it suitable for various types of applications and services.\n\n**Seamless integration.** Envoy Gateway can be easily integrated into existing infrastructure, including Kubernetes, service meshes, and other orchestration platforms. This seamless integration ensures a smoother transition and reduced operational overhead when adopting Envoy.\n\nEnvoy Gateway makes it easy to get started using a modern application gateway and a great stepping stone towards adopting cloud-native architectures like service mesh.\n\n## Envoy Gateway Project Goals\n\nThe core of Envoy Proxy is inherently low-level, designed to be fast, flexible, capable, and extensible, but not necessarily easy to use by itself. Envoy Gateway was born to [“bring Envoy to the masses”](/external-link/)\n\n**A streamlined API tailored for gateway use cases.** Envoy Gateway uses the Kubernetes Gateway API with Envoy-specific extensions. This decision is based on the project’s initial focus on deployment within Kubernetes as an ingress controller plus the broad buy-in of the Gateway API.\n\n**A comprehensive, ready-to-use experience.** This approach enables users to quickly set up and start using Envoy Gateway. It encompasses lifecycle management functionalities that handle controller resources, control plane resources, proxy instances, and more.\n\n**A flexible and expandable API surface.** While the project aims to provide out-of-the-box functionality for common API gateway features (e.g., rate limiting, authentication, Let’s Encrypt integration), vendors can offer SaaS versions of all APIs, introduce additional APIs, and incorporate value-added features such as Web Application Firewall (WAF), enhanced observability, chaos engineering, etc.\n\n**High-quality documentation and beginner-friendly guides.** The primary goal of Envoy Gateway is to simplify the process of setting up and using the most prevalent gateway use cases for the average user. This is achieved by providing comprehensive documentation and easy-to-follow getting started guides.\n\n## Envoy Gateway vs Traditional API Gateway\n\nThe main difference between Envoy Gateway and a traditional API Gateway is the architecture and the way they handle API traffic.\n\nTraditional API Gateways are often monolithic systems that handle all API traffic in a centralized manner. They are responsible for managing API routing, authentication, authorization, rate limiting, and other functions. Traditional API Gateways can become a bottleneck in high-traffic scenarios, and they can be difficult to scale.\n\nOn the other hand, Envoy Gateway is designed to be a lightweight, modular component that can be easily deployed in front of microservices. It leverages the power of Envoy to provide API Gateway capabilities such as routing, authentication, and rate limiting, but it can also be extended with custom filters to build additional functionality.\n\nEnvoy Gateway is a more flexible and scalable alternative to traditional API Gateways, designed to handle API traffic in a distributed, microservices-based architecture.\n\n## Get Enterprise Support for Your Envoy Gateway Deployment\n\n[Tetrate Enterprise Gateway for Envoy](/tetrate-enterprise-gateway-for-envoy) provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.\n\nTetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.\n\n- [Get access now ›](/demo-request)\n- [Learn more ›](/tetrate-enterprise-gateway-for-envoy)\n- [Quick start ›](/quick-start)\n\n## Get Started with Envoy Gateway\n\nGetting started with Envoy Gateway is easy. Go to the documentation on [the Envoy Gateway project site](/external-link/)\n\n- [Quick start ›](/quick-start)\n- [User guides ›](/user-guides)","src/content/learn/what-is-envoy-gateway/index.md","9632f3425cef7d05",{html:381,metadata:382},"<h2 id=\"overview\">Overview</h2>\n<p><a href=\"/external-link/\">Envoy Gateway</a></p>\n<blockquote>\n<p>Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a>. TEG is the easiest way to get started with Envoy for production use cases. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<p>Envoy Proxy is a high-performance open-source proxy server designed for cloud-native applications. It was initially created by Lyft in 2016, where it was used as an API gateway and edge proxy, offering observability insights <a href=\"/external-link/\">that aided Lyft’s transition</a></p>\n<p>While Envoy Proxy is typically used in conjunction with other technologies like <a href=\"/what-is-istio-service-mesh\">the Istio service mesh</a>, Envoy Gateway is a great way to get started with Envoy on its own as a cloud-native application gateway—especially for those coming from traditional API gateway technologies looking to adopt a more modern approach.</p>\n<p>Envoy Gateway provides a suite of services and features including:</p>\n<ul>\n<li>An <a href=\"/external-link/\">xDS</a></li>\n<li>An expressive API based on the Kubernetes Gateway API, with reasonable settings to simplify the Envoy user experience.</li>\n<li>Support for heterogeneous environments (although, early versions focus on deployment in Kubernetes).</li>\n<li>Extensibility to support a multitude of application gateway use cases.</li>\n<li>Envoy infrastructure provisioning and management.</li>\n<li>High-quality documentation, tooling, and a diverse group of project maintainers for support.</li>\n</ul>\n<p>Envoy Gateway makes it easy for organizations to leverage the power of Envoy for “north-south” traffic. With its support for multiple user personas, organizations can leave their existing operational models unchanged. For example, <strong>infrastructure admins</strong> can use Envoy Gateway to provision and manage fleets of Envoys while <strong>application developers</strong> can simply route application traffic to their backend services. In addition, Envoy Gateway implements the Kubernetes Gateway API which aims to standardize and improve all the Kubernetes APIs that are currently used for ingress.</p>\n<h2 id=\"benefits-of-envoy-gateway\">Benefits of Envoy Gateway</h2>\n<p>Using Envoy Gateway provides several benefits for microservices and cloud-native applications, including:</p>\n<p><strong>High performance.</strong> Envoy Gateway is built on top of the high-performance Envoy proxy, which can handle millions of requests per second. This makes it an ideal choice for high-traffic APIs.</p>\n<p><strong>Extensibility.</strong> Envoy has a flexible architecture and supports custom filters, allowing developers to extend its functionality according to their specific needs.</p>\n<p><strong>Dynamic configuration.</strong> Envoy supports dynamic service discovery and configuration updates, which allows it to adapt to changes in the system without the need for manual intervention or downtime.</p>\n<p><strong>Security.</strong> Envoy Gateway supports a variety of security features, such as Transport Layer Security (TLS), TLS pass-through, and secure gRPC. It also provides features for authentication and rate-limiting to protect backend services from unauthorized access and potential attacks.</p>\n<p><strong>Protocol support.</strong> Envoy Gateway supports a wide range of protocols, including HTTP/1.x, HTTP/2, gRPC, and TCP. This makes it suitable for various types of applications and services.</p>\n<p><strong>Seamless integration.</strong> Envoy Gateway can be easily integrated into existing infrastructure, including Kubernetes, service meshes, and other orchestration platforms. This seamless integration ensures a smoother transition and reduced operational overhead when adopting Envoy.</p>\n<p>Envoy Gateway makes it easy to get started using a modern application gateway and a great stepping stone towards adopting cloud-native architectures like service mesh.</p>\n<h2 id=\"envoy-gateway-project-goals\">Envoy Gateway Project Goals</h2>\n<p>The core of Envoy Proxy is inherently low-level, designed to be fast, flexible, capable, and extensible, but not necessarily easy to use by itself. Envoy Gateway was born to <a href=\"/external-link/\">“bring Envoy to the masses”</a></p>\n<p><strong>A streamlined API tailored for gateway use cases.</strong> Envoy Gateway uses the Kubernetes Gateway API with Envoy-specific extensions. This decision is based on the project’s initial focus on deployment within Kubernetes as an ingress controller plus the broad buy-in of the Gateway API.</p>\n<p><strong>A comprehensive, ready-to-use experience.</strong> This approach enables users to quickly set up and start using Envoy Gateway. It encompasses lifecycle management functionalities that handle controller resources, control plane resources, proxy instances, and more.</p>\n<p><strong>A flexible and expandable API surface.</strong> While the project aims to provide out-of-the-box functionality for common API gateway features (e.g., rate limiting, authentication, Let’s Encrypt integration), vendors can offer SaaS versions of all APIs, introduce additional APIs, and incorporate value-added features such as Web Application Firewall (WAF), enhanced observability, chaos engineering, etc.</p>\n<p><strong>High-quality documentation and beginner-friendly guides.</strong> The primary goal of Envoy Gateway is to simplify the process of setting up and using the most prevalent gateway use cases for the average user. This is achieved by providing comprehensive documentation and easy-to-follow getting started guides.</p>\n<h2 id=\"envoy-gateway-vs-traditional-api-gateway\">Envoy Gateway vs Traditional API Gateway</h2>\n<p>The main difference between Envoy Gateway and a traditional API Gateway is the architecture and the way they handle API traffic.</p>\n<p>Traditional API Gateways are often monolithic systems that handle all API traffic in a centralized manner. They are responsible for managing API routing, authentication, authorization, rate limiting, and other functions. Traditional API Gateways can become a bottleneck in high-traffic scenarios, and they can be difficult to scale.</p>\n<p>On the other hand, Envoy Gateway is designed to be a lightweight, modular component that can be easily deployed in front of microservices. It leverages the power of Envoy to provide API Gateway capabilities such as routing, authentication, and rate limiting, but it can also be extended with custom filters to build additional functionality.</p>\n<p>Envoy Gateway is a more flexible and scalable alternative to traditional API Gateways, designed to handle API traffic in a distributed, microservices-based architecture.</p>\n<h2 id=\"get-enterprise-support-for-your-envoy-gateway-deployment\">Get Enterprise Support for Your Envoy Gateway Deployment</h2>\n<p><a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy</a> provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.</p>\n<p>Tetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.</p>\n<ul>\n<li><a href=\"/demo-request\">Get access now ›</a></li>\n<li><a href=\"/tetrate-enterprise-gateway-for-envoy\">Learn more ›</a></li>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n</ul>\n<h2 id=\"get-started-with-envoy-gateway\">Get Started with Envoy Gateway</h2>\n<p>Getting started with Envoy Gateway is easy. Go to the documentation on <a href=\"/external-link/\">the Envoy Gateway project site</a></p>\n<ul>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n<li><a href=\"/user-guides\">User guides ›</a></li>\n</ul>",{headings:383,localImagePaths:400,remoteImagePaths:401,frontmatter:402,imagePaths:405},[384,385,388,391,394,397],{depth:29,slug:186,text:187},{depth:29,slug:386,text:387},"benefits-of-envoy-gateway","Benefits of Envoy Gateway",{depth:29,slug:389,text:390},"envoy-gateway-project-goals","Envoy Gateway Project Goals",{depth:29,slug:392,text:393},"envoy-gateway-vs-traditional-api-gateway","Envoy Gateway vs Traditional API Gateway",{depth:29,slug:395,text:396},"get-enterprise-support-for-your-envoy-gateway-deployment","Get Enterprise Support for Your Envoy Gateway Deployment",{depth:29,slug:398,text:399},"get-started-with-envoy-gateway","Get Started with Envoy Gateway",[],[],{title:372,slug:369,date:403,description:376,categories:404,excerpt:373},["Date","2024-10-08T07:50:50.000Z"],[337],[],"what-is-envoy-gateway/index.md","what-is-kubernetes-gateway-api",{id:407,data:409,body:415,filePath:416,digest:417,rendered:418,legacyId:457},{title:410,excerpt:411,categories:412,date:413,description:414,draft:20},"What Is the Kubernetes Gateway API?","The Kubernetes Gateway API(/learn/what-is-the-kubernetes-gateway-api/), aka “Gateway.",[337],["Date","2025-04-17T20:28:17.000Z"],"Discover Kubernetes Gateway API for efficient traffic management. Learn benefits, design goals, and comparison to Ingress. Optimize traffic management now!","## Overview\n\nThe [Kubernetes Gateway API](/learn/what-is-the-kubernetes-gateway-api), aka “[Gateway API,](/external-link/)\n\nConceived as a successor to the earlier Ingress API, Gateway API aims to enhance the configuration and management of Kubernetes ingress, service discovery, load balancing, and traffic routing by providing a unified and extensible API that integrates with Kubernetes’ native resources such as Services, Endpoints, and Ingresses. While there is no default implementation of the Gateway API out of the box in Kubernetes, there is [a wide range of commercial and open-source implementations available](/external-link/)\n\n> Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy). TEG is the easiest way to get started with Envoy for production use cases. [Get access now ›](/demo-request)\n\n## Benefits of the Gateway API\n\nGateway API represents a superset of Ingress functionality, enabling more advanced use cases. It offers the following benefits over earlier ingress implementations:\n\n- A comprehensive, unified, and standardized API for managing traffic into and out of a Kubernetes cluster.\n\n- More powerful and granular control, including expanded protocol support and routing options.\n- More flexible configuration that can be extended to address specific use cases.\n\n### Design Goals\n\nThe Gateway API is designed to be role-oriented, portable, expressive, and extensible:\n\n**Role-oriented:** Since [Kubernetes infrastructure](/learn/what-is-mtls) is typically a shared resource, multiple people with different roles and responsibilities must jointly participate in various aspects of the configuration and management of those resources. The Gateway API seeks to strike a balance between distributed flexibility and centralized control, allowing shared infrastructure to be used effectively by multiple, potentially non-coordinated teams.\n\n**Expressive:** As an advancement over Kubernetes Ingress, [the Gateway API](/learn/what-is-kubernetes-gateway-api) is meant to provide built-in core capabilities, such as header-based matching, traffic weighting, and other features that were previously only available through custom annotations that were not portable across implementations.\n\n**Portable:** The Gateway API is designed to be a universal specification supported by multiple implementations.\n\n**Flexible Conformance:** To accommodate a broad feature set and wide range of implementations, Gateway API offers three support levels: _core features_ that must be implemented; _extended features_ that are expected to be portable across implementations, but not universally supported; and _implementation-specific_ features that are not expected to be portable and are vendor-specific.\n\n**Extensible:** Users can add new features and functionality by defining their own custom resources that can be used alongside the existing resources defined in the specification. This extensibility allows the Gateway API to evolve over time and adapt to new use cases and requirements.\n\n## Gateway API vs Kubernetes Ingress\n\nBoth the Gateway API and Ingress are used for managing inbound traffic to Kubernetes clusters, but they differ in their approach and functionality.\n\n### Ingress\n\nIngress is an earlier API originally introduced to route incoming HTTP traffic to services using a straightforward, declarative syntax. It offered more control than the more limited options available at the time for exposing services in Kubernetes to the outside world.\n\nIngress controllers, such as NGINX, Traefik, or Istio, may be used to implement the Ingress resource and provide additional features like SSL termination, load balancing algorithms, and traffic shaping. More advanced features have been added by vendors, typically in the form of custom annotations that aren’t always portable across implementations.\n\n### Ingress Limitations\n\nWhile effective for basic use cases, Ingress has significant limitations for advanced uses, including:\n\n- **Limited power and flexibility:** Ingress is often not powerful or flexible enough for most real-world use and it only supports HTTP protocol routing.\n- **Limited expressiveness:** It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through non-portable annotations.\n- **Proliferation of non-portable, vendor-specific annotations:** The lack of advanced capabilities has driven a proliferation of implementation-specific annotations. For example,  URL redirection using the NGINX Ingress Controller requires configuration of the nginx.ingress.kubernetes.io/rewrite-target annotation, which makes it incompatible with a programmable proxy like Envoy.\n- **Lack of cross-namespace support.** Since Ingress can only route traffic to a single namespace, it can’t be used as a unified gateway across multiple namespaces.\n- **Lack of role-based configuration and management responsibilities.** Since there is no built-in delineation of responsibilities, operational tasks like creating and managing gateways that more properly belong to platform engineering are often shouldered by app developers.\n\n### Gateway API vs Ingress\n\nGateway API is more general-purpose than Ingress and can be used to configure a variety of traffic management features such as load balancing, TLS passthrough, traffic routing based on request headers, and integration with external services in a more consistent, portable way.\n\nLike Ingress, Gateway API is an official [Kubernetes API](/learn/what-is-the-kubernetes-gateway-api) and represents a superset of Ingress functionality, enabling more advanced use cases. Similar to Ingress, there is no default implementation of Gateway API built into Kubernetes. Instead, there are [many different implementations available](/external-link/)\n\nFor a deep dive on the history of Kubernetes Ingress and Gateway API, read our article, [Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh](/blog/why-the-gateway-api-is-the-unified-future-of-ingress-for-kubernetes-and-service-mesh).\n\n## Gateway API vs API Gateway\n\nThe Gateway API is a built-in Kubernetes API that provides a standardized way to manage and configure inbound traffic in Kubernetes environments.\n\nAn API Gateway provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the [API endpoints of an application](/learn/what-is-an-api-gateway).\n\nImplementations of Gateway API, such as the Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API Gateway capabilities.\n\n## How Does the Gateway API Work?\n\nThe Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for multiple protocols in addition to HTTP. It models more infrastructure components, providing better deployment and management options for cluster operations.\n\nIn addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API:\n\nThe following is an example of using the Gateway API in Istio:\n\n```\napiVersion: gateway.networking.k8s.io/v1alpha2\n kind: Gateway\n metadata:\n  name: gateway\n  namespace: istio-ingress\n  spec:\n  gatewayClassName: istio\n  listeners:\n\n    hostname: \"*.example.com\"\n    port: 80\n    protocol: HTTP\n    allowedRoutes:\n      namespaces:\n        from: All\n ---\n apiVersion: gateway.networking.k8s.io/v1alpha2\n kind: HTTPRoute\n metadata:\n  name: http\n  namespace: default\n  spec:\n  parentRefs:\n\n    namespace: istio-ingress\n  hostnames: [\"httpbin.example.com\"]\n  rules:\n\n        type: PathPrefix\n        value: /\n    backendRefs:\n\n      port: 8000\n```\n\nSimilar to Ingress, Gateway uses `gatewayClassName` to declare the controller it uses, which needs to be created by the platform administrator and allows client requests for the `*.example.com` domain. Application developers can create routing rules in the namespace where their service resides, in this case, default, and bind to the Gateway via parentRefs, but only if the Gateway explicitly allows them to do so (via the rules set in the `allowRoutes` field).\n\nWhen you apply the above configuration, Istio will automatically create a load-balancing gateway for you. The following diagram shows the workflow of the Gateway API:\n\nThe detailed process is as follows:\n\n1.  The infrastructure provider provides GatewayClass and Gateway Controller.\n2.  Platform operator deploy Gateway (multiple Gateways possible, or using different GatewayClasses).\n3.  Gateway Controller continuously monitors changes to the GatewayClass and Gateway objects in the Kubernetes API Server.\n4.  Gateway controller will create the corresponding gateway based on cluster operations and maintenance configuration.\n5.  Application developers apply xRoutes and bind them to the service.\n6.  If in the cloud, the client accesses the load balancer for that ingress gateway.\n7.  The gateway will route to the corresponding back-end service based on the matching criteria in the traffic request.\n\nFrom the above steps, we can see that the Gateway API has a clear division of roles compared to Ingress and that routing rules can be decoupled from the gateway configuration, significantly increasing management flexibility.\n\nThe following diagram shows the route flow after it is accessed at the gateway and processed:\n\nFrom this figure, we can see that the route is bound to the gateway. The route is generally deployed in the same namespace as its backend services. Suppose the route is in a different namespace, and you need to explicitly give the route cross-namespace reference rights in ReferenceGrant, for example. In that case, the following HTTPRoute `foo` in the `foo` namespace can refer to the `bar` service in the `bar` namespace:\n\n```\nkind: HTTPRoute\n metadata:\n  name: foo\n  namespace: foo\n spec:\n  rules:\n\n    forwardTo:\n      backend:\n\n        namespace: bar\n ---\n kind: ReferenceGrant\n metadata:\n  name: bar\n  namespace: bar\n spec:\n  from:\n\n    kind: HTTPRoute\n    namespace: foo\n  to:\n\n    kind: Service\n```\n\n## Get Started with Gateway API Using Envoy Gateway\n\nEnvoy Gateway is an implementation of the Gateway API that uses [Envoy Proxy](/what-is-envoy-proxy) as an API gateway to deliver a simplified deployment model and an API layer aimed at lighter use cases.\n\nGetting started with Gateway API and Envoy Gateway is easy. Go to the documentation on [the Envoy Gateway project site](/external-link/)\n\n- [Quick start ›](/quick-start)\n- [User guides ›](/user-guides)\n\n## Get Enterprise Support for Your Envoy Gateway Deployment\n\n[Tetrate Enterprise Gateway for Envoy](/tetrate-enterprise-gateway-for-envoy) provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.\n\nTetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.\n\n- [Get access now ›](/demo-request)\n- [Learn more ›](/tetrate-enterprise-gateway-for-envoy)\n- [Quick start ›](/quick-start)","src/content/learn/what-is-kubernetes-gateway-api/index.md","1a51146ba11799ea",{html:419,metadata:420},"<h2 id=\"overview\">Overview</h2>\n<p>The <a href=\"/learn/what-is-the-kubernetes-gateway-api\">Kubernetes Gateway API</a>, aka “<a href=\"/external-link/\">Gateway API,</a></p>\n<p>Conceived as a successor to the earlier Ingress API, Gateway API aims to enhance the configuration and management of Kubernetes ingress, service discovery, load balancing, and traffic routing by providing a unified and extensible API that integrates with Kubernetes’ native resources such as Services, Endpoints, and Ingresses. While there is no default implementation of the Gateway API out of the box in Kubernetes, there is <a href=\"/external-link/\">a wide range of commercial and open-source implementations available</a></p>\n<blockquote>\n<p>Tetrate offers an enterprise-ready, 100% upstream distribution of Envoy Gateway, <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a>. TEG is the easiest way to get started with Envoy for production use cases. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h2 id=\"benefits-of-the-gateway-api\">Benefits of the Gateway API</h2>\n<p>Gateway API represents a superset of Ingress functionality, enabling more advanced use cases. It offers the following benefits over earlier ingress implementations:</p>\n<ul>\n<li>\n<p>A comprehensive, unified, and standardized API for managing traffic into and out of a Kubernetes cluster.</p>\n</li>\n<li>\n<p>More powerful and granular control, including expanded protocol support and routing options.</p>\n</li>\n<li>\n<p>More flexible configuration that can be extended to address specific use cases.</p>\n</li>\n</ul>\n<h3 id=\"design-goals\">Design Goals</h3>\n<p>The Gateway API is designed to be role-oriented, portable, expressive, and extensible:</p>\n<p><strong>Role-oriented:</strong> Since <a href=\"/learn/what-is-mtls\">Kubernetes infrastructure</a> is typically a shared resource, multiple people with different roles and responsibilities must jointly participate in various aspects of the configuration and management of those resources. The Gateway API seeks to strike a balance between distributed flexibility and centralized control, allowing shared infrastructure to be used effectively by multiple, potentially non-coordinated teams.</p>\n<p><strong>Expressive:</strong> As an advancement over Kubernetes Ingress, <a href=\"/learn/what-is-kubernetes-gateway-api\">the Gateway API</a> is meant to provide built-in core capabilities, such as header-based matching, traffic weighting, and other features that were previously only available through custom annotations that were not portable across implementations.</p>\n<p><strong>Portable:</strong> The Gateway API is designed to be a universal specification supported by multiple implementations.</p>\n<p><strong>Flexible Conformance:</strong> To accommodate a broad feature set and wide range of implementations, Gateway API offers three support levels: <em>core features</em> that must be implemented; <em>extended features</em> that are expected to be portable across implementations, but not universally supported; and <em>implementation-specific</em> features that are not expected to be portable and are vendor-specific.</p>\n<p><strong>Extensible:</strong> Users can add new features and functionality by defining their own custom resources that can be used alongside the existing resources defined in the specification. This extensibility allows the Gateway API to evolve over time and adapt to new use cases and requirements.</p>\n<h2 id=\"gateway-api-vs-kubernetes-ingress\">Gateway API vs Kubernetes Ingress</h2>\n<p>Both the Gateway API and Ingress are used for managing inbound traffic to Kubernetes clusters, but they differ in their approach and functionality.</p>\n<h3 id=\"ingress\">Ingress</h3>\n<p>Ingress is an earlier API originally introduced to route incoming HTTP traffic to services using a straightforward, declarative syntax. It offered more control than the more limited options available at the time for exposing services in Kubernetes to the outside world.</p>\n<p>Ingress controllers, such as NGINX, Traefik, or Istio, may be used to implement the Ingress resource and provide additional features like SSL termination, load balancing algorithms, and traffic shaping. More advanced features have been added by vendors, typically in the form of custom annotations that aren’t always portable across implementations.</p>\n<h3 id=\"ingress-limitations\">Ingress Limitations</h3>\n<p>While effective for basic use cases, Ingress has significant limitations for advanced uses, including:</p>\n<ul>\n<li><strong>Limited power and flexibility:</strong> Ingress is often not powerful or flexible enough for most real-world use and it only supports HTTP protocol routing.</li>\n<li><strong>Limited expressiveness:</strong> It only supports host and path matching, and there is no standard configuration for advanced routing features, which can only be achieved through non-portable annotations.</li>\n<li><strong>Proliferation of non-portable, vendor-specific annotations:</strong> The lack of advanced capabilities has driven a proliferation of implementation-specific annotations. For example,  URL redirection using the NGINX Ingress Controller requires configuration of the nginx.ingress.kubernetes.io/rewrite-target annotation, which makes it incompatible with a programmable proxy like Envoy.</li>\n<li><strong>Lack of cross-namespace support.</strong> Since Ingress can only route traffic to a single namespace, it can’t be used as a unified gateway across multiple namespaces.</li>\n<li><strong>Lack of role-based configuration and management responsibilities.</strong> Since there is no built-in delineation of responsibilities, operational tasks like creating and managing gateways that more properly belong to platform engineering are often shouldered by app developers.</li>\n</ul>\n<h3 id=\"gateway-api-vs-ingress\">Gateway API vs Ingress</h3>\n<p>Gateway API is more general-purpose than Ingress and can be used to configure a variety of traffic management features such as load balancing, TLS passthrough, traffic routing based on request headers, and integration with external services in a more consistent, portable way.</p>\n<p>Like Ingress, Gateway API is an official <a href=\"/learn/what-is-the-kubernetes-gateway-api\">Kubernetes API</a> and represents a superset of Ingress functionality, enabling more advanced use cases. Similar to Ingress, there is no default implementation of Gateway API built into Kubernetes. Instead, there are <a href=\"/external-link/\">many different implementations available</a></p>\n<p>For a deep dive on the history of Kubernetes Ingress and Gateway API, read our article, <a href=\"/blog/why-the-gateway-api-is-the-unified-future-of-ingress-for-kubernetes-and-service-mesh\">Why the Gateway API Is the Unified Future of Ingress for Kubernetes and Service Mesh</a>.</p>\n<h2 id=\"gateway-api-vs-api-gateway\">Gateway API vs API Gateway</h2>\n<p>The Gateway API is a built-in Kubernetes API that provides a standardized way to manage and configure inbound traffic in Kubernetes environments.</p>\n<p>An API Gateway provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the <a href=\"/learn/what-is-an-api-gateway\">API endpoints of an application</a>.</p>\n<p>Implementations of Gateway API, such as the Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API Gateway capabilities.</p>\n<h2 id=\"how-does-the-gateway-api-work\">How Does the Gateway API Work?</h2>\n<p>The Gateway API is a collection of API resources: GatewayClass, Gateway, HTTPRoute, TCPRoute, ReferenceGrant, etc. The Gateway API exposes a more generic proxy API that can be used for multiple protocols in addition to HTTP. It models more infrastructure components, providing better deployment and management options for cluster operations.</p>\n<p>In addition, the Gateway API achieves configuration decoupling by separating resource objects that people can manage in different roles. The following diagram shows the roles and objects in the Gateway API:</p>\n<p>The following is an example of using the Gateway API in Istio:</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: gateway.networking.k8s.io/v1alpha2</span></span>\n<span class=\"line\"><span> kind: Gateway</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: gateway</span></span>\n<span class=\"line\"><span>  namespace: istio-ingress</span></span>\n<span class=\"line\"><span>  spec:</span></span>\n<span class=\"line\"><span>  gatewayClassName: istio</span></span>\n<span class=\"line\"><span>  listeners:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    hostname: \"*.example.com\"</span></span>\n<span class=\"line\"><span>    port: 80</span></span>\n<span class=\"line\"><span>    protocol: HTTP</span></span>\n<span class=\"line\"><span>    allowedRoutes:</span></span>\n<span class=\"line\"><span>      namespaces:</span></span>\n<span class=\"line\"><span>        from: All</span></span>\n<span class=\"line\"><span> ---</span></span>\n<span class=\"line\"><span> apiVersion: gateway.networking.k8s.io/v1alpha2</span></span>\n<span class=\"line\"><span> kind: HTTPRoute</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: http</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span>  spec:</span></span>\n<span class=\"line\"><span>  parentRefs:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    namespace: istio-ingress</span></span>\n<span class=\"line\"><span>  hostnames: [\"httpbin.example.com\"]</span></span>\n<span class=\"line\"><span>  rules:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>        type: PathPrefix</span></span>\n<span class=\"line\"><span>        value: /</span></span>\n<span class=\"line\"><span>    backendRefs:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>      port: 8000</span></span></code></pre>\n<p>Similar to Ingress, Gateway uses <code>gatewayClassName</code> to declare the controller it uses, which needs to be created by the platform administrator and allows client requests for the <code>*.example.com</code> domain. Application developers can create routing rules in the namespace where their service resides, in this case, default, and bind to the Gateway via parentRefs, but only if the Gateway explicitly allows them to do so (via the rules set in the <code>allowRoutes</code> field).</p>\n<p>When you apply the above configuration, Istio will automatically create a load-balancing gateway for you. The following diagram shows the workflow of the Gateway API:</p>\n<p>The detailed process is as follows:</p>\n<ol>\n<li>The infrastructure provider provides GatewayClass and Gateway Controller.</li>\n<li>Platform operator deploy Gateway (multiple Gateways possible, or using different GatewayClasses).</li>\n<li>Gateway Controller continuously monitors changes to the GatewayClass and Gateway objects in the Kubernetes API Server.</li>\n<li>Gateway controller will create the corresponding gateway based on cluster operations and maintenance configuration.</li>\n<li>Application developers apply xRoutes and bind them to the service.</li>\n<li>If in the cloud, the client accesses the load balancer for that ingress gateway.</li>\n<li>The gateway will route to the corresponding back-end service based on the matching criteria in the traffic request.</li>\n</ol>\n<p>From the above steps, we can see that the Gateway API has a clear division of roles compared to Ingress and that routing rules can be decoupled from the gateway configuration, significantly increasing management flexibility.</p>\n<p>The following diagram shows the route flow after it is accessed at the gateway and processed:</p>\n<p>From this figure, we can see that the route is bound to the gateway. The route is generally deployed in the same namespace as its backend services. Suppose the route is in a different namespace, and you need to explicitly give the route cross-namespace reference rights in ReferenceGrant, for example. In that case, the following HTTPRoute <code>foo</code> in the <code>foo</code> namespace can refer to the <code>bar</code> service in the <code>bar</code> namespace:</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>kind: HTTPRoute</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: foo</span></span>\n<span class=\"line\"><span>  namespace: foo</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  rules:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    forwardTo:</span></span>\n<span class=\"line\"><span>      backend:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>        namespace: bar</span></span>\n<span class=\"line\"><span> ---</span></span>\n<span class=\"line\"><span> kind: ReferenceGrant</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: bar</span></span>\n<span class=\"line\"><span>  namespace: bar</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  from:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    kind: HTTPRoute</span></span>\n<span class=\"line\"><span>    namespace: foo</span></span>\n<span class=\"line\"><span>  to:</span></span>\n<span class=\"line\"><span></span></span>\n<span class=\"line\"><span>    kind: Service</span></span></code></pre>\n<h2 id=\"get-started-with-gateway-api-using-envoy-gateway\">Get Started with Gateway API Using Envoy Gateway</h2>\n<p>Envoy Gateway is an implementation of the Gateway API that uses <a href=\"/what-is-envoy-proxy\">Envoy Proxy</a> as an API gateway to deliver a simplified deployment model and an API layer aimed at lighter use cases.</p>\n<p>Getting started with Gateway API and Envoy Gateway is easy. Go to the documentation on <a href=\"/external-link/\">the Envoy Gateway project site</a></p>\n<ul>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n<li><a href=\"/user-guides\">User guides ›</a></li>\n</ul>\n<h2 id=\"get-enterprise-support-for-your-envoy-gateway-deployment\">Get Enterprise Support for Your Envoy Gateway Deployment</h2>\n<p><a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy</a> provides available 24/7 enterprise support and enablement plus FIPS-verified and CVE-protected builds of Envoy Gateway suitable for mission critical applications and regulated environments like FedRAMP.</p>\n<p>Tetrate is a leading contributor to open source Envoy and Envoy Gateway. Tetrate Enterprise Gateway for Envoy brings them to the enterprise, with the scale, reliability, performance and security necessary for large and mission-critical apps. Whatever your Kubernetes platforms of choice, rely on Tetrate’s expertise to deliver your services without missing a beat.</p>\n<ul>\n<li><a href=\"/demo-request\">Get access now ›</a></li>\n<li><a href=\"/tetrate-enterprise-gateway-for-envoy\">Learn more ›</a></li>\n<li><a href=\"/quick-start\">Quick start ›</a></li>\n</ul>",{headings:421,localImagePaths:451,remoteImagePaths:452,frontmatter:453,imagePaths:456},[422,423,426,430,433,435,438,441,444,447,450],{depth:29,slug:186,text:187},{depth:29,slug:424,text:425},"benefits-of-the-gateway-api","Benefits of the Gateway API",{depth:427,slug:428,text:429},3,"design-goals","Design Goals",{depth:29,slug:431,text:432},"gateway-api-vs-kubernetes-ingress","Gateway API vs Kubernetes Ingress",{depth:427,slug:434,text:82},"ingress",{depth:427,slug:436,text:437},"ingress-limitations","Ingress Limitations",{depth:427,slug:439,text:440},"gateway-api-vs-ingress","Gateway API vs Ingress",{depth:29,slug:442,text:443},"gateway-api-vs-api-gateway","Gateway API vs API Gateway",{depth:29,slug:445,text:446},"how-does-the-gateway-api-work","How Does the Gateway API Work?",{depth:29,slug:448,text:449},"get-started-with-gateway-api-using-envoy-gateway","Get Started with Gateway API Using Envoy Gateway",{depth:29,slug:395,text:396},[],[],{title:410,slug:407,date:454,description:414,categories:455,excerpt:411},["Date","2025-04-17T20:28:17.000Z"],[337],[],"what-is-kubernetes-gateway-api/index.md","what-is-an-api-gateway",{id:458,data:460,body:467,filePath:468,assetImports:469,digest:471,rendered:472,legacyId:492},{title:461,excerpt:462,featuredImage:463,categories:464,date:465,description:466,draft:20},"What Is an API Gateway?","Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different.","__ASTRO_IMAGE_https://lh5.googleusercontent.com/WSns3Ifou8w8ifxl3PTBGIvSP6UevjPdQW0PoT0zZdbwyouY1JFvYRB2Pm6Zx0eHygBK8IkkyKr6gtc4c_K56HJfMhACIWOFF4t20WpuJOof8fB7Zr4d9xeYEBQvefKRvTBfFLSOFUC05-maH_fpNTc",[337],["Date","2024-07-19T01:05:00.000Z"],"Overview Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications…","## Overview\n\nApplication Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different protocols.\n\n[An API gateway](/learn/what-is-an-api-gateway) is a software layer that sits between API clients and the services implementing the APIs they consume. It acts as a unified entry point for all API requests and provides several functionalities, such as request routing, security, rate limiting, and API versioning. It can also perform additional functions such as protocol translation and message transformation.\n\nIn addition, API gateways provide a layer of abstraction between client applications and backend services. This means client applications do not need to know the internal workings of the backend services, or even the API itself. Instead, they can rely on the API gateway to handle requests and responses.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. [Get access now ›](/demo-request)\n\nSimplifying the management of complex microservice-based architectures by providing a single point of entry for clients and allowing for the addition or removal of services without affecting clients is also a key benefit of API gateways . They can also improve security by handling authentication and access control for all services in a consistent and centralized way.\n\nallows single-IP-port to access all services running in k8s through ingress rules. The Ingress Controller service is set to load balancer to be accessible from the public internet.\n\nAn Ingress Controller is a Layer 4 and Layer 7 proxy that routes traffic from clients to the services deployed into Kubernetes. Like an API gateway, an Ingress Controller can manage traffic and provide visibility, troubleshooting, security, and identity. An Ingress Controller is limited to only Kubernetes services, while an API gateway can manage traffic for both Kubernetes and VM workloads. Envoy proxy is used by some popular Ingress Controllers such as Ambassador and Contour. Other tools that are widely used as Ingress controllers include Kong Ingress, HAProxy Ingress, NGINX Ingress, etc.\n\n## API Gateway vs the Kubernetes Gateway API\n\nThe Gateway API is a built-in Kubernetes API that represents a superset of Kubernetes Ingress and  provides a standardized way to manage and configure inbound traffic in Kubernetes environments.\n\nAn API gateway is an API management component that provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the API endpoints of an application.\n\nImplementations of Gateway API, such as [Envoy Gateway](/learn/what-is-an-api-gateway), the  Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API gateway capabilities.\n\n## API Gateway vs Service Mesh\n\nAPI gateway and service mesh are two different architectural patterns that can be used in modern application architectures that use microservices.\n\nAn API gateway is a software layer that sits between API clients and the APIs they consume. Its primary function is to act as an entry point for all API requests and provide several functionalities, such as request routing, security, rate limiting, and API versioning. Its focus is on managing the traffic between the API clients and the backend services that provide the APIs.\n\nService mesh, on the other hand, is a dedicated infrastructure layer that provides several functionalities, such as service discovery, load balancing, traffic management, security, and observability. Its focus is on managing the communication between the microservices within an application architecture, providing a reliable and scalable infrastructure layer for microservices.\n\nWhile API gateway and service mesh are different architectural patterns, they can complement each other in modern application architectures. An API gateway can be used as an entry point for external API requests, while a service mesh can be used to manage the communication between microservices within an application architecture.\n\nAPI gateway and service mesh can also share some functionalities, such as traffic management and security. For example, a service mesh can be used to manage the traffic between microservices within an application architecture, while an API gateway can be used to manage the traffic between the external API clients and the backend services that provide the APIs.\n\nA service mesh handles traffic flowing from external clients into an application and communication between services. A service mesh can drive both north-south traffic (i.e., among services in a data center) and east-west traffic (services between various data centers).\n\nIstio is the most widely-deployed service mesh. The figure below highlights how Istio handles the communication flow among various microservices (including Kubernetes and VMs):\n\nis an open source project that can be easily used as an API gateway. It is based on the Gateway API—a resource used for service networking in Kubernetes. This means when users create Gateway API resources in Kubernetes cluster, they will be translated into native Envoy API calls, so Envoy and xDS, its native API, will not need to be changed to add this new support.\n\n### Operational Benefits of Envoy Gateway\n\n- **App developers** can use Envoy Gateway to route external traffic to their application easily, without needing to build or extend control planes to manage traffic.\n- **Infrastructure teams** can get basic gateway functionality quickly with Envoy Gateway. They can provide Envoy-native experience to the application team without purchasing a vendor solution.\n\nEnvoy Gateway makes it easy for platform architects, infrastructure administrators, and developers to quickly adopt an Envoy-based API gateway. [Learn more about getting started with Envoy Gateway ›](/learn/what-is-envoy-gateway)","src/content/learn/what-is-an-api-gateway/index.md",[470],"https://lh5.googleusercontent.com/WSns3Ifou8w8ifxl3PTBGIvSP6UevjPdQW0PoT0zZdbwyouY1JFvYRB2Pm6Zx0eHygBK8IkkyKr6gtc4c_K56HJfMhACIWOFF4t20WpuJOof8fB7Zr4d9xeYEBQvefKRvTBfFLSOFUC05-maH_fpNTc","b7e8a91368405656",{html:473,metadata:474},"<h2 id=\"overview\">Overview</h2>\n<p>Application Programming Interfaces (APIs) are essential building blocks of modern software applications. APIs provide a standard way for different applications to communicate and exchange data with each other. However, managing APIs can be challenging, especially when dealing with multiple microservices, legacy systems, and different protocols.</p>\n<p><a href=\"/learn/what-is-an-api-gateway\">An API gateway</a> is a software layer that sits between API clients and the services implementing the APIs they consume. It acts as a unified entry point for all API requests and provides several functionalities, such as request routing, security, rate limiting, and API versioning. It can also perform additional functions such as protocol translation and message transformation.</p>\n<p>In addition, API gateways provide a layer of abstraction between client applications and backend services. This means client applications do not need to know the internal workings of the backend services, or even the API itself. Instead, they can rely on the API gateway to handle requests and responses.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<p>Simplifying the management of complex microservice-based architectures by providing a single point of entry for clients and allowing for the addition or removal of services without affecting clients is also a key benefit of API gateways . They can also improve security by handling authentication and access control for all services in a consistent and centralized way.</p>\n<p>allows single-IP-port to access all services running in k8s through ingress rules. The Ingress Controller service is set to load balancer to be accessible from the public internet.</p>\n<p>An Ingress Controller is a Layer 4 and Layer 7 proxy that routes traffic from clients to the services deployed into Kubernetes. Like an API gateway, an Ingress Controller can manage traffic and provide visibility, troubleshooting, security, and identity. An Ingress Controller is limited to only Kubernetes services, while an API gateway can manage traffic for both Kubernetes and VM workloads. Envoy proxy is used by some popular Ingress Controllers such as Ambassador and Contour. Other tools that are widely used as Ingress controllers include Kong Ingress, HAProxy Ingress, NGINX Ingress, etc.</p>\n<h2 id=\"api-gateway-vs-the-kubernetes-gateway-api\">API Gateway vs the Kubernetes Gateway API</h2>\n<p>The Gateway API is a built-in Kubernetes API that represents a superset of Kubernetes Ingress and  provides a standardized way to manage and configure inbound traffic in Kubernetes environments.</p>\n<p>An API gateway is an API management component that provides a single entry point for incoming requests and outgoing responses in front of the backend services that implement an API’s functionality. It typically provides a range of advanced features such as traffic routing, rate limiting, authentication, and authorization, among others, to help manage and secure the API endpoints of an application.</p>\n<p>Implementations of Gateway API, such as <a href=\"/learn/what-is-an-api-gateway\">Envoy Gateway</a>, the  Istio service mesh, and more advanced commercial offerings like Tetrate Service Bridge, can be used to implement API gateway capabilities.</p>\n<h2 id=\"api-gateway-vs-service-mesh\">API Gateway vs Service Mesh</h2>\n<p>API gateway and service mesh are two different architectural patterns that can be used in modern application architectures that use microservices.</p>\n<p>An API gateway is a software layer that sits between API clients and the APIs they consume. Its primary function is to act as an entry point for all API requests and provide several functionalities, such as request routing, security, rate limiting, and API versioning. Its focus is on managing the traffic between the API clients and the backend services that provide the APIs.</p>\n<p>Service mesh, on the other hand, is a dedicated infrastructure layer that provides several functionalities, such as service discovery, load balancing, traffic management, security, and observability. Its focus is on managing the communication between the microservices within an application architecture, providing a reliable and scalable infrastructure layer for microservices.</p>\n<p>While API gateway and service mesh are different architectural patterns, they can complement each other in modern application architectures. An API gateway can be used as an entry point for external API requests, while a service mesh can be used to manage the communication between microservices within an application architecture.</p>\n<p>API gateway and service mesh can also share some functionalities, such as traffic management and security. For example, a service mesh can be used to manage the traffic between microservices within an application architecture, while an API gateway can be used to manage the traffic between the external API clients and the backend services that provide the APIs.</p>\n<p>A service mesh handles traffic flowing from external clients into an application and communication between services. A service mesh can drive both north-south traffic (i.e., among services in a data center) and east-west traffic (services between various data centers).</p>\n<p>Istio is the most widely-deployed service mesh. The figure below highlights how Istio handles the communication flow among various microservices (including Kubernetes and VMs):</p>\n<p>is an open source project that can be easily used as an API gateway. It is based on the Gateway API—a resource used for service networking in Kubernetes. This means when users create Gateway API resources in Kubernetes cluster, they will be translated into native Envoy API calls, so Envoy and xDS, its native API, will not need to be changed to add this new support.</p>\n<h3 id=\"operational-benefits-of-envoy-gateway\">Operational Benefits of Envoy Gateway</h3>\n<ul>\n<li><strong>App developers</strong> can use Envoy Gateway to route external traffic to their application easily, without needing to build or extend control planes to manage traffic.</li>\n<li><strong>Infrastructure teams</strong> can get basic gateway functionality quickly with Envoy Gateway. They can provide Envoy-native experience to the application team without purchasing a vendor solution.</li>\n</ul>\n<p>Envoy Gateway makes it easy for platform architects, infrastructure administrators, and developers to quickly adopt an Envoy-based API gateway. <a href=\"/learn/what-is-envoy-gateway\">Learn more about getting started with Envoy Gateway ›</a></p>",{headings:475,localImagePaths:486,remoteImagePaths:487,frontmatter:488,imagePaths:491},[476,477,480,483],{depth:29,slug:186,text:187},{depth:29,slug:478,text:479},"api-gateway-vs-the-kubernetes-gateway-api","API Gateway vs the Kubernetes Gateway API",{depth:29,slug:481,text:482},"api-gateway-vs-service-mesh","API Gateway vs Service Mesh",{depth:427,slug:484,text:485},"operational-benefits-of-envoy-gateway","Operational Benefits of Envoy Gateway",[],[],{title:461,slug:458,date:489,description:466,featuredImage:470,categories:490,excerpt:462},["Date","2024-07-19T01:05:00.000Z"],[337],[],"what-is-an-api-gateway/index.md","what-are-microservices",{id:493,data:495,body:501,filePath:502,digest:503,rendered:504,legacyId:602},{title:496,excerpt:497,categories:498,date:499,description:500,draft:20},"What Are Microservices?","Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over.",[337],["Date","2023-06-30T07:21:19.000Z"],"Overview Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide…","## Overview\n\nMicroservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over time.\n\n## Benefits of Microservices\n\nSome of the key advantages of a [microservices architecture include](/learn/what-are-microservices):\n\n### Scalability\n\nMicroservices make it easier to scale applications as needed, by allowing developers to add or remove services as necessary.\n\n### Agility \n\nMicroservices enable faster development and deployment of new features, as each service can be developed and tested independently.\n\n### Resilience \n\nMicroservices architecture can help ensure that applications remain resilient in the face of failures, as services can be designed to fail independently of one another.\n\n### Maintainability\n\nBy breaking applications down into smaller, more manageable services, it becomes easier to maintain and update them over time.\n\n## Challenges of a Microservices Architecture\n\nThese are some of the key challenges that need to be addressed when developing and deploying microservices applications:\n\n### Complexity\n\nMicroservices architecture can be highly complex, with numerous services communicating with each other. It is important to have effective management tools and strategies in place to ensure that the entire system remains manageable and maintainable.\n\n### Observability\n\nAs highly distributed systems, it can be challenging to understand all of the interactions between services and to identify issues as they arise. It is important to have a suite of observability tools that allow you to monitor and track the performance of individual services and the system as a whole. This can help you identify and troubleshoot issues more quickly and effectively.\n\n### Service Discovery\n\nService discovery enables services to locate and communicate with each other in a scalable and efficient manner. A robust service discovery mechanism is essential for ensuring the reliable operation of the entire system.\n\n### Security \n\nAs many services communicate with each other over networks, it is important to have a comprehensive security strategy that protects sensitive data and ensures that only authorized services can access each other.\n\n### Deployment\n\nAn efficient automated deployment pipeline is key to enjoying the agility benefits of microservices applications. Having automation and continuous deployment in place will help ensure that updates are deployed quickly, efficiently and reliably.\n\n### Configuration Management\n\nManaging configurations across multiple services can be challenging. It is important to have effective configuration management tools and strategies in place to ensure that the entire system is configured properly and consistently.\n\n### Dependency Management\n\nWith multiple services interacting with each other, it is important to manage dependencies effectively to ensure that changes to one service do not impact the entire system. It is important to have effective dependency management tools and strategies in place to minimize the risk of such issues.\n\n### Resilience and Fault Tolerance\n\nFor applications with multiple dynamically deployed instances and many services, having mechanisms in place to ensure that the system remains available and responsive in the face of service failures or other issues is important. This can involve implementing resilience and fault tolerance mechanisms, such as circuit breaking and retries.\n\n## When Should You Use Microservices?\n\nMicroservices architecture can be a good fit for large, complex applications that require high scalability, flexibility and agility. Here are some scenarios where microservices architecture may be appropriate.\n\n### Large, Complex Applications\n\nMicroservices architecture is ideal for large, complex applications that require high scalability and agility. This approach can help you break down the application into smaller, more manageable components, making it easier to develop, test and deploy.\n\n### Rapid Iteration\n\nMicroservices architecture can help you develop and deploy new features more quickly and with less risk. Since each service is developed and deployed independently, it is easier to test and deploy new features without affecting the entire application.\n\n### High Traffic And Peak Loads\n\nIf your application experiences high traffic and peak loads, microservices architecture can help you scale individual components independently. This can help ensure that your application remains responsive and available even during high traffic periods.\n\n### Multiple Development Teams\n\nMicroservices architecture can help you divide the application into smaller, more manageable components, making it easier for multiple development teams to work on different services simultaneously. This can help speed up development times and reduce time-to-market.\n\n### Polyglot Development\n\nMicroservices architecture allows you to use different programming languages and technologies for different services. This can be useful if you have a diverse development team with different skill sets and preferences.\n\nMicroservices architecture can be a good fit for applications that require high scalability, agility and flexibility. However, it is important to carefully consider the complexity and potential challenges of this approach before adopting it.\n\n## Microservices vs Monolithic Architecture\n\nMicroservices and monolithic architectures are two different software development approaches that have their own advantages and disadvantages. Here are some key differences between the two:\n\n### Architecture\n\nIn a monolithic architecture, the entire application is built as a single, self-contained unit. In contrast, microservices applications are broken down into smaller, independent services that communicate with each other.\n\n### Development\n\nDevelopment of monolithic applications can be slower and more cumbersome, as any changes made to the application require the entire application to be rebuilt and redeployed. On the other hand, microservices development can be faster and more agile, as developers can work on different services simultaneously.\n\n### Deployment\n\nMonolithic applications are deployed as a single unit. This can make it challenging to deploy new features or updates without affecting the entire system. With microservices, each service can be deployed independently, which makes it easier to test and deploy new features without affecting the entire application.\n\n### Maintenance\n\nMaintenance and updates to monoliths can be more challenging, as any changes made to the application can affect the entire system. In contrast, maintenance and updates of microservices can be easier to manage, as each service can be updated independently without affecting the entire application.\n\n### Scalability\n\nScaling a monolithic application requires scaling the entire system. In contrast, with microservices, each service can be scaled independently, which means that you can allocate resources where they are needed most.\n\nMicroservices architecture can offer some advantages over monolithic architecture, including increased scalability, flexibility and faster development times. However, it also presents some unique challenges, including increased complexity. It’s important to carefully consider the advantages and disadvantages of both approaches before choosing the one that is right for your application.\n\n## Role of Service Mesh in Microservices Applications\n\nA service mesh is a dedicated infrastructure layer that handles communication between microservices. The service mesh provides a number of important functions, including:\n\n### Traffic Management\n\nA service mesh can handle traffic routing, load balancing and service discovery between services within the application. This can help ensure that traffic is directed to the appropriate service and can help improve application performance and reliability.\n\n### Service Discovery\n\nMicroservices scalability and high availability is typically accomplished by dynamically scaling up the number of service instances in response to increased load or service outage and scaling back down when needed to save costs. A service mesh keeps track of all available service instances, automatically routing requests to them. When service instances become unavailable due to down-scaling, maintenance, or failure, service discovery removes them from the list of available instances and the mesh routes new requests to remaining available instances.\n\n### Security\n\nA service mesh can provide security features such as service authentication, encryption and access control. This can help ensure that sensitive data is protected and that only authorized services can communicate with each other.\n\n### Observability \n\nA service mesh can provide monitoring and logging capabilities that allow you to track the health and performance of individual services within the application. This can help you identify and troubleshoot issues more quickly and effectively.\n\n### Resilience\n\nA service mesh can provide features such as circuit breaking and retries that can help ensure that the application remains available and responsive even during periods of high traffic or service failures.\n\nLearn more about Istio, the most widely-deployed service mesh ›","src/content/learn/what-are-microservices/index.md","97a204d6a64e0091",{html:505,metadata:506},"<h2 id=\"overview\">Overview</h2>\n<p>Microservices are a software architecture approach where applications are built as a collection of small, independent services that work together to provide functionality. Each microservice typically performs a single function or task and communicates with other microservices through lightweight protocols, such as RESTful APIs. This allows developers to work on different parts of the application without affecting other services, making it easier to maintain and update the system. The decomposition of functionality into smaller units also makes it easier to scale them independently, which makes the system more resilient to changes in traffic or demand over time.</p>\n<h2 id=\"benefits-of-microservices\">Benefits of Microservices</h2>\n<p>Some of the key advantages of a <a href=\"/learn/what-are-microservices\">microservices architecture include</a>:</p>\n<h3 id=\"scalability\">Scalability</h3>\n<p>Microservices make it easier to scale applications as needed, by allowing developers to add or remove services as necessary.</p>\n<h3 id=\"agility\">Agility </h3>\n<p>Microservices enable faster development and deployment of new features, as each service can be developed and tested independently.</p>\n<h3 id=\"resilience\">Resilience </h3>\n<p>Microservices architecture can help ensure that applications remain resilient in the face of failures, as services can be designed to fail independently of one another.</p>\n<h3 id=\"maintainability\">Maintainability</h3>\n<p>By breaking applications down into smaller, more manageable services, it becomes easier to maintain and update them over time.</p>\n<h2 id=\"challenges-of-a-microservices-architecture\">Challenges of a Microservices Architecture</h2>\n<p>These are some of the key challenges that need to be addressed when developing and deploying microservices applications:</p>\n<h3 id=\"complexity\">Complexity</h3>\n<p>Microservices architecture can be highly complex, with numerous services communicating with each other. It is important to have effective management tools and strategies in place to ensure that the entire system remains manageable and maintainable.</p>\n<h3 id=\"observability\">Observability</h3>\n<p>As highly distributed systems, it can be challenging to understand all of the interactions between services and to identify issues as they arise. It is important to have a suite of observability tools that allow you to monitor and track the performance of individual services and the system as a whole. This can help you identify and troubleshoot issues more quickly and effectively.</p>\n<h3 id=\"service-discovery\">Service Discovery</h3>\n<p>Service discovery enables services to locate and communicate with each other in a scalable and efficient manner. A robust service discovery mechanism is essential for ensuring the reliable operation of the entire system.</p>\n<h3 id=\"security\">Security </h3>\n<p>As many services communicate with each other over networks, it is important to have a comprehensive security strategy that protects sensitive data and ensures that only authorized services can access each other.</p>\n<h3 id=\"deployment\">Deployment</h3>\n<p>An efficient automated deployment pipeline is key to enjoying the agility benefits of microservices applications. Having automation and continuous deployment in place will help ensure that updates are deployed quickly, efficiently and reliably.</p>\n<h3 id=\"configuration-management\">Configuration Management</h3>\n<p>Managing configurations across multiple services can be challenging. It is important to have effective configuration management tools and strategies in place to ensure that the entire system is configured properly and consistently.</p>\n<h3 id=\"dependency-management\">Dependency Management</h3>\n<p>With multiple services interacting with each other, it is important to manage dependencies effectively to ensure that changes to one service do not impact the entire system. It is important to have effective dependency management tools and strategies in place to minimize the risk of such issues.</p>\n<h3 id=\"resilience-and-fault-tolerance\">Resilience and Fault Tolerance</h3>\n<p>For applications with multiple dynamically deployed instances and many services, having mechanisms in place to ensure that the system remains available and responsive in the face of service failures or other issues is important. This can involve implementing resilience and fault tolerance mechanisms, such as circuit breaking and retries.</p>\n<h2 id=\"when-should-you-use-microservices\">When Should You Use Microservices?</h2>\n<p>Microservices architecture can be a good fit for large, complex applications that require high scalability, flexibility and agility. Here are some scenarios where microservices architecture may be appropriate.</p>\n<h3 id=\"large-complex-applications\">Large, Complex Applications</h3>\n<p>Microservices architecture is ideal for large, complex applications that require high scalability and agility. This approach can help you break down the application into smaller, more manageable components, making it easier to develop, test and deploy.</p>\n<h3 id=\"rapid-iteration\">Rapid Iteration</h3>\n<p>Microservices architecture can help you develop and deploy new features more quickly and with less risk. Since each service is developed and deployed independently, it is easier to test and deploy new features without affecting the entire application.</p>\n<h3 id=\"high-traffic-and-peak-loads\">High Traffic And Peak Loads</h3>\n<p>If your application experiences high traffic and peak loads, microservices architecture can help you scale individual components independently. This can help ensure that your application remains responsive and available even during high traffic periods.</p>\n<h3 id=\"multiple-development-teams\">Multiple Development Teams</h3>\n<p>Microservices architecture can help you divide the application into smaller, more manageable components, making it easier for multiple development teams to work on different services simultaneously. This can help speed up development times and reduce time-to-market.</p>\n<h3 id=\"polyglot-development\">Polyglot Development</h3>\n<p>Microservices architecture allows you to use different programming languages and technologies for different services. This can be useful if you have a diverse development team with different skill sets and preferences.</p>\n<p>Microservices architecture can be a good fit for applications that require high scalability, agility and flexibility. However, it is important to carefully consider the complexity and potential challenges of this approach before adopting it.</p>\n<h2 id=\"microservices-vs-monolithic-architecture\">Microservices vs Monolithic Architecture</h2>\n<p>Microservices and monolithic architectures are two different software development approaches that have their own advantages and disadvantages. Here are some key differences between the two:</p>\n<h3 id=\"architecture\">Architecture</h3>\n<p>In a monolithic architecture, the entire application is built as a single, self-contained unit. In contrast, microservices applications are broken down into smaller, independent services that communicate with each other.</p>\n<h3 id=\"development\">Development</h3>\n<p>Development of monolithic applications can be slower and more cumbersome, as any changes made to the application require the entire application to be rebuilt and redeployed. On the other hand, microservices development can be faster and more agile, as developers can work on different services simultaneously.</p>\n<h3 id=\"deployment-1\">Deployment</h3>\n<p>Monolithic applications are deployed as a single unit. This can make it challenging to deploy new features or updates without affecting the entire system. With microservices, each service can be deployed independently, which makes it easier to test and deploy new features without affecting the entire application.</p>\n<h3 id=\"maintenance\">Maintenance</h3>\n<p>Maintenance and updates to monoliths can be more challenging, as any changes made to the application can affect the entire system. In contrast, maintenance and updates of microservices can be easier to manage, as each service can be updated independently without affecting the entire application.</p>\n<h3 id=\"scalability-1\">Scalability</h3>\n<p>Scaling a monolithic application requires scaling the entire system. In contrast, with microservices, each service can be scaled independently, which means that you can allocate resources where they are needed most.</p>\n<p>Microservices architecture can offer some advantages over monolithic architecture, including increased scalability, flexibility and faster development times. However, it also presents some unique challenges, including increased complexity. It’s important to carefully consider the advantages and disadvantages of both approaches before choosing the one that is right for your application.</p>\n<h2 id=\"role-of-service-mesh-in-microservices-applications\">Role of Service Mesh in Microservices Applications</h2>\n<p>A service mesh is a dedicated infrastructure layer that handles communication between microservices. The service mesh provides a number of important functions, including:</p>\n<h3 id=\"traffic-management\">Traffic Management</h3>\n<p>A service mesh can handle traffic routing, load balancing and service discovery between services within the application. This can help ensure that traffic is directed to the appropriate service and can help improve application performance and reliability.</p>\n<h3 id=\"service-discovery-1\">Service Discovery</h3>\n<p>Microservices scalability and high availability is typically accomplished by dynamically scaling up the number of service instances in response to increased load or service outage and scaling back down when needed to save costs. A service mesh keeps track of all available service instances, automatically routing requests to them. When service instances become unavailable due to down-scaling, maintenance, or failure, service discovery removes them from the list of available instances and the mesh routes new requests to remaining available instances.</p>\n<h3 id=\"security-1\">Security</h3>\n<p>A service mesh can provide security features such as service authentication, encryption and access control. This can help ensure that sensitive data is protected and that only authorized services can communicate with each other.</p>\n<h3 id=\"observability-1\">Observability </h3>\n<p>A service mesh can provide monitoring and logging capabilities that allow you to track the health and performance of individual services within the application. This can help you identify and troubleshoot issues more quickly and effectively.</p>\n<h3 id=\"resilience-1\">Resilience</h3>\n<p>A service mesh can provide features such as circuit breaking and retries that can help ensure that the application remains available and responsive even during periods of high traffic or service failures.</p>\n<p>Learn more about Istio, the most widely-deployed service mesh ›</p>",{headings:507,localImagePaths:596,remoteImagePaths:597,frontmatter:598,imagePaths:601},[508,509,512,515,518,521,524,527,530,531,534,536,539,542,545,548,551,554,557,560,563,566,569,570,573,575,578,580,583,586,588,590,593],{depth:29,slug:186,text:187},{depth:29,slug:510,text:511},"benefits-of-microservices","Benefits of Microservices",{depth:427,slug:513,text:514},"scalability","Scalability",{depth:427,slug:516,text:517},"agility","Agility ",{depth:427,slug:519,text:520},"resilience","Resilience ",{depth:427,slug:522,text:523},"maintainability","Maintainability",{depth:29,slug:525,text:526},"challenges-of-a-microservices-architecture","Challenges of a Microservices Architecture",{depth:427,slug:528,text:529},"complexity","Complexity",{depth:427,slug:276,text:277},{depth:427,slug:532,text:533},"service-discovery","Service Discovery",{depth:427,slug:279,text:535},"Security ",{depth:427,slug:537,text:538},"deployment","Deployment",{depth:427,slug:540,text:541},"configuration-management","Configuration Management",{depth:427,slug:543,text:544},"dependency-management","Dependency Management",{depth:427,slug:546,text:547},"resilience-and-fault-tolerance","Resilience and Fault Tolerance",{depth:29,slug:549,text:550},"when-should-you-use-microservices","When Should You Use Microservices?",{depth:427,slug:552,text:553},"large-complex-applications","Large, Complex Applications",{depth:427,slug:555,text:556},"rapid-iteration","Rapid Iteration",{depth:427,slug:558,text:559},"high-traffic-and-peak-loads","High Traffic And Peak Loads",{depth:427,slug:561,text:562},"multiple-development-teams","Multiple Development Teams",{depth:427,slug:564,text:565},"polyglot-development","Polyglot Development",{depth:29,slug:567,text:568},"microservices-vs-monolithic-architecture","Microservices vs Monolithic Architecture",{depth:427,slug:273,text:274},{depth:427,slug:571,text:572},"development","Development",{depth:427,slug:574,text:538},"deployment-1",{depth:427,slug:576,text:577},"maintenance","Maintenance",{depth:427,slug:579,text:514},"scalability-1",{depth:29,slug:581,text:582},"role-of-service-mesh-in-microservices-applications","Role of Service Mesh in Microservices Applications",{depth:427,slug:584,text:585},"traffic-management","Traffic Management",{depth:427,slug:587,text:533},"service-discovery-1",{depth:427,slug:589,text:280},"security-1",{depth:427,slug:591,text:592},"observability-1","Observability ",{depth:427,slug:594,text:595},"resilience-1","Resilience",[],[],{title:496,slug:493,date:599,description:500,categories:600,excerpt:497},["Date","2023-06-30T07:21:19.000Z"],[337],[],"what-are-microservices/index.md","kubernetes-security-best-practices",{id:603,data:605,body:611,filePath:612,digest:613,rendered:614,legacyId:636},{title:606,excerpt:607,categories:608,date:609,description:610,draft:20},"Kubernetes Security Best Practices","Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into Kubernetes security(/learn/kubernetes-security-best-practices) best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity.",[17],["Date","2025-02-12T00:11:19.000Z"],"Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications.…","Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into [Kubernetes security](/learn/kubernetes-security-best-practices) best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity threats.\n\nWe assume that readers know what Kubernetes is, the specialized language used to define components (clusters, pods, etc.), and typical use cases. We’ll proceed directly to discuss various Kubernetes security best practices and end with a short call to action pointing to [Tetrate’s Kubernetes management solutions](/kubernetes-consulting-services) and how adding them to your toolkit can simplify and enhance the [security and management of Kubernetes environments](/learn/kubernetes-security-architecture).\n\n## Kubernetes Network Security\n\nUsing Kubernetes to deploy and manage applications and services in containers doesn’t change the need to provide robust cybersecurity protections. It is still essential to secure the networking used within a Kubernetes cluster.\n\nA core step in delivering Kubernetes network security is defining and implementing network policies to control how pods communicate with each other and the wider network infrastructure beyond the Kubernetes environment. Another crucial step is the isolation of sensitive workloads using network segmentation techniques (discussed further in a section below).\n\nNetwork security best practices for Kubernetes deployments occur at multiple levels and via various techniques such as:\n\n**The API Server**. The central management interface for a Kubernetes deployment. It is critical to protect it against unauthorized access.\n\n**Authentication and Authorization Controls**. Implement [robust access control](/blog/rbac-vs-abac-vs-ngac) to provide granular access control for managing user and service account permissions.\n\n**Network Security Settings**. Use network policies and segmentation to control traffic flow, isolate sensitive workloads, and limit the potential attack surface available to cyber attackers.\n\n**Strong Secrets Management**. Use Kubernetes Secrets or third-party secret stores to enable the secure storage and use of sensitive security access data such as passwords and API keys.\n\n**Pod Security Policies**. Deploy well-tested Pod security policies to enforce security settings within the container environments to ensure secure configurations get deployed and limit potential attack vectors.\n\n**Image Security**. Regularly scan container deployment images for known vulnerabilities and ensure that security patches for all components that comprise a container package get updated with the latest security updates.\n\n## Kubernetes Microsegmentation\n\nMicrosegmentation in Kubernetes divides clusters into smaller, more manageable segments, each with specific security policies. This approach limits the potential impact of a breach, as attackers can only access a small portion of the network if they compromise a segment. Microsegmentation is core to delivering zero-trust networking best practices within a Kubernetes deployment.\n\nIn Kubernetes, microsegmentation is achieved using various techniques such as the namespaces outlined in the previous section, network policies to restrict traffic flows plus user/service access to Pods, and also via functionality provided by service meshes, such as encryption and traffic monitoring.\n\n## Kubernetes PCI Compliance\n\nAny organization handling credit card cardholder data is required to comply with the [Payment Card Industry (PCI) regulations](/resource/meeting-pci-compliance-standards). You don’t need to operate at the consumer or retail border and collect payments directly from customers to be required to implement PCI requirements. If you handle or store cardholder data, you must be PCI compliant.\n\nThe inherent security built into Kubernetes management containerized financial application deployments can help reduce the burden of achieving PCI compliance. Features such as implementing strict access controls, encrypting data at rest and in transit, and maintaining a [secure network architecture](/learn/kubernetes-security-architecture) directly map to PCI requirements. Kubernetes helps deliver PCI compliance through these built-in security features you will already use for basic security. It also allows you to easily integrate your deployed container-based financial applications with additional third-party security tools.\n\nSome PCI requirements that Kubernetes addresses as a core part of the platform are access control, microsegmentation, encrypting data at rest and in transit, logging and monitoring of traffic, plus vulnerability scanning and patching of deployed images and deployed containers.\n\n## Enhancing Kubernetes with Tetrate\n\nTetrate provides tools for seamlessly implementing security, identity, [traffic management, and other policies across Kubernetes](/manage-kubernetes-complexity) clusters. The tools address the unique challenges containers and Kubernetes bring regarding complexity, security, and consistency.\n\nOrganizations cannot approach security in Kubernetes with a one-size-fits-all mentality. Instead, it requires a comprehensive strategy tailored to each environment’s specific needs and architecture. By following the best practices mentioned above, in conjunction with management solutions such as Tetrate’s Istio Service Mesh and other Kubernetes tools, organizations can build solid and secure Kubernetes ecosystems capable of supporting their most critical applications without any additional management overhead.\n\nVisit our website’s Products, Solutions, and Learn section to read more. [Reach out to us today](/contact-sales) to discuss how we can collaborate to secure and simplify managing your Kubernetes container environment.","src/content/learn/kubernetes-security-best-practices/index.md","9db19c76ac960cf0",{html:615,metadata:616},"<p>Kubernetes has become a core component of modern application deployment by delivering flexibility, scalability, and efficiency for managing containerized applications. This article delves into <a href=\"/learn/kubernetes-security-best-practices\">Kubernetes security</a> best practices to guide technical decision-makers in fortifying their Kubernetes infrastructure against potential cybersecurity threats.</p>\n<p>We assume that readers know what Kubernetes is, the specialized language used to define components (clusters, pods, etc.), and typical use cases. We’ll proceed directly to discuss various Kubernetes security best practices and end with a short call to action pointing to <a href=\"/kubernetes-consulting-services\">Tetrate’s Kubernetes management solutions</a> and how adding them to your toolkit can simplify and enhance the <a href=\"/learn/kubernetes-security-architecture\">security and management of Kubernetes environments</a>.</p>\n<h2 id=\"kubernetes-network-security\">Kubernetes Network Security</h2>\n<p>Using Kubernetes to deploy and manage applications and services in containers doesn’t change the need to provide robust cybersecurity protections. It is still essential to secure the networking used within a Kubernetes cluster.</p>\n<p>A core step in delivering Kubernetes network security is defining and implementing network policies to control how pods communicate with each other and the wider network infrastructure beyond the Kubernetes environment. Another crucial step is the isolation of sensitive workloads using network segmentation techniques (discussed further in a section below).</p>\n<p>Network security best practices for Kubernetes deployments occur at multiple levels and via various techniques such as:</p>\n<p><strong>The API Server</strong>. The central management interface for a Kubernetes deployment. It is critical to protect it against unauthorized access.</p>\n<p><strong>Authentication and Authorization Controls</strong>. Implement <a href=\"/blog/rbac-vs-abac-vs-ngac\">robust access control</a> to provide granular access control for managing user and service account permissions.</p>\n<p><strong>Network Security Settings</strong>. Use network policies and segmentation to control traffic flow, isolate sensitive workloads, and limit the potential attack surface available to cyber attackers.</p>\n<p><strong>Strong Secrets Management</strong>. Use Kubernetes Secrets or third-party secret stores to enable the secure storage and use of sensitive security access data such as passwords and API keys.</p>\n<p><strong>Pod Security Policies</strong>. Deploy well-tested Pod security policies to enforce security settings within the container environments to ensure secure configurations get deployed and limit potential attack vectors.</p>\n<p><strong>Image Security</strong>. Regularly scan container deployment images for known vulnerabilities and ensure that security patches for all components that comprise a container package get updated with the latest security updates.</p>\n<h2 id=\"kubernetes-microsegmentation\">Kubernetes Microsegmentation</h2>\n<p>Microsegmentation in Kubernetes divides clusters into smaller, more manageable segments, each with specific security policies. This approach limits the potential impact of a breach, as attackers can only access a small portion of the network if they compromise a segment. Microsegmentation is core to delivering zero-trust networking best practices within a Kubernetes deployment.</p>\n<p>In Kubernetes, microsegmentation is achieved using various techniques such as the namespaces outlined in the previous section, network policies to restrict traffic flows plus user/service access to Pods, and also via functionality provided by service meshes, such as encryption and traffic monitoring.</p>\n<h2 id=\"kubernetes-pci-compliance\">Kubernetes PCI Compliance</h2>\n<p>Any organization handling credit card cardholder data is required to comply with the <a href=\"/resource/meeting-pci-compliance-standards\">Payment Card Industry (PCI) regulations</a>. You don’t need to operate at the consumer or retail border and collect payments directly from customers to be required to implement PCI requirements. If you handle or store cardholder data, you must be PCI compliant.</p>\n<p>The inherent security built into Kubernetes management containerized financial application deployments can help reduce the burden of achieving PCI compliance. Features such as implementing strict access controls, encrypting data at rest and in transit, and maintaining a <a href=\"/learn/kubernetes-security-architecture\">secure network architecture</a> directly map to PCI requirements. Kubernetes helps deliver PCI compliance through these built-in security features you will already use for basic security. It also allows you to easily integrate your deployed container-based financial applications with additional third-party security tools.</p>\n<p>Some PCI requirements that Kubernetes addresses as a core part of the platform are access control, microsegmentation, encrypting data at rest and in transit, logging and monitoring of traffic, plus vulnerability scanning and patching of deployed images and deployed containers.</p>\n<h2 id=\"enhancing-kubernetes-with-tetrate\">Enhancing Kubernetes with Tetrate</h2>\n<p>Tetrate provides tools for seamlessly implementing security, identity, <a href=\"/manage-kubernetes-complexity\">traffic management, and other policies across Kubernetes</a> clusters. The tools address the unique challenges containers and Kubernetes bring regarding complexity, security, and consistency.</p>\n<p>Organizations cannot approach security in Kubernetes with a one-size-fits-all mentality. Instead, it requires a comprehensive strategy tailored to each environment’s specific needs and architecture. By following the best practices mentioned above, in conjunction with management solutions such as Tetrate’s Istio Service Mesh and other Kubernetes tools, organizations can build solid and secure Kubernetes ecosystems capable of supporting their most critical applications without any additional management overhead.</p>\n<p>Visit our website’s Products, Solutions, and Learn section to read more. <a href=\"/contact-sales\">Reach out to us today</a> to discuss how we can collaborate to secure and simplify managing your Kubernetes container environment.</p>",{headings:617,localImagePaths:630,remoteImagePaths:631,frontmatter:632,imagePaths:635},[618,621,624,627],{depth:29,slug:619,text:620},"kubernetes-network-security","Kubernetes Network Security",{depth:29,slug:622,text:623},"kubernetes-microsegmentation","Kubernetes Microsegmentation",{depth:29,slug:625,text:626},"kubernetes-pci-compliance","Kubernetes PCI Compliance",{depth:29,slug:628,text:629},"enhancing-kubernetes-with-tetrate","Enhancing Kubernetes with Tetrate",[],[],{title:606,slug:603,date:633,description:610,categories:634,excerpt:607},["Date","2025-02-12T00:11:19.000Z"],[17],[],"kubernetes-security-best-practices/index.md","what-is-mtls",{id:637,data:639,body:645,filePath:646,digest:647,rendered:648,legacyId:742},{title:640,excerpt:641,categories:642,date:643,description:644,draft:20},"What Is mTLS?","Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where both parties must prove their identities to each other.  mTLS extends the security provided by TLS(/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery/) by adding mutual authentication between the client and the.",[17],["Date","2025-04-16T10:10:53.000Z"],"Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed…","Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where **both parties must prove their identities to each other**.  mTLS extends the security provided by [TLS](/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery) by adding mutual authentication between the client and the server.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the easiest way to implement mTLS for cloud-native applications. [Get access now ›](/demo-request)\n\n## How Does mTLS Work?\n\nSince mTLS is essentially two-way TLS, it’s helpful to understand how TLS works and the system of public key cryptography it relies on to establish trust and secure encryption.\n\nTLS is built on top of TCP as the session layer in the OSI model and provides the encryption for multiple application-layer protocols such as the HTTPS we see when browsing the web (Figure 1).\n\n**Figure 1.** _The unencrypted HTTP protocol vs the HTTPS protocol which uses TLS for encryption._\n\n### Encryption\n\nEncryption is the process of converting plaintext into ciphertext in order to protect it from unauthorized access. The process is done using an encryption algorithm and an encryption key, which is kept secret by the parties involved in the communication.\n\nThe encrypted data, or ciphertext, can only be decrypted and read by someone who has the corresponding decryption key. This allows the original plaintext to be, for example, transmitted securely over a network without the risk of being intercepted and read by unauthorized parties. Encrypting network data is known as “encryption in transit” or “encryption on the wire.” Encrypting data when it is stored, such as on a hard drive or in a cloud storage service, is often called “encryption at rest.” When we talk about encryption in the context of TLS and mTLS, we are talking about encryption in transit.\n\nThere are two main types of encryption: symmetric encryption and asymmetric encryption. In symmetric encryption, the same key is used for both encryption and decryption, while in asymmetric encryption, a different key is used for encryption and decryption. Both symmetric and asymmetric encryption have their own strengths and weaknesses, and are often used together to provide a combination of security and convenience.\n\n### Symmetric Cryptography Using a Secret Key\n\nSymmetric cryptography uses a single secret key to encrypt and decrypt messages. The secret key must be shared between the client and the server, but it must be kept secret (only the client and server  know what it is) since anyone with the secret key can read the messages.Symmetric cryptography is less computationally intensive than asymmetric encryption, but it has some disadvantages, especially with regard to key management. For client-server communication, the key must be shared securely between the client and server—which is hard to do securely over a network, especially across  public networks like the Internet. Also, the shared key must also be kept secret, since anyone with the key can decrypt the ciphertext. For these reasons, **TLS and mTLS use asymmetric encryption to establish a secure channel, then switch to the more efficient symmetric cryptography to encrypt the rest of the communication.**\n\n**Figure 2.** _TLS uses different encryption methods for establishing connections and transmitting data._\n\n### Asymmetric Cryptography Using Pairs of Public and Private Keys\n\nAsymmetric cryptography uses pairs of public and private keys to encrypt and decrypt data. Messages are encrypted with the public key, but can only be decrypted with the corresponding private key. The client and server share their public keys with each other, but keep their private keys private. When the client sends a message to the server, it encrypts the message with the server’s public key and the server uses its private key to decrypt the message. The same thing happens in reverse when the server sends a message to the client: the server encrypts the message with the _client’s_ _public key_ and the client uses its private key to decrypt the message from the server.\n\nSince neither the client nor the server need to share a secret key and their public keys don’t need to be shared securely, asymmetric cryptography works well in hostile environments like the open Internet. But, asymmetric cryptography can be orders of magnitude more computationally intensive than symmetric cryptography.\n\n### TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency\n\nTLS uses a combination of symmetric and asymmetric cryptography to strike a balance between [security](/learn/kubernetes-security-best-practices) and computational efficiency. The more computationally expensive asymmetric encryption is used to generate and exchange shared, secret session keys during the handshake when the TLS connection is established. The shared session keys  are then used by both the client and server to encrypt and decrypt the rest of their communication using less expensive symmetric encryption. The session keys are unique to each communication session, providing additional security by ensuring that each communication session has its own set of keys, limiting the risk of session key compromise. After the communication session is complete, the session keys are discarded and a new set of session keys is generated for each subsequent session.\n\n### Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)\n\nThe public keys used in asymmetric cryptography don’t have to be secret and can be shared publicly. But, we need a way to make sure that public keys are _authentic_—that the server you’re connecting to actually _is_ the server you think it is (and not a bad actor pretending to be that server, also known as a man-in-the-middle attack) and that the public key it gives you isn’t fake. To do this, TLS relies on a system of digitally signed certificates issued by a trusted third party called a certificate authority (CA) to prove the authenticity of the public key and the identity of the server presenting the key.\n\nThis system established to support issuing, validating, and revoking public key certificates is known as a public key infrastructure (PKI) ([Recommendation ITU-T X.509 | ISO/IEC 9594-8](/external-link/)\n\nDuring the TLS handshake, the server presents the certificate to the client to prove the authenticity of the server’s identity and the public key attached to the certificate. The client can then use the public key attached to the certificate to encrypt messages to the server.\n\n### Root CAs and Root Certificates\n\nFor public key infrastructure to work, everyone (and everything) using it must agree on a set of one or more trusted third parties. Those trusted third parties are known as “root CAs”  and they create and publish a self-signed “root certificate.”\n\nTypically, devices have a set of those trusted root certificates securely installed in what’s known as a “root store” or “root CA bundle.”  The presence of a root certificate in the root store of a device (or software installed on the device) establishes trust in the root CA that issued the root certificate.\n\nFor the public Internet, the root CAs are operated by well-known commercial or non-profit organizations, but some organizations operate their own PKI with their own CAs.\n\n### Subordinate CAs and Intermediate Certificates\n\nThe security of the private key held by a root CA is critical, since there is no (practical) way to revoke a root certificate. As a result, root CAs are almost never online. Instead, root CA’s delegate their authority to subordinate CAs by issuing an “intermediate certificate” to the subordinate CA. The validity of intermediate certificates can be traced back to the root CA that issued them and intermediate certificates can be revoked. Subordinate CAs can further delegate their authority to their own subordinate CAs, also by issuing intermediate certificates.\n\n### Leaf Certificates and Certificate Chains\n\nAn “end-entity” or “leaf” certificate is typically issued to individual servers by a CA. These certificates are the “leaves” of a hierarchical tree of authority that’s traceable from the leaf certificates up through the intermediate certificates, and ultimately to a trusted root certificate.\n\nWhen a client connects to the server, the server sends its leaf certificate along with the chain of certificates that can be traced all the way back to a root certificate. The client then validates the certificate, typically with the following checks:\n\n- **Verifying the certificate’s signature** to ensure that the certificate was actually issued by the CA and has not been tampered with.\n- **Checking the certificate’s status:** The client checks the certificate’s expiration date, as well as any other relevant information such as the domain name or the subject’s public key, to ensure that the certificate is still valid.\n- **Checking the certificate chain:** The client follows the links in the certificate chain, starting with the server’s leaf certificate, and checking each intermediate certificate in the chain. It uses the information in each certificate to [verify](/external-link/)\n- **Validating the root certificate:** The client validates the trusted root certificate by checking that it is in its list of trusted root CAs. This ensures that the root certificate is a trusted and authoritative source of information about other certificates.\n\n### CA Bundle\n\nA TLS client has a list of trusted root certificates installed in a “CA bundle”. The client uses the CA bundle to verify the signature on the server’s certificate and determine if it was issued by a trusted CA. If the signature is verified, the client can trust that the certificate belongs to the server it is communicating with.\n\n### Certificate Revocation\n\nIn a public key infrastructure, certificate revocation is a process that allows a CA to declare a certificate as invalid before its expiration date. This is done when a certificate is no longer considered trustworthy, such as when the private key associated with the certificate has been compromised or the subject of the certificate is no longer authorized to use it.\n\nClients can ask the CA if the certificate offered by a server is still valid. Two commonly used techniques are  certificate revocation lists (CRL) and the newer online certificate status protocol (OCSP). For CRLs, a CA publishes a signed list of revoked certificates that a client can check to ensure a server’s certificate is still valid.  With OCSP, a client can request the status of a particular certificate.\n\nBy allowing certificates to be declared invalid when they are no longer trustworthy, certificate revocation helps to protect against security risks and ensure the confidentiality and privacy of sensitive information transmitted over secure connections.\n\n### TLS Handshake\n\nWhen a client initiates a secure connection with a server, they perform a TLS handshake to negotiate the TLS protocol to use, establish the identity of the server, and to generate and share session keys that will be used for symmetric encryption of the subsequent messages in the session.\n\nFigure 3 below represents a simplified version of what happens during the TLS handshake.\n\n**Figure 3.** _Simplified TLS handshake flow._\n\n1.  A request from the client to the server containing information such as the TLS version and password combination supported by the client.The server responds to the client request and attaches a digital certificate.\n2.  The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. The client and server agree on and exchange a shared, secret session key.\n3.  Encrypted communication commences between the client and the server using a symmetric encryption with the shared secret key.\n\n## How Is mTLS Different from TLS and SSL?\n\nmTLS extends the security provided by TLS by adding mutual authentication between the client and the server. In mTLS, both the client and the server present their own certificates to each other and verify the identity of each other before establishing a secure connection.\n\nIn contrast, TLS (and its predecessor SSL) only provides authentication of the server to the client, which is sufficient for many use cases where the client trusts the server and wants to verify the identity of the server before sending sensitive data.\n\nTLS is an IETF standard that evolved from secure sockets layer (SSL) developed by Netscape in the 1990s and are closely related. The two terms, TLS and SSL, are often used interchangeably, though SSL has been deprecated due to security issues in favor of TLS.\n\n## Why Do I Need mTLS?\n\nmTLS is a crucial component of a [zero trust architecture](/zero-trust). One of the tenets of zero trust networking is to assume that an attacker is already on your network. To limit the “blast radius” of an intrusion, it’s important to prevent the intruder from pivoting to other resources on your network. mTLS connections limit reconnaissance and provide for the authenticity of communication so that an intruder can’t eavesdrop, alter messages, impersonate resources, or otherwise intercept messages on your network.\n\n## When Do I Need mTLS?\n\nAs part of a zero trust security posture, you should use mTLS for network communication between application components that you have some control over—like between microservices in a cluster.\n\nOne-way TLS is typically used by Internet clients to connect to web services, which means that only the server needs to show identification and is unconcerned with the identity of the client. One-way TLS allows you to use passwords, tokens, two-factor authentication, and other methods when you need to confirm the identity of the client. However, when using a supporting technology like a service mesh, mTLS operates outside the application and doesn’t require many changes to the application logic to implement.\n\nSince mTLS implementation calls for certificate exchange between services, as the number of services rises, managing numerous certificates becomes a laborious task. You can implement automatic mTLS to mitigate the complexity of certificate management with the aid of a service mesh.\n\n## When Don’t I Need mTLS?\n\nAlthough mTLS is the preferred protocol for securing inter-service communication in cloud-native applications, implementing mTLS is more complex. In some cases where there is high traffic volume or CPU utilization must be optimized, terminatingTLS at the traffic entry point and turning on mTLS internally only for specific, security-sensitive services can help minimize request response times and decrease compute resource consumption for some traffic with lower security requirements.\n\nThere may be cases where mTLS for certain connections is impractical, such as health checks or access to external services.\n\n## How Do I Implement mTLS?\n\n### The Hard Part of mTLS: Proving Identity\n\nWhile mTLS offers significant security advantages, it offers some implementation challenges, not least of which is establishing a secure mechanism for services to prove their identity to each other.\n\nFor regular TLS, it used to be hard to manage the certificates that prove the identity of a server to its clients. With the advent of [Let’s Encrypt](/external-link/)\n\nRolling your own automated certificate management system is impractical and risky. Getting mTLS certificate management right is hard and the consequences of getting it wrong are bad. You need a trusted, proven way to do it. A service mesh is purpose-built to provide the infrastructure you need to safely and securely implement mTLS between services.\n\n### Use a Service Mesh, the NIST Standard for Microservices Security\n\nIn its [standards for microservices security](/external-link/)\n\nIf you want the details of NIST’s standards for microservices security and how Tetrate helps meet them, check out [Tetrate’s Guide to Federal Security Requirements for Microservices](/resource/tetrate-guide-to-federal-security-requirements-for-microservices).\n\n## How Do I Implement mTLS with Istio?\n\nFigure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.\n\n**Figure 4.** _Istio mTLS flow_.\n\nIstio includes a built-in CA, and [Secret Discovery Service (SDS)](/external-link/)\n\n1.  The sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and _istiod_ issues the [SVID](/external-link/)\n2.  The sidecar of every workload intercepts all client requests within the pod.\n3.  The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the [JWT](/external-link/)\n4.  If the request is authenticated and authorized, the client and the server start to establish a connection for communication.\n\nIn Istio, authentication and authorization between services can be configured using one of three resource objects:\n\n- [**RequestAuthentication**](/external-link/)\n- [**PeerAuthentication**](/external-link/)\n- [**AuthorizationPolicy**](/external-link/)\n\n### How to Enable Automatic mTLS in Istio\n\nIn [Istio’s PeerAuthentication configuration](/external-link/)\n\n- PERMISSIVE: The workload’s default setting that allows it to accept either mTLS or plain text traffic.\n- STRICT: The workload accepts only mTLS traffic.\n- DISABLE: Disable mTLS. From a security perspective, mTLS should not be disabled unless you have your own security solution.\n- UNSET: Inherited from the parent, with the following priority: service specific > namespace scope > mesh scope setting.\n\nIstio’s peer authentication uses PERMISSIVE mode by default, automatically sending mTLS traffic to these workloads and clear text traffic to workloads without a sidecar. After including Kubernetes services in the Istio mesh, we can use PERMISSIVE mode first to prevent services from failing mTLS. We can use one of two ways to enable strict mTLS mode for certain services:\n\n- Use PeerAuthentication to define how traffic is transferred between sidecars.\n- Use DestinationRule to define the TLS settings in the traffic routing policy.\n\nThe reviews service’s mTLS configuration in the default namespace can be seen in the example below.\n\n### Use PeerAuthentication to Set mTLS for Workloads\n\nFor instance, the following configuration can be used to specify that a workload under a namespace has strict mTLS enabled.\n\n```\napiVersion: security.istio.io/v1beta1\n kind: PeerAuthentication\n metadata:\n  name: foo-peer-policy\n  namespace: default\n spec:\n  selector:\n    matchLabels:\n      app: reviews\n  mtls:\n    mode: STRICT\n```\n\nAccording to the [Istio documentation](/external-link/)\n\n### Use DestinationRule to Set up mTLS for Workloads\n\nTraffic routing policies, such as load balancing, anomaly detection, TLS settings, etc., are set using DestinationRule. In the TLS settings, there are various modes. As shown below, use `ISTIO_MUTUAL` mode to enable Istio’s workload-based automatic TLS.\n\n```\napiVersion: networking.istio.io/v1beta1\n kind: DestinationRule\n metadata:\n  name: reviews\n  namespace: default\n spec:\n  host: reviews\n  trafficPolicy:\n    tls:\n      mode: ISTIO_MUTUAL\n```\n\n## Service Mesh mTLS Best Practices for Enterprise Deployments\n\nIstio is the standard for implementing mTLS in a cloud-native environment, but there are some best practices that will help make your deployment of  mTLS in an enterprise environment more secure.\n\n### Best Practice: Don’t Use Self-Signed Certificates\n\nWhile Istio will implement mTLS for you, it uses self-signed certificates by default so you can see the mesh working right away, with minimal configuration. This makes the initial user experience easy, but it’s not not suitable for production environments. NIST’s guidance (NIST SP 800-204A, SM-DR12) is to disable the ability to generate self-signed certificates entirely.\n\n### Best Practice: Root Istio’s Trust in Your Existing PKI\n\nIf you’re not supposed to use Istio’s default self-signed certificates, what’s the alternative? The short answer is that you should [root Istio’s trust in your existing public key infrastructure (PKI)](/external-link/)\n\n### Best Practice: Use an Intermediate Certificate\n\nHow, exactly, do you root Istio’s trust in your existing PKI? Tetrate founding engineer and co-author of NIST’s security standards for microservices, Zack Butcher, [has all the details here](/external-link/)\n\n- Allow for fine-grained cert revocation without forcing new certificates across your entire infrastructure at the same time.\n- Enable easy rotation of signing certificates.\n\nFor step-by-step instructions on how to automate [Istio certificate](/blog/how-are-certificates-managed-in-istio) authority (CA) rotation, see our article on [automating Istio CA rotation in production at scale](/blog/automate-istio-ca-rotation-in-production-at-scale).\n\n## Is mTLS All I Need for Zero Trust Security?\n\nIn short, zero trust security is more than just mTLS, although mTLS is an important part of a zero trust architecture, especially for microservices. Zero trust networking is an approach governed by a few important principles more than it is a particular technology. In a zero trust network, all access to resources should be:\n\n- **Authenticated and dynamically authorized**, not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.\n- **Bounded in time:** authentication and authorization are bound to a short-lived session after which they must be re-established.\n- **Bounded in space:** the perimeter of trust around a service should be as small as possible.\n- **Encrypted**, both to prevent eavesdropping and to ensure messages are authentic and\n- unaltered.\n- **Observable**, so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.\n\nImplementing mTLS between resources like microservices in Kubernetes cluster provides authentication and encryption, but doesn’t address the rest of the full scope of a zero trust architecture. To learn more, [download our zero trust architecture white paper](/resource/zero-trust-architecture-white-paper) written by Zack Butcher, Tetrate founding engineer and co-author of the [NIST security standards for microservices applications](/blog/nist-standards-for-zero-trust-the-sp-800-204-series).","src/content/learn/what-is-mtls/index.md","88f6c3a9e2e82443",{html:649,metadata:650},"<p>Mutual TLS (mTLS) is a variation on transport layer security (TLS). Traditional TLS is the successor to secure sockets layer (SSL) and is the most widely deployed standard for secure communication, most visibly in HTTPS. TLS establishes secure communication that is both confidential (resistant to eavesdropping) and authentic (resistant to tampering) between a server that needs to prove its identity to its clients. But, in situations where both parties need to prove their identity to each other—such as between microservices in a Kubernetes application—TLS isn’t sufficient. mTLS is used in cases where <strong>both parties must prove their identities to each other</strong>.  mTLS extends the security provided by <a href=\"/blog/istio-gateway-upgrade-challenges-how-we-solved-tls-issues-and-ensured-seamless-service-delivery\">TLS</a> by adding mutual authentication between the client and the server.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the easiest way to implement mTLS for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h2 id=\"how-does-mtls-work\">How Does mTLS Work?</h2>\n<p>Since mTLS is essentially two-way TLS, it’s helpful to understand how TLS works and the system of public key cryptography it relies on to establish trust and secure encryption.</p>\n<p>TLS is built on top of TCP as the session layer in the OSI model and provides the encryption for multiple application-layer protocols such as the HTTPS we see when browsing the web (Figure 1).</p>\n<p><strong>Figure 1.</strong> <em>The unencrypted HTTP protocol vs the HTTPS protocol which uses TLS for encryption.</em></p>\n<h3 id=\"encryption\">Encryption</h3>\n<p>Encryption is the process of converting plaintext into ciphertext in order to protect it from unauthorized access. The process is done using an encryption algorithm and an encryption key, which is kept secret by the parties involved in the communication.</p>\n<p>The encrypted data, or ciphertext, can only be decrypted and read by someone who has the corresponding decryption key. This allows the original plaintext to be, for example, transmitted securely over a network without the risk of being intercepted and read by unauthorized parties. Encrypting network data is known as “encryption in transit” or “encryption on the wire.” Encrypting data when it is stored, such as on a hard drive or in a cloud storage service, is often called “encryption at rest.” When we talk about encryption in the context of TLS and mTLS, we are talking about encryption in transit.</p>\n<p>There are two main types of encryption: symmetric encryption and asymmetric encryption. In symmetric encryption, the same key is used for both encryption and decryption, while in asymmetric encryption, a different key is used for encryption and decryption. Both symmetric and asymmetric encryption have their own strengths and weaknesses, and are often used together to provide a combination of security and convenience.</p>\n<h3 id=\"symmetric-cryptography-using-a-secret-key\">Symmetric Cryptography Using a Secret Key</h3>\n<p>Symmetric cryptography uses a single secret key to encrypt and decrypt messages. The secret key must be shared between the client and the server, but it must be kept secret (only the client and server  know what it is) since anyone with the secret key can read the messages.Symmetric cryptography is less computationally intensive than asymmetric encryption, but it has some disadvantages, especially with regard to key management. For client-server communication, the key must be shared securely between the client and server—which is hard to do securely over a network, especially across  public networks like the Internet. Also, the shared key must also be kept secret, since anyone with the key can decrypt the ciphertext. For these reasons, <strong>TLS and mTLS use asymmetric encryption to establish a secure channel, then switch to the more efficient symmetric cryptography to encrypt the rest of the communication.</strong></p>\n<p><strong>Figure 2.</strong> <em>TLS uses different encryption methods for establishing connections and transmitting data.</em></p>\n<h3 id=\"asymmetric-cryptography-using-pairs-of-public-and-private-keys\">Asymmetric Cryptography Using Pairs of Public and Private Keys</h3>\n<p>Asymmetric cryptography uses pairs of public and private keys to encrypt and decrypt data. Messages are encrypted with the public key, but can only be decrypted with the corresponding private key. The client and server share their public keys with each other, but keep their private keys private. When the client sends a message to the server, it encrypts the message with the server’s public key and the server uses its private key to decrypt the message. The same thing happens in reverse when the server sends a message to the client: the server encrypts the message with the <em>client’s</em> <em>public key</em> and the client uses its private key to decrypt the message from the server.</p>\n<p>Since neither the client nor the server need to share a secret key and their public keys don’t need to be shared securely, asymmetric cryptography works well in hostile environments like the open Internet. But, asymmetric cryptography can be orders of magnitude more computationally intensive than symmetric cryptography.</p>\n<h3 id=\"tls-uses-both-symmetric-and-asymmetric-cryptography-to-balance-security-and-efficiency\">TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency</h3>\n<p>TLS uses a combination of symmetric and asymmetric cryptography to strike a balance between <a href=\"/learn/kubernetes-security-best-practices\">security</a> and computational efficiency. The more computationally expensive asymmetric encryption is used to generate and exchange shared, secret session keys during the handshake when the TLS connection is established. The shared session keys  are then used by both the client and server to encrypt and decrypt the rest of their communication using less expensive symmetric encryption. The session keys are unique to each communication session, providing additional security by ensuring that each communication session has its own set of keys, limiting the risk of session key compromise. After the communication session is complete, the session keys are discarded and a new set of session keys is generated for each subsequent session.</p>\n<h3 id=\"public-key-certificates-certificate-authorities-ca-and-public-key-infrastructure-pki\">Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)</h3>\n<p>The public keys used in asymmetric cryptography don’t have to be secret and can be shared publicly. But, we need a way to make sure that public keys are <em>authentic</em>—that the server you’re connecting to actually <em>is</em> the server you think it is (and not a bad actor pretending to be that server, also known as a man-in-the-middle attack) and that the public key it gives you isn’t fake. To do this, TLS relies on a system of digitally signed certificates issued by a trusted third party called a certificate authority (CA) to prove the authenticity of the public key and the identity of the server presenting the key.</p>\n<p>This system established to support issuing, validating, and revoking public key certificates is known as a public key infrastructure (PKI) (<a href=\"/external-link/\">Recommendation ITU-T X.509 | ISO/IEC 9594-8</a></p>\n<p>During the TLS handshake, the server presents the certificate to the client to prove the authenticity of the server’s identity and the public key attached to the certificate. The client can then use the public key attached to the certificate to encrypt messages to the server.</p>\n<h3 id=\"root-cas-and-root-certificates\">Root CAs and Root Certificates</h3>\n<p>For public key infrastructure to work, everyone (and everything) using it must agree on a set of one or more trusted third parties. Those trusted third parties are known as “root CAs”  and they create and publish a self-signed “root certificate.”</p>\n<p>Typically, devices have a set of those trusted root certificates securely installed in what’s known as a “root store” or “root CA bundle.”  The presence of a root certificate in the root store of a device (or software installed on the device) establishes trust in the root CA that issued the root certificate.</p>\n<p>For the public Internet, the root CAs are operated by well-known commercial or non-profit organizations, but some organizations operate their own PKI with their own CAs.</p>\n<h3 id=\"subordinate-cas-and-intermediate-certificates\">Subordinate CAs and Intermediate Certificates</h3>\n<p>The security of the private key held by a root CA is critical, since there is no (practical) way to revoke a root certificate. As a result, root CAs are almost never online. Instead, root CA’s delegate their authority to subordinate CAs by issuing an “intermediate certificate” to the subordinate CA. The validity of intermediate certificates can be traced back to the root CA that issued them and intermediate certificates can be revoked. Subordinate CAs can further delegate their authority to their own subordinate CAs, also by issuing intermediate certificates.</p>\n<h3 id=\"leaf-certificates-and-certificate-chains\">Leaf Certificates and Certificate Chains</h3>\n<p>An “end-entity” or “leaf” certificate is typically issued to individual servers by a CA. These certificates are the “leaves” of a hierarchical tree of authority that’s traceable from the leaf certificates up through the intermediate certificates, and ultimately to a trusted root certificate.</p>\n<p>When a client connects to the server, the server sends its leaf certificate along with the chain of certificates that can be traced all the way back to a root certificate. The client then validates the certificate, typically with the following checks:</p>\n<ul>\n<li><strong>Verifying the certificate’s signature</strong> to ensure that the certificate was actually issued by the CA and has not been tampered with.</li>\n<li><strong>Checking the certificate’s status:</strong> The client checks the certificate’s expiration date, as well as any other relevant information such as the domain name or the subject’s public key, to ensure that the certificate is still valid.</li>\n<li><strong>Checking the certificate chain:</strong> The client follows the links in the certificate chain, starting with the server’s leaf certificate, and checking each intermediate certificate in the chain. It uses the information in each certificate to <a href=\"/external-link/\">verify</a></li>\n<li><strong>Validating the root certificate:</strong> The client validates the trusted root certificate by checking that it is in its list of trusted root CAs. This ensures that the root certificate is a trusted and authoritative source of information about other certificates.</li>\n</ul>\n<h3 id=\"ca-bundle\">CA Bundle</h3>\n<p>A TLS client has a list of trusted root certificates installed in a “CA bundle”. The client uses the CA bundle to verify the signature on the server’s certificate and determine if it was issued by a trusted CA. If the signature is verified, the client can trust that the certificate belongs to the server it is communicating with.</p>\n<h3 id=\"certificate-revocation\">Certificate Revocation</h3>\n<p>In a public key infrastructure, certificate revocation is a process that allows a CA to declare a certificate as invalid before its expiration date. This is done when a certificate is no longer considered trustworthy, such as when the private key associated with the certificate has been compromised or the subject of the certificate is no longer authorized to use it.</p>\n<p>Clients can ask the CA if the certificate offered by a server is still valid. Two commonly used techniques are  certificate revocation lists (CRL) and the newer online certificate status protocol (OCSP). For CRLs, a CA publishes a signed list of revoked certificates that a client can check to ensure a server’s certificate is still valid.  With OCSP, a client can request the status of a particular certificate.</p>\n<p>By allowing certificates to be declared invalid when they are no longer trustworthy, certificate revocation helps to protect against security risks and ensure the confidentiality and privacy of sensitive information transmitted over secure connections.</p>\n<h3 id=\"tls-handshake\">TLS Handshake</h3>\n<p>When a client initiates a secure connection with a server, they perform a TLS handshake to negotiate the TLS protocol to use, establish the identity of the server, and to generate and share session keys that will be used for symmetric encryption of the subsequent messages in the session.</p>\n<p>Figure 3 below represents a simplified version of what happens during the TLS handshake.</p>\n<p><strong>Figure 3.</strong> <em>Simplified TLS handshake flow.</em></p>\n<ol>\n<li>A request from the client to the server containing information such as the TLS version and password combination supported by the client.The server responds to the client request and attaches a digital certificate.</li>\n<li>The client verifies the status, validity, and digital signature of the certificate and confirms the identity of the server. The client and server agree on and exchange a shared, secret session key.</li>\n<li>Encrypted communication commences between the client and the server using a symmetric encryption with the shared secret key.</li>\n</ol>\n<h2 id=\"how-is-mtls-different-from-tls-and-ssl\">How Is mTLS Different from TLS and SSL?</h2>\n<p>mTLS extends the security provided by TLS by adding mutual authentication between the client and the server. In mTLS, both the client and the server present their own certificates to each other and verify the identity of each other before establishing a secure connection.</p>\n<p>In contrast, TLS (and its predecessor SSL) only provides authentication of the server to the client, which is sufficient for many use cases where the client trusts the server and wants to verify the identity of the server before sending sensitive data.</p>\n<p>TLS is an IETF standard that evolved from secure sockets layer (SSL) developed by Netscape in the 1990s and are closely related. The two terms, TLS and SSL, are often used interchangeably, though SSL has been deprecated due to security issues in favor of TLS.</p>\n<h2 id=\"why-do-i-need-mtls\">Why Do I Need mTLS?</h2>\n<p>mTLS is a crucial component of a <a href=\"/zero-trust\">zero trust architecture</a>. One of the tenets of zero trust networking is to assume that an attacker is already on your network. To limit the “blast radius” of an intrusion, it’s important to prevent the intruder from pivoting to other resources on your network. mTLS connections limit reconnaissance and provide for the authenticity of communication so that an intruder can’t eavesdrop, alter messages, impersonate resources, or otherwise intercept messages on your network.</p>\n<h2 id=\"when-do-i-need-mtls\">When Do I Need mTLS?</h2>\n<p>As part of a zero trust security posture, you should use mTLS for network communication between application components that you have some control over—like between microservices in a cluster.</p>\n<p>One-way TLS is typically used by Internet clients to connect to web services, which means that only the server needs to show identification and is unconcerned with the identity of the client. One-way TLS allows you to use passwords, tokens, two-factor authentication, and other methods when you need to confirm the identity of the client. However, when using a supporting technology like a service mesh, mTLS operates outside the application and doesn’t require many changes to the application logic to implement.</p>\n<p>Since mTLS implementation calls for certificate exchange between services, as the number of services rises, managing numerous certificates becomes a laborious task. You can implement automatic mTLS to mitigate the complexity of certificate management with the aid of a service mesh.</p>\n<h2 id=\"when-dont-i-need-mtls\">When Don’t I Need mTLS?</h2>\n<p>Although mTLS is the preferred protocol for securing inter-service communication in cloud-native applications, implementing mTLS is more complex. In some cases where there is high traffic volume or CPU utilization must be optimized, terminatingTLS at the traffic entry point and turning on mTLS internally only for specific, security-sensitive services can help minimize request response times and decrease compute resource consumption for some traffic with lower security requirements.</p>\n<p>There may be cases where mTLS for certain connections is impractical, such as health checks or access to external services.</p>\n<h2 id=\"how-do-i-implement-mtls\">How Do I Implement mTLS?</h2>\n<h3 id=\"the-hard-part-of-mtls-proving-identity\">The Hard Part of mTLS: Proving Identity</h3>\n<p>While mTLS offers significant security advantages, it offers some implementation challenges, not least of which is establishing a secure mechanism for services to prove their identity to each other.</p>\n<p>For regular TLS, it used to be hard to manage the certificates that prove the identity of a server to its clients. With the advent of <a href=\"/external-link/\">Let’s Encrypt</a></p>\n<p>Rolling your own automated certificate management system is impractical and risky. Getting mTLS certificate management right is hard and the consequences of getting it wrong are bad. You need a trusted, proven way to do it. A service mesh is purpose-built to provide the infrastructure you need to safely and securely implement mTLS between services.</p>\n<h3 id=\"use-a-service-mesh-the-nist-standard-for-microservices-security\">Use a Service Mesh, the NIST Standard for Microservices Security</h3>\n<p>In its <a href=\"/external-link/\">standards for microservices security</a></p>\n<p>If you want the details of NIST’s standards for microservices security and how Tetrate helps meet them, check out <a href=\"/resource/tetrate-guide-to-federal-security-requirements-for-microservices\">Tetrate’s Guide to Federal Security Requirements for Microservices</a>.</p>\n<h2 id=\"how-do-i-implement-mtls-with-istio\">How Do I Implement mTLS with Istio?</h2>\n<p>Figure 4 depicts the security architecture of Istio. This figure clearly shows that at the entry point, JSON Web Token (JWT) + TLS authentication and encryption are used, and that mTLS is enabled between all services within the Istio mesh.</p>\n<p><strong>Figure 4.</strong> <em>Istio mTLS flow</em>.</p>\n<p>Istio includes a built-in CA, and <a href=\"/external-link/\">Secret Discovery Service (SDS)</a></p>\n<ol>\n<li>The sidecar of every service requests a certificate from Istiod on behalf of the workload at startup, and <em>istiod</em> issues the <a href=\"/external-link/\">SVID</a></li>\n<li>The sidecar of every workload intercepts all client requests within the pod.</li>\n<li>The client sidecar starts an mTLS handshake with the server sidecar. During the handshake, the <a href=\"/external-link/\">JWT</a></li>\n<li>If the request is authenticated and authorized, the client and the server start to establish a connection for communication.</li>\n</ol>\n<p>In Istio, authentication and authorization between services can be configured using one of three resource objects:</p>\n<ul>\n<li><a href=\"/external-link/\"><strong>RequestAuthentication</strong></a></li>\n<li><a href=\"/external-link/\"><strong>PeerAuthentication</strong></a></li>\n<li><a href=\"/external-link/\"><strong>AuthorizationPolicy</strong></a></li>\n</ul>\n<h3 id=\"how-to-enable-automatic-mtls-in-istio\">How to Enable Automatic mTLS in Istio</h3>\n<p>In <a href=\"/external-link/\">Istio’s PeerAuthentication configuration</a></p>\n<ul>\n<li>PERMISSIVE: The workload’s default setting that allows it to accept either mTLS or plain text traffic.</li>\n<li>STRICT: The workload accepts only mTLS traffic.</li>\n<li>DISABLE: Disable mTLS. From a security perspective, mTLS should not be disabled unless you have your own security solution.</li>\n<li>UNSET: Inherited from the parent, with the following priority: service specific > namespace scope > mesh scope setting.</li>\n</ul>\n<p>Istio’s peer authentication uses PERMISSIVE mode by default, automatically sending mTLS traffic to these workloads and clear text traffic to workloads without a sidecar. After including Kubernetes services in the Istio mesh, we can use PERMISSIVE mode first to prevent services from failing mTLS. We can use one of two ways to enable strict mTLS mode for certain services:</p>\n<ul>\n<li>Use PeerAuthentication to define how traffic is transferred between sidecars.</li>\n<li>Use DestinationRule to define the TLS settings in the traffic routing policy.</li>\n</ul>\n<p>The reviews service’s mTLS configuration in the default namespace can be seen in the example below.</p>\n<h3 id=\"use-peerauthentication-to-set-mtls-for-workloads\">Use PeerAuthentication to Set mTLS for Workloads</h3>\n<p>For instance, the following configuration can be used to specify that a workload under a namespace has strict mTLS enabled.</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: security.istio.io/v1beta1</span></span>\n<span class=\"line\"><span> kind: PeerAuthentication</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: foo-peer-policy</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  selector:</span></span>\n<span class=\"line\"><span>    matchLabels:</span></span>\n<span class=\"line\"><span>      app: reviews</span></span>\n<span class=\"line\"><span>  mtls:</span></span>\n<span class=\"line\"><span>    mode: STRICT</span></span></code></pre>\n<p>According to the <a href=\"/external-link/\">Istio documentation</a></p>\n<h3 id=\"use-destinationrule-to-set-up-mtls-for-workloads\">Use DestinationRule to Set up mTLS for Workloads</h3>\n<p>Traffic routing policies, such as load balancing, anomaly detection, TLS settings, etc., are set using DestinationRule. In the TLS settings, there are various modes. As shown below, use <code>ISTIO_MUTUAL</code> mode to enable Istio’s workload-based automatic TLS.</p>\n<pre class=\"astro-code nord\" style=\"background-color:#2e3440ff;color:#d8dee9ff; overflow-x: auto;\" tabindex=\"0\" data-language=\"plaintext\"><code><span class=\"line\"><span>apiVersion: networking.istio.io/v1beta1</span></span>\n<span class=\"line\"><span> kind: DestinationRule</span></span>\n<span class=\"line\"><span> metadata:</span></span>\n<span class=\"line\"><span>  name: reviews</span></span>\n<span class=\"line\"><span>  namespace: default</span></span>\n<span class=\"line\"><span> spec:</span></span>\n<span class=\"line\"><span>  host: reviews</span></span>\n<span class=\"line\"><span>  trafficPolicy:</span></span>\n<span class=\"line\"><span>    tls:</span></span>\n<span class=\"line\"><span>      mode: ISTIO_MUTUAL</span></span></code></pre>\n<h2 id=\"service-mesh-mtls-best-practices-for-enterprise-deployments\">Service Mesh mTLS Best Practices for Enterprise Deployments</h2>\n<p>Istio is the standard for implementing mTLS in a cloud-native environment, but there are some best practices that will help make your deployment of  mTLS in an enterprise environment more secure.</p>\n<h3 id=\"best-practice-dont-use-self-signed-certificates\">Best Practice: Don’t Use Self-Signed Certificates</h3>\n<p>While Istio will implement mTLS for you, it uses self-signed certificates by default so you can see the mesh working right away, with minimal configuration. This makes the initial user experience easy, but it’s not not suitable for production environments. NIST’s guidance (NIST SP 800-204A, SM-DR12) is to disable the ability to generate self-signed certificates entirely.</p>\n<h3 id=\"best-practice-root-istios-trust-in-your-existing-pki\">Best Practice: Root Istio’s Trust in Your Existing PKI</h3>\n<p>If you’re not supposed to use Istio’s default self-signed certificates, what’s the alternative? The short answer is that you should <a href=\"/external-link/\">root Istio’s trust in your existing public key infrastructure (PKI)</a></p>\n<h3 id=\"best-practice-use-an-intermediate-certificate\">Best Practice: Use an Intermediate Certificate</h3>\n<p>How, exactly, do you root Istio’s trust in your existing PKI? Tetrate founding engineer and co-author of NIST’s security standards for microservices, Zack Butcher, <a href=\"/external-link/\">has all the details here</a></p>\n<ul>\n<li>Allow for fine-grained cert revocation without forcing new certificates across your entire infrastructure at the same time.</li>\n<li>Enable easy rotation of signing certificates.</li>\n</ul>\n<p>For step-by-step instructions on how to automate <a href=\"/blog/how-are-certificates-managed-in-istio\">Istio certificate</a> authority (CA) rotation, see our article on <a href=\"/blog/automate-istio-ca-rotation-in-production-at-scale\">automating Istio CA rotation in production at scale</a>.</p>\n<h2 id=\"is-mtls-all-i-need-for-zero-trust-security\">Is mTLS All I Need for Zero Trust Security?</h2>\n<p>In short, zero trust security is more than just mTLS, although mTLS is an important part of a zero trust architecture, especially for microservices. Zero trust networking is an approach governed by a few important principles more than it is a particular technology. In a zero trust network, all access to resources should be:</p>\n<ul>\n<li><strong>Authenticated and dynamically authorized</strong>, not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.</li>\n<li><strong>Bounded in time:</strong> authentication and authorization are bound to a short-lived session after which they must be re-established.</li>\n<li><strong>Bounded in space:</strong> the perimeter of trust around a service should be as small as possible.</li>\n<li><strong>Encrypted</strong>, both to prevent eavesdropping and to ensure messages are authentic and</li>\n<li>unaltered.</li>\n<li><strong>Observable</strong>, so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.</li>\n</ul>\n<p>Implementing mTLS between resources like microservices in Kubernetes cluster provides authentication and encryption, but doesn’t address the rest of the full scope of a zero trust architecture. To learn more, <a href=\"/resource/zero-trust-architecture-white-paper\">download our zero trust architecture white paper</a> written by Zack Butcher, Tetrate founding engineer and co-author of the <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series\">NIST security standards for microservices applications</a>.</p>",{headings:651,localImagePaths:736,remoteImagePaths:737,frontmatter:738,imagePaths:741},[652,655,658,661,664,667,670,673,676,679,682,685,688,691,694,697,700,703,706,709,712,715,718,721,724,727,730,733],{depth:29,slug:653,text:654},"how-does-mtls-work","How Does mTLS Work?",{depth:427,slug:656,text:657},"encryption","Encryption",{depth:427,slug:659,text:660},"symmetric-cryptography-using-a-secret-key","Symmetric Cryptography Using a Secret Key",{depth:427,slug:662,text:663},"asymmetric-cryptography-using-pairs-of-public-and-private-keys","Asymmetric Cryptography Using Pairs of Public and Private Keys",{depth:427,slug:665,text:666},"tls-uses-both-symmetric-and-asymmetric-cryptography-to-balance-security-and-efficiency","TLS Uses Both Symmetric and Asymmetric Cryptography to Balance Security and Efficiency",{depth:427,slug:668,text:669},"public-key-certificates-certificate-authorities-ca-and-public-key-infrastructure-pki","Public Key Certificates, Certificate Authorities (CA), and Public Key Infrastructure (PKI)",{depth:427,slug:671,text:672},"root-cas-and-root-certificates","Root CAs and Root Certificates",{depth:427,slug:674,text:675},"subordinate-cas-and-intermediate-certificates","Subordinate CAs and Intermediate Certificates",{depth:427,slug:677,text:678},"leaf-certificates-and-certificate-chains","Leaf Certificates and Certificate Chains",{depth:427,slug:680,text:681},"ca-bundle","CA Bundle",{depth:427,slug:683,text:684},"certificate-revocation","Certificate Revocation",{depth:427,slug:686,text:687},"tls-handshake","TLS Handshake",{depth:29,slug:689,text:690},"how-is-mtls-different-from-tls-and-ssl","How Is mTLS Different from TLS and SSL?",{depth:29,slug:692,text:693},"why-do-i-need-mtls","Why Do I Need mTLS?",{depth:29,slug:695,text:696},"when-do-i-need-mtls","When Do I Need mTLS?",{depth:29,slug:698,text:699},"when-dont-i-need-mtls","When Don’t I Need mTLS?",{depth:29,slug:701,text:702},"how-do-i-implement-mtls","How Do I Implement mTLS?",{depth:427,slug:704,text:705},"the-hard-part-of-mtls-proving-identity","The Hard Part of mTLS: Proving Identity",{depth:427,slug:707,text:708},"use-a-service-mesh-the-nist-standard-for-microservices-security","Use a Service Mesh, the NIST Standard for Microservices Security",{depth:29,slug:710,text:711},"how-do-i-implement-mtls-with-istio","How Do I Implement mTLS with Istio?",{depth:427,slug:713,text:714},"how-to-enable-automatic-mtls-in-istio","How to Enable Automatic mTLS in Istio",{depth:427,slug:716,text:717},"use-peerauthentication-to-set-mtls-for-workloads","Use PeerAuthentication to Set mTLS for Workloads",{depth:427,slug:719,text:720},"use-destinationrule-to-set-up-mtls-for-workloads","Use DestinationRule to Set up mTLS for Workloads",{depth:29,slug:722,text:723},"service-mesh-mtls-best-practices-for-enterprise-deployments","Service Mesh mTLS Best Practices for Enterprise Deployments",{depth:427,slug:725,text:726},"best-practice-dont-use-self-signed-certificates","Best Practice: Don’t Use Self-Signed Certificates",{depth:427,slug:728,text:729},"best-practice-root-istios-trust-in-your-existing-pki","Best Practice: Root Istio’s Trust in Your Existing PKI",{depth:427,slug:731,text:732},"best-practice-use-an-intermediate-certificate","Best Practice: Use an Intermediate Certificate",{depth:29,slug:734,text:735},"is-mtls-all-i-need-for-zero-trust-security","Is mTLS All I Need for Zero Trust Security?",[],[],{title:640,slug:637,date:739,description:644,categories:740,excerpt:641},["Date","2025-04-16T10:10:53.000Z"],[17],[],"what-is-mtls/index.md","what-is-kubernetes-service-mesh",{id:743,data:745,body:751,filePath:752,digest:753,rendered:754,legacyId:780},{title:746,excerpt:747,categories:748,date:749,description:750,draft:20},"What Is Kubernetes Service Mesh","A Kubernetes service mesh(/what-is-istio-service-mesh/) is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via.",[17],["Date","2024-05-22T02:33:51.000Z"],"Traffic Management, Security, Observability and Reliability for Kubernetes A Dedicated Infrastructure Layer A Kubernetes service mesh is a software infrastructure layer…","## Traffic Management, Security, Observability and Reliability for Kubernetes\n\n### A Dedicated Infrastructure Layer\n\nA Kubernetes [service mesh](/what-is-istio-service-mesh) is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via containers.\n\nWhile containers and microservices bring many benefits when deploying applications, they also introduce issues that IT teams need to deal with.\n\n## Kubernetes Challenges\n\nCommon such issues are:\n\n**Increasing complexity**. When applications are decomposed into microservices that need to communicate within clusters, the network between them becomes very complex. Tracking the connections and their security policies as the number of microservices increases is difficult. Using a Kubernetes service mesh hides this complexity.    \n\n**Service opacity.** As the number of microservices increases, observing their interactions and communications becomes harder. A service mesh uses monitoring, logs, and connection tracing to pierce this opacity so you can see what’s happening. This is essential when optimizing performance and troubleshooting issues.\n\n**Managing dynamic changes.** Kubernetes supports auto-scaling of services, which can pose challenges for maintaining security policies and access control as services are dynamically added and removed.\n\n**Ingress and egress control.** Securing data flow into and out of a Kubernetes cluster is critical. Kubernetes’s dynamic nature may undermine any traditional security model organizations use. The service mesh makes delivering security in Kubernetes-based infrastructure easier.\n\nA Kubernetes service mesh links microservices, manages traffic flow between them, enforces policies, and collects telemetry data. The service mesh comprises several lightweight network proxies that are deployed alongside application containers in each service instance, using a sidecar proxy pattern. In a cluster, every pod has an extra container deployed, which is known as a sidecar, that intercepts traffic flowing to and from the primary application containers. It uses this network traffic to manage communication with other services and implement the features and policies that the mesh defines.\n\n## Service Mesh Solutions\n\nA service mesh delivers this key functionality within Kubernetes deployments:\n\n**Traffic management**. Service meshes provide advanced capabilities for traffic routing, including staged canary deployments, A/B testing, and blue-green deployments to production applications. They handle traffic flow within a cluster by ensuring that requests are directed to the correct service instance, even during network downtime or service updates, thus ensuring reliable routing.\n\n**Security**. Service meshes enforce policies around authentication, encryption, and authorization, ensuring communications within the cluster use secure service-to-service communication and are allowed to talk to each other. One standard method for this is mutual TLS (mTLS) for encryption and authentication. A service mesh like Istio also aligns with the Zero Trust security model defined in NIST SP 800-207A. Zero trust is becoming more important, and using a service mesh with Kubernetes helps deliver it. Also, it helps demonstrate the usage of zero trust principles to relevant third parties (such as regulators, insurers, and auditors). \n\n**Observability**. Service meshes collect telemetry data on service communication, such as container metrics, logs, and network traces. This data enables enhanced observability and is vital to understanding service behavior, detecting issues, and optimizing performance in production applications.\n\n**Reliability**. Service meshes help improve communication reliability through features like rate limiting, retries, circuit breaking, and timeouts. By handling these automatically, service meshes can ensure that the system remains resilient despite failures.\n\nIstio is one of the most popular and feature-rich service mesh offerings for Kubernetes. Istio is open source and uses a component called Envoy as the sidecar to intercept and deliver service mesh functionality.\n\n## Enterprise-Ready Service Mesh\n\nDeploying and managing Istio properly can still be complex, even if it’s not as difficult as managing Kubernetes clusters, pods, and containers directly. Tetrate is focused on the deployment and management of complex Kubernetes infrastructure using service mesh solutions. Tetrate’s enterprise service mesh products, [Tetrate Service Bridge (TSB)](/resource/tetrate-service-bridge-application-security-architecture), [Tetrate Istio Subscription (TIS)](/tetrate-istio-subscription), and [Tetrate Enterprise Gateway for Envoy (TEG)](/tetrate-enterprise-gateway-for-envoy) are enterprise-ready service mesh solutions that build on core Istio and Envoy functionality.\n\n## Conclusion\n\nIn summary, a service mesh in Kubernetes simplifies the management of microservices-based applications. With the complexities of network management abstracted away, organizations can focus on their application’s business logic. The service mesh creates a dedicated infrastructure layer for handling service communication, improving microservice observability, reliability, and security. Because of this, it is now an essential component of modern container-based and Kubernetes-managed application architectures. Tetrate Service Bridge takes service mesh functionality to the next level by enhancing the popular Istio service mesh while also making it easier to use in enterprise deployments.","src/content/learn/what-is-kubernetes-service-mesh/index.md","89053a21fd5f4475",{html:755,metadata:756},"<h2 id=\"traffic-management-security-observability-and-reliability-for-kubernetes\">Traffic Management, Security, Observability and Reliability for Kubernetes</h2>\n<h3 id=\"a-dedicated-infrastructure-layer\">A Dedicated Infrastructure Layer</h3>\n<p>A Kubernetes <a href=\"/what-is-istio-service-mesh\">service mesh</a> is a software infrastructure layer added within a Kubernetes cluster designed to simplify service-to-service communication, observability, and management of microservices running via containers.</p>\n<p>While containers and microservices bring many benefits when deploying applications, they also introduce issues that IT teams need to deal with.</p>\n<h2 id=\"kubernetes-challenges\">Kubernetes Challenges</h2>\n<p>Common such issues are:</p>\n<p><strong>Increasing complexity</strong>. When applications are decomposed into microservices that need to communicate within clusters, the network between them becomes very complex. Tracking the connections and their security policies as the number of microservices increases is difficult. Using a Kubernetes service mesh hides this complexity.    </p>\n<p><strong>Service opacity.</strong> As the number of microservices increases, observing their interactions and communications becomes harder. A service mesh uses monitoring, logs, and connection tracing to pierce this opacity so you can see what’s happening. This is essential when optimizing performance and troubleshooting issues.</p>\n<p><strong>Managing dynamic changes.</strong> Kubernetes supports auto-scaling of services, which can pose challenges for maintaining security policies and access control as services are dynamically added and removed.</p>\n<p><strong>Ingress and egress control.</strong> Securing data flow into and out of a Kubernetes cluster is critical. Kubernetes’s dynamic nature may undermine any traditional security model organizations use. The service mesh makes delivering security in Kubernetes-based infrastructure easier.</p>\n<p>A Kubernetes service mesh links microservices, manages traffic flow between them, enforces policies, and collects telemetry data. The service mesh comprises several lightweight network proxies that are deployed alongside application containers in each service instance, using a sidecar proxy pattern. In a cluster, every pod has an extra container deployed, which is known as a sidecar, that intercepts traffic flowing to and from the primary application containers. It uses this network traffic to manage communication with other services and implement the features and policies that the mesh defines.</p>\n<h2 id=\"service-mesh-solutions\">Service Mesh Solutions</h2>\n<p>A service mesh delivers this key functionality within Kubernetes deployments:</p>\n<p><strong>Traffic management</strong>. Service meshes provide advanced capabilities for traffic routing, including staged canary deployments, A/B testing, and blue-green deployments to production applications. They handle traffic flow within a cluster by ensuring that requests are directed to the correct service instance, even during network downtime or service updates, thus ensuring reliable routing.</p>\n<p><strong>Security</strong>. Service meshes enforce policies around authentication, encryption, and authorization, ensuring communications within the cluster use secure service-to-service communication and are allowed to talk to each other. One standard method for this is mutual TLS (mTLS) for encryption and authentication. A service mesh like Istio also aligns with the Zero Trust security model defined in NIST SP 800-207A. Zero trust is becoming more important, and using a service mesh with Kubernetes helps deliver it. Also, it helps demonstrate the usage of zero trust principles to relevant third parties (such as regulators, insurers, and auditors). </p>\n<p><strong>Observability</strong>. Service meshes collect telemetry data on service communication, such as container metrics, logs, and network traces. This data enables enhanced observability and is vital to understanding service behavior, detecting issues, and optimizing performance in production applications.</p>\n<p><strong>Reliability</strong>. Service meshes help improve communication reliability through features like rate limiting, retries, circuit breaking, and timeouts. By handling these automatically, service meshes can ensure that the system remains resilient despite failures.</p>\n<p>Istio is one of the most popular and feature-rich service mesh offerings for Kubernetes. Istio is open source and uses a component called Envoy as the sidecar to intercept and deliver service mesh functionality.</p>\n<h2 id=\"enterprise-ready-service-mesh\">Enterprise-Ready Service Mesh</h2>\n<p>Deploying and managing Istio properly can still be complex, even if it’s not as difficult as managing Kubernetes clusters, pods, and containers directly. Tetrate is focused on the deployment and management of complex Kubernetes infrastructure using service mesh solutions. Tetrate’s enterprise service mesh products, <a href=\"/resource/tetrate-service-bridge-application-security-architecture\">Tetrate Service Bridge (TSB)</a>, <a href=\"/tetrate-istio-subscription\">Tetrate Istio Subscription (TIS)</a>, and <a href=\"/tetrate-enterprise-gateway-for-envoy\">Tetrate Enterprise Gateway for Envoy (TEG)</a> are enterprise-ready service mesh solutions that build on core Istio and Envoy functionality.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>In summary, a service mesh in Kubernetes simplifies the management of microservices-based applications. With the complexities of network management abstracted away, organizations can focus on their application’s business logic. The service mesh creates a dedicated infrastructure layer for handling service communication, improving microservice observability, reliability, and security. Because of this, it is now an essential component of modern container-based and Kubernetes-managed application architectures. Tetrate Service Bridge takes service mesh functionality to the next level by enhancing the popular Istio service mesh while also making it easier to use in enterprise deployments.</p>",{headings:757,localImagePaths:774,remoteImagePaths:775,frontmatter:776,imagePaths:779},[758,761,764,767,770,773],{depth:29,slug:759,text:760},"traffic-management-security-observability-and-reliability-for-kubernetes","Traffic Management, Security, Observability and Reliability for Kubernetes",{depth:427,slug:762,text:763},"a-dedicated-infrastructure-layer","A Dedicated Infrastructure Layer",{depth:29,slug:765,text:766},"kubernetes-challenges","Kubernetes Challenges",{depth:29,slug:768,text:769},"service-mesh-solutions","Service Mesh Solutions",{depth:29,slug:771,text:772},"enterprise-ready-service-mesh","Enterprise-Ready Service Mesh",{depth:29,slug:360,text:361},[],[],{title:746,slug:743,date:777,description:750,categories:778,excerpt:747},["Date","2024-05-22T02:33:51.000Z"],[17],[],"what-is-kubernetes-service-mesh/index.md","what-is-observability",{id:781,data:783,body:789,filePath:790,digest:791,rendered:792,legacyId:839},{title:784,excerpt:785,categories:786,date:787,description:788,draft:20},"What Is Observability?","Observability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal.",[17],["Date","2024-03-19T17:04:19.000Z"],"Discover observability basics with Tetrate. Learn to optimize performance and troubleshoot efficiently. Explore now!","## Overview\n\nObservability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal workings.\n\nIn software engineering, observability is the ability to monitor and understand the behavior of distributed systems, microservices, or applications through the collection, processing, and visualization of telemetry data such as logs, metrics, traces, and events.\n\nObservability is essential for engineers to maintain and operate complex systems, troubleshoot issues, and ensure high availability and performance. Without observability, it’s difficult to understand what’s happening inside a system, leading to longer resolution times and decreased reliability.\n\n## The Three Pillars of Observability\n\nObservability for distributed software applications is typically achieved through the collection and analysis of three types of data: logs, metrics, and traces. Let’s take a closer look at each of these pillars.\n\n### Logs\n\nLogs are essentially records of events that occur within a system, such as user requests, errors, or system events. They provide a detailed view of system behavior, allowing engineers and operators to identify patterns and diagnose issues quickly.\n\n### Metrics\n\nMetrics are numerical measurements of system performance, such as CPU usage or network traffic. They provide a high-level view of system behavior, enabling engineers and operators to identify trends and anomalies.\n\n### Traces\n\nTraces are a record of the path of a user request as it moves through a system. They enable engineers and operators to gain a deep understanding of how different components of a system are interacting, allowing for more efficient problem solving.\n\nTogether, logs, metrics, and traces provide a comprehensive view of system behavior, enabling engineers and operators to gain a deep understanding of how their systems operate.\n\n## Tools and Techniques for Implementing Observability\n\nImplementing observability requires the use of specialized tools and techniques that enable the collection and analysis of the data types discussed above. Here are some of the most commonly used tools and techniques in the observability space:\n\n### Logging Frameworks\n\nLogging frameworks enable the collection and analysis of logs generated by a system. They allow engineers and operators to define what types of events should be logged and how they should be formatted, as well as providing tools for searching and analyzing log data.\n\nSome popular logging frameworks include ELK Stack, Graylog, and Fluentd.\n\n### Metrics Collection Tools\n\nMetrics collection tools enable the collection and analysis of numerical measurements generated by a system. They typically provide real-time dashboards that display key metrics such as CPU usage, memory usage, and network traffic.\n\nSome popular metrics collection tools include Prometheus, Graphite, and InfluxDB.\n\n### Tracing Tools\n\nTracing tools enable the collection and analysis of traces generated by a system. They provide a detailed view of how a user request moves through a system, including any microservices or other components it interacts with.\n\nSome popular tracing tools include Jaeger, Zipkin, and OpenTelemetry.\n\n### Apache SkyWalking\n\nIn addition to point solutions mentioned above, broader observability solutions like Apache SkyWalking are also available. SkyWalking is designed to be a more comprehensive observability platform that includes tracing as one of its core features. SkyWalking also includes a broader range of features beyond tracing, such as metrics collection and log analysis.\n\n## What’s the Difference Between Monitoring and Observability?\n\nMonitoring involves tracking and measuring predefined metrics or events related to the performance or behavior of a system. For example, a monitoring system might track CPU usage, network traffic, or the number of requests being processed per second. The goal of monitoring is to provide a high-level overview of system behavior, identify trends or anomalies, and trigger alerts or notifications when certain thresholds are exceeded.\n\nObservability, on the other hand, is a more holistic approach that focuses on the ability to understand and analyze a system based on its external outputs. Rather than being limited to a predefined set of metrics or events, observability involves collecting and analyzing a wide range of data points, including those that may not have been previously considered important or relevant. The goal of observability is to gain a deep understanding of system behavior, identify the root cause of issues or anomalies, and provide actionable insights for improving system performance and reliability.\n\n## The Role of Apache Skywalking in Observability\n\nApache SkyWalking is an open-source application performance monitor (APM) that can play an important role in enabling observability in modern applications. SkyWalking provides a comprehensive set of features for monitoring the performance of distributed systems, including tracing, metrics collection, and log analysis.\n\nObservability is a holistic approach to understanding system behavior that involves collecting and analyzing a wide range of data points, including logs, metrics, and traces. Apache SkyWalking is designed to provide visibility into many of these data points, particularly those related to application performance.\n\nOne of the key features of Apache SkyWalking is its distributed tracing capability, which enables teams to track the flow of requests through their systems and identify bottlenecks or issues that may be impacting performance. SkyWalking also supports the collection of metrics related to application performance, such as response times, error rates, and resource utilization, as well as the analysis of log data to identify patterns or anomalies.\n\nIn addition to these features, Apache SkyWalking is highly configurable and supports a wide range of programming languages and frameworks, making it a versatile tool for monitoring applications across different technology stacks. It also supports integrations with other monitoring and observability tools, such as Prometheus and Grafana, to provide a more comprehensive view of system behavior.\n\n## The Role of Service Mesh in Observability\n\nA service mesh is a dedicated infrastructure layer that provides connectivity and security for microservices within a distributed system. It can play an important role in enabling observability for such systems.\n\nOne of the key benefits of a service mesh is that it can provide visibility into the interactions between microservices. By capturing data related to requests, responses, and other interactions between services, a service mesh can provide valuable insights into the behavior of a system. This information can be used to improve system performance, identify issues or anomalies, and troubleshoot problems as they arise.\n\nIn addition, a service mesh can provide a centralized location for collecting and analyzing data related to the performance of microservices. This can include metrics such as response times, error rates, and resource utilization, as well as log data related to specific transactions or events. By providing a comprehensive view of system behavior, a service mesh can enable effective observability and help ensure the reliability, scalability, and maintainability of distributed systems.\n\nSome popular service mesh platforms that offer observability features include Istio, Linkerd, and Consul. These platforms provide a range of tools and techniques for collecting and analyzing data related to system behavior, and can be integrated with other observability tools to provide a complete view of system performance.\n\nEnterprise service mesh offerings like Tetrate Service Bridge can help provide unified and consistent observability signals across a fleet of applications in multiple clusters, clouds, and on premises.","src/content/learn/what-is-observability/index.md","ce6b9815c083a267",{html:793,metadata:794},"<h2 id=\"overview\">Overview</h2>\n<p>Observability refers to the ability to gain insight into the internal state of a system by observing its external behavior. In other words, it’s the ability to understand what’s happening inside a complex system by looking at its outputs, without necessarily needing to understand the system’s internal workings.</p>\n<p>In software engineering, observability is the ability to monitor and understand the behavior of distributed systems, microservices, or applications through the collection, processing, and visualization of telemetry data such as logs, metrics, traces, and events.</p>\n<p>Observability is essential for engineers to maintain and operate complex systems, troubleshoot issues, and ensure high availability and performance. Without observability, it’s difficult to understand what’s happening inside a system, leading to longer resolution times and decreased reliability.</p>\n<h2 id=\"the-three-pillars-of-observability\">The Three Pillars of Observability</h2>\n<p>Observability for distributed software applications is typically achieved through the collection and analysis of three types of data: logs, metrics, and traces. Let’s take a closer look at each of these pillars.</p>\n<h3 id=\"logs\">Logs</h3>\n<p>Logs are essentially records of events that occur within a system, such as user requests, errors, or system events. They provide a detailed view of system behavior, allowing engineers and operators to identify patterns and diagnose issues quickly.</p>\n<h3 id=\"metrics\">Metrics</h3>\n<p>Metrics are numerical measurements of system performance, such as CPU usage or network traffic. They provide a high-level view of system behavior, enabling engineers and operators to identify trends and anomalies.</p>\n<h3 id=\"traces\">Traces</h3>\n<p>Traces are a record of the path of a user request as it moves through a system. They enable engineers and operators to gain a deep understanding of how different components of a system are interacting, allowing for more efficient problem solving.</p>\n<p>Together, logs, metrics, and traces provide a comprehensive view of system behavior, enabling engineers and operators to gain a deep understanding of how their systems operate.</p>\n<h2 id=\"tools-and-techniques-for-implementing-observability\">Tools and Techniques for Implementing Observability</h2>\n<p>Implementing observability requires the use of specialized tools and techniques that enable the collection and analysis of the data types discussed above. Here are some of the most commonly used tools and techniques in the observability space:</p>\n<h3 id=\"logging-frameworks\">Logging Frameworks</h3>\n<p>Logging frameworks enable the collection and analysis of logs generated by a system. They allow engineers and operators to define what types of events should be logged and how they should be formatted, as well as providing tools for searching and analyzing log data.</p>\n<p>Some popular logging frameworks include ELK Stack, Graylog, and Fluentd.</p>\n<h3 id=\"metrics-collection-tools\">Metrics Collection Tools</h3>\n<p>Metrics collection tools enable the collection and analysis of numerical measurements generated by a system. They typically provide real-time dashboards that display key metrics such as CPU usage, memory usage, and network traffic.</p>\n<p>Some popular metrics collection tools include Prometheus, Graphite, and InfluxDB.</p>\n<h3 id=\"tracing-tools\">Tracing Tools</h3>\n<p>Tracing tools enable the collection and analysis of traces generated by a system. They provide a detailed view of how a user request moves through a system, including any microservices or other components it interacts with.</p>\n<p>Some popular tracing tools include Jaeger, Zipkin, and OpenTelemetry.</p>\n<h3 id=\"apache-skywalking\">Apache SkyWalking</h3>\n<p>In addition to point solutions mentioned above, broader observability solutions like Apache SkyWalking are also available. SkyWalking is designed to be a more comprehensive observability platform that includes tracing as one of its core features. SkyWalking also includes a broader range of features beyond tracing, such as metrics collection and log analysis.</p>\n<h2 id=\"whats-the-difference-between-monitoring-and-observability\">What’s the Difference Between Monitoring and Observability?</h2>\n<p>Monitoring involves tracking and measuring predefined metrics or events related to the performance or behavior of a system. For example, a monitoring system might track CPU usage, network traffic, or the number of requests being processed per second. The goal of monitoring is to provide a high-level overview of system behavior, identify trends or anomalies, and trigger alerts or notifications when certain thresholds are exceeded.</p>\n<p>Observability, on the other hand, is a more holistic approach that focuses on the ability to understand and analyze a system based on its external outputs. Rather than being limited to a predefined set of metrics or events, observability involves collecting and analyzing a wide range of data points, including those that may not have been previously considered important or relevant. The goal of observability is to gain a deep understanding of system behavior, identify the root cause of issues or anomalies, and provide actionable insights for improving system performance and reliability.</p>\n<h2 id=\"the-role-of-apache-skywalking-in-observability\">The Role of Apache Skywalking in Observability</h2>\n<p>Apache SkyWalking is an open-source application performance monitor (APM) that can play an important role in enabling observability in modern applications. SkyWalking provides a comprehensive set of features for monitoring the performance of distributed systems, including tracing, metrics collection, and log analysis.</p>\n<p>Observability is a holistic approach to understanding system behavior that involves collecting and analyzing a wide range of data points, including logs, metrics, and traces. Apache SkyWalking is designed to provide visibility into many of these data points, particularly those related to application performance.</p>\n<p>One of the key features of Apache SkyWalking is its distributed tracing capability, which enables teams to track the flow of requests through their systems and identify bottlenecks or issues that may be impacting performance. SkyWalking also supports the collection of metrics related to application performance, such as response times, error rates, and resource utilization, as well as the analysis of log data to identify patterns or anomalies.</p>\n<p>In addition to these features, Apache SkyWalking is highly configurable and supports a wide range of programming languages and frameworks, making it a versatile tool for monitoring applications across different technology stacks. It also supports integrations with other monitoring and observability tools, such as Prometheus and Grafana, to provide a more comprehensive view of system behavior.</p>\n<h2 id=\"the-role-of-service-mesh-in-observability\">The Role of Service Mesh in Observability</h2>\n<p>A service mesh is a dedicated infrastructure layer that provides connectivity and security for microservices within a distributed system. It can play an important role in enabling observability for such systems.</p>\n<p>One of the key benefits of a service mesh is that it can provide visibility into the interactions between microservices. By capturing data related to requests, responses, and other interactions between services, a service mesh can provide valuable insights into the behavior of a system. This information can be used to improve system performance, identify issues or anomalies, and troubleshoot problems as they arise.</p>\n<p>In addition, a service mesh can provide a centralized location for collecting and analyzing data related to the performance of microservices. This can include metrics such as response times, error rates, and resource utilization, as well as log data related to specific transactions or events. By providing a comprehensive view of system behavior, a service mesh can enable effective observability and help ensure the reliability, scalability, and maintainability of distributed systems.</p>\n<p>Some popular service mesh platforms that offer observability features include Istio, Linkerd, and Consul. These platforms provide a range of tools and techniques for collecting and analyzing data related to system behavior, and can be integrated with other observability tools to provide a complete view of system performance.</p>\n<p>Enterprise service mesh offerings like Tetrate Service Bridge can help provide unified and consistent observability signals across a fleet of applications in multiple clusters, clouds, and on premises.</p>",{headings:795,localImagePaths:833,remoteImagePaths:834,frontmatter:835,imagePaths:838},[796,797,800,803,806,809,812,815,818,821,824,827,830],{depth:29,slug:186,text:187},{depth:29,slug:798,text:799},"the-three-pillars-of-observability","The Three Pillars of Observability",{depth:427,slug:801,text:802},"logs","Logs",{depth:427,slug:804,text:805},"metrics","Metrics",{depth:427,slug:807,text:808},"traces","Traces",{depth:29,slug:810,text:811},"tools-and-techniques-for-implementing-observability","Tools and Techniques for Implementing Observability",{depth:427,slug:813,text:814},"logging-frameworks","Logging Frameworks",{depth:427,slug:816,text:817},"metrics-collection-tools","Metrics Collection Tools",{depth:427,slug:819,text:820},"tracing-tools","Tracing Tools",{depth:427,slug:822,text:823},"apache-skywalking","Apache SkyWalking",{depth:29,slug:825,text:826},"whats-the-difference-between-monitoring-and-observability","What’s the Difference Between Monitoring and Observability?",{depth:29,slug:828,text:829},"the-role-of-apache-skywalking-in-observability","The Role of Apache Skywalking in Observability",{depth:29,slug:831,text:832},"the-role-of-service-mesh-in-observability","The Role of Service Mesh in Observability",[],[],{title:784,slug:781,date:836,description:788,categories:837,excerpt:785},["Date","2024-03-19T17:04:19.000Z"],[17],[],"what-is-observability/index.md","what-is-platform-team",{id:840,data:842,body:848,filePath:849,digest:850,rendered:851,legacyId:907},{title:843,excerpt:844,categories:845,date:846,description:847,draft:20},"What Is a Platform Team?","A platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software.",[17],["Date","2025-04-18T10:02:55.000Z"],"Explore Tetrate's guide to platform teams: their role, functions, and significance in modern software development for scalability and efficiency.","## Overview\n\nA platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software [applications](/external-link/)\n\nThe platform team typically includes software engineers, systems administrators, network engineers, database administrators, security experts and other specialists who work together to ensure the platform is secure, scalable and reliable. The platform team is responsible for the technical aspects of the platform, including [architecture](/learn/kubernetes-security-architecture) design, system configuration, coding, testing, deployment and ongoing maintenance.\n\n> Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. [Get access now ›](/demo-request)\n\n### Origins of Platform Engineering\n\nPlatform engineering can be traced back to the rise of cloud computing and the need for scalable and reliable infrastructure to support software applications.\n\nAs organizations began to move their applications and data to the cloud, they saw the need for standardized and scalable infrastructure that could support the rapid development and deployment of software applications. This led to the development of platform-as-a-service (PaaS) offerings, which provided developers with a pre-built platform for building and deploying applications.\n\nAs PaaS offerings became more popular, organizations recognized the value of a dedicated internal developer platform and teams with a product mindset dedicated to building and managing it. This led to the emergence of platform teams and the concept of platform engineering.\n\nPlatform engineering is focused on the development and maintenance of the underlying platform and infrastructure that supports software applications, and involves designing and building the tools, frameworks and systems that developers can use to build and deploy their applications.\n\n## Platform Team Goals and Benefits\n\nThe goals and benefits of platform teams and platform engineering include:\n\n### Increased Efficiency\n\nBy standardizing and streamlining software development, deployment, and maintenance processes, platform teams can help developers to deliver and improve applications more efficiently, reducing errors, improving consistency and increasing speed.\n\n### Improved Scalability and Reliability\n\nBy designing and building a scalable and reliable platform, platform teams can reduce downtime and improve the end-user experience.\n\n### Improved Security\n\nA well-engineered platform can help prevent [data breaches](/resource/common-vulnerabilities-and-exposures-cve-explained), unauthorized access, or other security issues that can compromise sensitive information or harm the business.\n\n### Technical Support and Expertise\n\nBy providing technical support and expertise to development teams, platform teams can help to solve complex technical challenges and ensure that applications are built to industry standards.\n\n### Improved Collaboration and Communication\n\nBy improving collaboration and communication between development teams and IT operations teams, platform teams can help to break down the barriers between these traditionally separate teams and create a culture of continuous integration and delivery.\n\n### Ongoing Evolution and Improvement\n\nBy managing the ongoing evolution and improvement of the platform, platform teams can ensure that it remains up-to-date and able to support the changing needs of the organization, providing ongoing value to the organization and its customers.\n\n## Product Mindset\n\nPlatform teams with a product mindset approach their work as if they are building a product that meets the needs of application teams, treating them as their internal customers. Instead of only focusing on technical requirements, a product-focused platform team considers the user experience and business value that the platform provides.\n\nThis approach involves working closely with application teams to understand their requirements and priorities and using that information to inform the development of the platform. A product-focused platform team takes a product management approach to designing, prioritizing and building the features and capabilities that are most important to application teams and the broader organization. This also involves an ongoing focus on continuous improvement and evolution of the platform based on feedback and changing requirements to better meet the needs of its users.\n\nA product-focused platform team also takes responsibility for the success of the platform, including its adoption and usage by development teams. They work to ensure that the platform is easy to use and integrate with, and that it provides clear benefits to application developers and the organization as a whole.\n\n## Frictionless Self Service\n\nThe product mindset of a platform team can help deliver frictionless self-service to application development teams by focusing on the needs and priorities of those teams and delivering a platform that is easy to use, integrate and operate. This approach involves a few key elements:\n\n- **Understanding the needs of application development teams.** A product-focused platform team will work closely with application development teams to understand their needs, priorities and pain points. This understanding allows the platform team to prioritize the features and capabilities that are most important to application development teams and deliver a platform that meets their needs.\n- **Delivering a user-friendly platform.** A product-focused platform team will design and build a platform that is user-friendly and easy to navigate. This can involve creating clear documentation, providing intuitive interfaces and offering self-service capabilities that enable app teams to easily provision and manage the resources they need.\n- **Providing ongoing support and feedback.** A product-focused platform team will provide ongoing support and feedback to application development teams, helping them to troubleshoot issues, optimize their usage of the platform and improve their overall experience.\n\nBy taking this approach, a product-focused platform team can deliver frictionless self-service to app teams, making it easy for them to provision, deploy and manage the resources they need to build and operate their applications. This can help to improve the efficiency and quality of software development, reduce the workload on operations teams, and ultimately deliver better software products to customers.\n\n## What’s the Difference Between Platform Engineering and DevOps?\n\nPlatform engineering refers to the process of designing, building and maintaining the infrastructure and underlying platform that supports the development and deployment of software applications. Platform engineers are responsible for creating the foundational tools, frameworks, and systems that software developers and other stakeholders can use to build and deliver software products. They focus on issues such as scalability, reliability, security and performance, and work to create a stable and efficient platform that can support the needs of the organization.\n\nDevOps, on the other hand, is a broader set of practices and principles that aim to improve collaboration and communication between software development teams and IT operations teams. DevOps is focused on breaking down the barriers between these traditionally separate teams and creating a culture of continuous integration and delivery. DevOps teams use tools and processes such as version control, automated testing and continuous deployment to streamline the software development and deployment process and improve the speed and quality of software releases.\n\n## What Role Does Service Mesh Play in the Platform?\n\n[A service mesh](/what-is-istio-service-mesh) provides several features and capability that are particularly important for internal developer platforms, including:\n\n### Traffic Management\n\nService mesh can manage traffic between microservices, [providing load balancing](/load-balance-failover-kafka-redhat-amq-streams-k8s-tsb), routing and service discovery capabilities. This can help developers to easily connect their microservices and create complex applications.\n\n### Security\n\nService mesh can provide secure communication between microservices, using features such as mutual TLS and access control policies. This can help to ensure that sensitive data is protected and that communication between microservices is authenticated, authorized and encrypted.\n\n### Observability\n\nService mesh can provide insights and visibility into the communication between microservices, allowing developers to monitor and troubleshoot issues in real-time. This can help to reduce the time and effort required to diagnose and fix problems, and ensure that applications are functioning correctly.\n\n### Resiliency \n\nService mesh provides features such as circuit breaking and retry logic, improving the resilience of the platform and reducing the impact of failures. This can help to ensure that applications are available and functioning correctly, even in the face of failures or errors.\n\n## Enterprise Service Mesh\n\nPlatform teams for large organizations with fleets of applications across multiple clusters, clouds, and data centers often need to “connect the dots” across service mesh deployments to provide a unified and consistent operational model for their customers. For platform teams in these larger organizations, Tetrate offers enterprise service mesh solutions to do just that.\n\nTetrate’s enterprise-grade service mesh platform, Tetrate Service Bridge, unifies and simplifies the connectivity, security, observability, and reliability for your entire application fleet—across [Kubernetes](/learn/kubernetes-security-best-practices) clusters, virtual machines, bare metal servers and across clouds and on-premises deployments\n\nTSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.\n\n[Learn more ›](/tetrate-service-bridge)","src/content/learn/what-is-platform-team/index.md","b241f95e6831103e",{html:852,metadata:853},"<h2 id=\"overview\">Overview</h2>\n<p>A platform team is responsible for creating and maintaining the underlying platform that supports the development, deployment and operations of software <a href=\"/external-link/\">applications</a></p>\n<p>The platform team typically includes software engineers, systems administrators, network engineers, database administrators, security experts and other specialists who work together to ensure the platform is secure, scalable and reliable. The platform team is responsible for the technical aspects of the platform, including <a href=\"/learn/kubernetes-security-architecture\">architecture</a> design, system configuration, coding, testing, deployment and ongoing maintenance.</p>\n<blockquote>\n<p>Tetrate offers enterprise-ready, 100% upstream distributions of Istio and Envoy Gateway, the de facto standard connectivity platform for cloud-native applications. <a href=\"/demo-request\">Get access now ›</a></p>\n</blockquote>\n<h3 id=\"origins-of-platform-engineering\">Origins of Platform Engineering</h3>\n<p>Platform engineering can be traced back to the rise of cloud computing and the need for scalable and reliable infrastructure to support software applications.</p>\n<p>As organizations began to move their applications and data to the cloud, they saw the need for standardized and scalable infrastructure that could support the rapid development and deployment of software applications. This led to the development of platform-as-a-service (PaaS) offerings, which provided developers with a pre-built platform for building and deploying applications.</p>\n<p>As PaaS offerings became more popular, organizations recognized the value of a dedicated internal developer platform and teams with a product mindset dedicated to building and managing it. This led to the emergence of platform teams and the concept of platform engineering.</p>\n<p>Platform engineering is focused on the development and maintenance of the underlying platform and infrastructure that supports software applications, and involves designing and building the tools, frameworks and systems that developers can use to build and deploy their applications.</p>\n<h2 id=\"platform-team-goals-and-benefits\">Platform Team Goals and Benefits</h2>\n<p>The goals and benefits of platform teams and platform engineering include:</p>\n<h3 id=\"increased-efficiency\">Increased Efficiency</h3>\n<p>By standardizing and streamlining software development, deployment, and maintenance processes, platform teams can help developers to deliver and improve applications more efficiently, reducing errors, improving consistency and increasing speed.</p>\n<h3 id=\"improved-scalability-and-reliability\">Improved Scalability and Reliability</h3>\n<p>By designing and building a scalable and reliable platform, platform teams can reduce downtime and improve the end-user experience.</p>\n<h3 id=\"improved-security\">Improved Security</h3>\n<p>A well-engineered platform can help prevent <a href=\"/resource/common-vulnerabilities-and-exposures-cve-explained\">data breaches</a>, unauthorized access, or other security issues that can compromise sensitive information or harm the business.</p>\n<h3 id=\"technical-support-and-expertise\">Technical Support and Expertise</h3>\n<p>By providing technical support and expertise to development teams, platform teams can help to solve complex technical challenges and ensure that applications are built to industry standards.</p>\n<h3 id=\"improved-collaboration-and-communication\">Improved Collaboration and Communication</h3>\n<p>By improving collaboration and communication between development teams and IT operations teams, platform teams can help to break down the barriers between these traditionally separate teams and create a culture of continuous integration and delivery.</p>\n<h3 id=\"ongoing-evolution-and-improvement\">Ongoing Evolution and Improvement</h3>\n<p>By managing the ongoing evolution and improvement of the platform, platform teams can ensure that it remains up-to-date and able to support the changing needs of the organization, providing ongoing value to the organization and its customers.</p>\n<h2 id=\"product-mindset\">Product Mindset</h2>\n<p>Platform teams with a product mindset approach their work as if they are building a product that meets the needs of application teams, treating them as their internal customers. Instead of only focusing on technical requirements, a product-focused platform team considers the user experience and business value that the platform provides.</p>\n<p>This approach involves working closely with application teams to understand their requirements and priorities and using that information to inform the development of the platform. A product-focused platform team takes a product management approach to designing, prioritizing and building the features and capabilities that are most important to application teams and the broader organization. This also involves an ongoing focus on continuous improvement and evolution of the platform based on feedback and changing requirements to better meet the needs of its users.</p>\n<p>A product-focused platform team also takes responsibility for the success of the platform, including its adoption and usage by development teams. They work to ensure that the platform is easy to use and integrate with, and that it provides clear benefits to application developers and the organization as a whole.</p>\n<h2 id=\"frictionless-self-service\">Frictionless Self Service</h2>\n<p>The product mindset of a platform team can help deliver frictionless self-service to application development teams by focusing on the needs and priorities of those teams and delivering a platform that is easy to use, integrate and operate. This approach involves a few key elements:</p>\n<ul>\n<li><strong>Understanding the needs of application development teams.</strong> A product-focused platform team will work closely with application development teams to understand their needs, priorities and pain points. This understanding allows the platform team to prioritize the features and capabilities that are most important to application development teams and deliver a platform that meets their needs.</li>\n<li><strong>Delivering a user-friendly platform.</strong> A product-focused platform team will design and build a platform that is user-friendly and easy to navigate. This can involve creating clear documentation, providing intuitive interfaces and offering self-service capabilities that enable app teams to easily provision and manage the resources they need.</li>\n<li><strong>Providing ongoing support and feedback.</strong> A product-focused platform team will provide ongoing support and feedback to application development teams, helping them to troubleshoot issues, optimize their usage of the platform and improve their overall experience.</li>\n</ul>\n<p>By taking this approach, a product-focused platform team can deliver frictionless self-service to app teams, making it easy for them to provision, deploy and manage the resources they need to build and operate their applications. This can help to improve the efficiency and quality of software development, reduce the workload on operations teams, and ultimately deliver better software products to customers.</p>\n<h2 id=\"whats-the-difference-between-platform-engineering-and-devops\">What’s the Difference Between Platform Engineering and DevOps?</h2>\n<p>Platform engineering refers to the process of designing, building and maintaining the infrastructure and underlying platform that supports the development and deployment of software applications. Platform engineers are responsible for creating the foundational tools, frameworks, and systems that software developers and other stakeholders can use to build and deliver software products. They focus on issues such as scalability, reliability, security and performance, and work to create a stable and efficient platform that can support the needs of the organization.</p>\n<p>DevOps, on the other hand, is a broader set of practices and principles that aim to improve collaboration and communication between software development teams and IT operations teams. DevOps is focused on breaking down the barriers between these traditionally separate teams and creating a culture of continuous integration and delivery. DevOps teams use tools and processes such as version control, automated testing and continuous deployment to streamline the software development and deployment process and improve the speed and quality of software releases.</p>\n<h2 id=\"what-role-does-service-mesh-play-in-the-platform\">What Role Does Service Mesh Play in the Platform?</h2>\n<p><a href=\"/what-is-istio-service-mesh\">A service mesh</a> provides several features and capability that are particularly important for internal developer platforms, including:</p>\n<h3 id=\"traffic-management\">Traffic Management</h3>\n<p>Service mesh can manage traffic between microservices, <a href=\"/load-balance-failover-kafka-redhat-amq-streams-k8s-tsb\">providing load balancing</a>, routing and service discovery capabilities. This can help developers to easily connect their microservices and create complex applications.</p>\n<h3 id=\"security\">Security</h3>\n<p>Service mesh can provide secure communication between microservices, using features such as mutual TLS and access control policies. This can help to ensure that sensitive data is protected and that communication between microservices is authenticated, authorized and encrypted.</p>\n<h3 id=\"observability\">Observability</h3>\n<p>Service mesh can provide insights and visibility into the communication between microservices, allowing developers to monitor and troubleshoot issues in real-time. This can help to reduce the time and effort required to diagnose and fix problems, and ensure that applications are functioning correctly.</p>\n<h3 id=\"resiliency\">Resiliency </h3>\n<p>Service mesh provides features such as circuit breaking and retry logic, improving the resilience of the platform and reducing the impact of failures. This can help to ensure that applications are available and functioning correctly, even in the face of failures or errors.</p>\n<h2 id=\"enterprise-service-mesh\">Enterprise Service Mesh</h2>\n<p>Platform teams for large organizations with fleets of applications across multiple clusters, clouds, and data centers often need to “connect the dots” across service mesh deployments to provide a unified and consistent operational model for their customers. For platform teams in these larger organizations, Tetrate offers enterprise service mesh solutions to do just that.</p>\n<p>Tetrate’s enterprise-grade service mesh platform, Tetrate Service Bridge, unifies and simplifies the connectivity, security, observability, and reliability for your entire application fleet—across <a href=\"/learn/kubernetes-security-best-practices\">Kubernetes</a> clusters, virtual machines, bare metal servers and across clouds and on-premises deployments</p>\n<p>TSB is also a “bridge” between your organization—its people, teams, and applications—and your compute infrastructure, making it easy to assign consistent policies and access rights so your teams can safely control application resources.</p>\n<p><a href=\"/tetrate-service-bridge\">Learn more ›</a></p>",{headings:854,localImagePaths:901,remoteImagePaths:902,frontmatter:903,imagePaths:906},[855,856,859,862,865,868,871,874,877,880,883,886,889,892,893,894,895,898],{depth:29,slug:186,text:187},{depth:427,slug:857,text:858},"origins-of-platform-engineering","Origins of Platform Engineering",{depth:29,slug:860,text:861},"platform-team-goals-and-benefits","Platform Team Goals and Benefits",{depth:427,slug:863,text:864},"increased-efficiency","Increased Efficiency",{depth:427,slug:866,text:867},"improved-scalability-and-reliability","Improved Scalability and Reliability",{depth:427,slug:869,text:870},"improved-security","Improved Security",{depth:427,slug:872,text:873},"technical-support-and-expertise","Technical Support and Expertise",{depth:427,slug:875,text:876},"improved-collaboration-and-communication","Improved Collaboration and Communication",{depth:427,slug:878,text:879},"ongoing-evolution-and-improvement","Ongoing Evolution and Improvement",{depth:29,slug:881,text:882},"product-mindset","Product Mindset",{depth:29,slug:884,text:885},"frictionless-self-service","Frictionless Self Service",{depth:29,slug:887,text:888},"whats-the-difference-between-platform-engineering-and-devops","What’s the Difference Between Platform Engineering and DevOps?",{depth:29,slug:890,text:891},"what-role-does-service-mesh-play-in-the-platform","What Role Does Service Mesh Play in the Platform?",{depth:427,slug:584,text:585},{depth:427,slug:279,text:280},{depth:427,slug:276,text:277},{depth:427,slug:896,text:897},"resiliency","Resiliency ",{depth:29,slug:899,text:900},"enterprise-service-mesh","Enterprise Service Mesh",[],[],{title:843,slug:840,date:904,description:847,categories:905,excerpt:844},["Date","2025-04-18T10:02:55.000Z"],[17],[],"what-is-platform-team/index.md","what-is-wasm",{id:908,data:910,body:916,filePath:917,digest:918,rendered:919,legacyId:969},{title:911,excerpt:912,categories:913,date:914,description:915,draft:20},"What Is Wasm?","WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and.",[17],["Date","2024-09-15T03:52:11.000Z"],"Uncover WebAssembly's power: revolutionizing web dev with enhanced performance, portability, security. Explore wazero, Go-based Wasm runtime.","WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and [security](/learn/kubernetes-security-best-practices).\n\nOne of the key advantages of Wasm is its performance. Unlike JavaScript, which is an interpreted language, Wasm is a compiled language, meaning that it can be executed more quickly and efficiently. This makes Wasm a good choice for applications that require high performance.\n\nAnother advantage of Wasm is its portability. Since it is a bytecode format, Wasm can be compiled from multiple high-level languages and run on any platform that has a Wasm runtime, such as web browsers, servers or even IoT devices.\n\n## History Of Wasm\n\nWebAssembly was first announced in 2015 as a collaboration between major browser vendors such as Mozilla, Google, Microsoft and Apple. It was designed to improve the performance of web applications and to make it possible to write apps in multiple languages that compile to a common, portable format.\n\n## How Wasm Works\n\nWebAssembly is a binary format designed to be executed in a virtual machine. The virtual machine is implemented in the browser or in other environments such as Node.js or Go (using [wazero](/external-link/)\n\nWebAssembly code is typically generated by compilers targeting the Wasm format. The code is optimized for performance and size and can be loaded and executed quickly.\n\n## Benefits Of Wasm\n\nWebAssembly has several benefits over other web technologies such as JavaScript:\n\n### Performance\n\nWebAssembly code executes much faster than JavaScript code, especially for computationally intensive tasks.\n\n### Portability\n\nWebAssembly code can be executed in any environment with a Wasm runtime, including web browsers and Node.js.\n\n### Security\n\nWebAssembly code is executed in a sandboxed environment, isolating it from the rest of the system, improving security.\n\n### Interoperability\n\nWebAssembly code can be written in any language that can be compiled to the Wasm format, enabling code reuse and interoperability between different programming languages.\n\n## Wasm Use Cases\n\nWasm can be used for a variety of purposes, ranging from improving the performance of web applications to enabling new types of applications to run in the browser. Here are some of the most common use cases for Wasm:\n\n### Web Applications\n\nOne of the most obvious Wasm use cases is improving the performance of web applications, particularly for tasks such as video editing and scientific simulations.\n\n### Game Development\n\nAnother use case is game development, where wasm can be used to build high-performance games that run in the browser.\n\n### Serverless Computing \n\nWebAssembly can be used to write serverless functions that can be executed in a serverless environment, such as AWS Lambda or Google Cloud Functions. This can improve the performance and scalability of serverless applications.\n\n### Desktop And Mobile Applications\n\nAnother use case for WebAssembly is building cross-platform desktop and mobile applications that can run on multiple platforms without requiring separate builds for each platform. This can reduce development time and cost compared to building separate applications for each platform.\n\n## What’s The Difference Between Wasm And Javascript?\n\nJavaScript is a high-level programming language that is commonly used for web development. In client side applications, it is interpreted and executed in the browser, allowing developers to create dynamic, interactive web pages. JavaScript is a versatile language that can be used for a wide range of purposes, including building web applications, server-side applications, and mobile applications.\n\nWebAssembly, on the other hand, is a low-level bytecode format suitable for compilation from multiple higher-level languages. It provides a portable, efficient, and secure way to execute code in the browser—and other environments, and it can be used alongside JavaScript or other programming languages.\n\nA key difference between Wasm and JavaScript is their performance characteristics. JavaScript is an interpreted language, which means that it can be slower than compiled code. WebAssembly, on the other hand, is designed to execute at near-native speeds, making it more suitable for high performance tasks.\n\nAnother difference between Wasm and JavaScript is their syntax and programming models. JavaScript is a high-level language that supports object-oriented and functional programming paradigms. WebAssembly, on the other hand, is a low-level language that is more akin to machine code. Wasm modules are typically written in higher-level languages and compiled to the Wasm format for execution in a Wasm runtime environment.\n\nDespite these differences, Wasm and JavaScript can be used together to provide a more powerful and versatile web development environment. For example, Wasm can be used to improve the performance of JavaScript applications by offloading computationally intensive tasks to Wasm modules.\n\n## Wazero And Wasm For Go\n\n[wazero](/external-link/)","src/content/learn/what-is-wasm/index.md","da058c8eb6ff5bc4",{html:920,metadata:921},"<p>WebAssembly (abbreviated as Wasm) is a low-level bytecode format designed as a portable target for the compilation of high-level languages like C, C++, and Rust, enabling deployment on the web for client and server applications. Wasm is designed to be executed in a sandboxed environment, ensuring safety and <a href=\"/learn/kubernetes-security-best-practices\">security</a>.</p>\n<p>One of the key advantages of Wasm is its performance. Unlike JavaScript, which is an interpreted language, Wasm is a compiled language, meaning that it can be executed more quickly and efficiently. This makes Wasm a good choice for applications that require high performance.</p>\n<p>Another advantage of Wasm is its portability. Since it is a bytecode format, Wasm can be compiled from multiple high-level languages and run on any platform that has a Wasm runtime, such as web browsers, servers or even IoT devices.</p>\n<h2 id=\"history-of-wasm\">History Of Wasm</h2>\n<p>WebAssembly was first announced in 2015 as a collaboration between major browser vendors such as Mozilla, Google, Microsoft and Apple. It was designed to improve the performance of web applications and to make it possible to write apps in multiple languages that compile to a common, portable format.</p>\n<h2 id=\"how-wasm-works\">How Wasm Works</h2>\n<p>WebAssembly is a binary format designed to be executed in a virtual machine. The virtual machine is implemented in the browser or in other environments such as Node.js or Go (using <a href=\"/external-link/\">wazero</a></p>\n<p>WebAssembly code is typically generated by compilers targeting the Wasm format. The code is optimized for performance and size and can be loaded and executed quickly.</p>\n<h2 id=\"benefits-of-wasm\">Benefits Of Wasm</h2>\n<p>WebAssembly has several benefits over other web technologies such as JavaScript:</p>\n<h3 id=\"performance\">Performance</h3>\n<p>WebAssembly code executes much faster than JavaScript code, especially for computationally intensive tasks.</p>\n<h3 id=\"portability\">Portability</h3>\n<p>WebAssembly code can be executed in any environment with a Wasm runtime, including web browsers and Node.js.</p>\n<h3 id=\"security\">Security</h3>\n<p>WebAssembly code is executed in a sandboxed environment, isolating it from the rest of the system, improving security.</p>\n<h3 id=\"interoperability\">Interoperability</h3>\n<p>WebAssembly code can be written in any language that can be compiled to the Wasm format, enabling code reuse and interoperability between different programming languages.</p>\n<h2 id=\"wasm-use-cases\">Wasm Use Cases</h2>\n<p>Wasm can be used for a variety of purposes, ranging from improving the performance of web applications to enabling new types of applications to run in the browser. Here are some of the most common use cases for Wasm:</p>\n<h3 id=\"web-applications\">Web Applications</h3>\n<p>One of the most obvious Wasm use cases is improving the performance of web applications, particularly for tasks such as video editing and scientific simulations.</p>\n<h3 id=\"game-development\">Game Development</h3>\n<p>Another use case is game development, where wasm can be used to build high-performance games that run in the browser.</p>\n<h3 id=\"serverless-computing\">Serverless Computing </h3>\n<p>WebAssembly can be used to write serverless functions that can be executed in a serverless environment, such as AWS Lambda or Google Cloud Functions. This can improve the performance and scalability of serverless applications.</p>\n<h3 id=\"desktop-and-mobile-applications\">Desktop And Mobile Applications</h3>\n<p>Another use case for WebAssembly is building cross-platform desktop and mobile applications that can run on multiple platforms without requiring separate builds for each platform. This can reduce development time and cost compared to building separate applications for each platform.</p>\n<h2 id=\"whats-the-difference-between-wasm-and-javascript\">What’s The Difference Between Wasm And Javascript?</h2>\n<p>JavaScript is a high-level programming language that is commonly used for web development. In client side applications, it is interpreted and executed in the browser, allowing developers to create dynamic, interactive web pages. JavaScript is a versatile language that can be used for a wide range of purposes, including building web applications, server-side applications, and mobile applications.</p>\n<p>WebAssembly, on the other hand, is a low-level bytecode format suitable for compilation from multiple higher-level languages. It provides a portable, efficient, and secure way to execute code in the browser—and other environments, and it can be used alongside JavaScript or other programming languages.</p>\n<p>A key difference between Wasm and JavaScript is their performance characteristics. JavaScript is an interpreted language, which means that it can be slower than compiled code. WebAssembly, on the other hand, is designed to execute at near-native speeds, making it more suitable for high performance tasks.</p>\n<p>Another difference between Wasm and JavaScript is their syntax and programming models. JavaScript is a high-level language that supports object-oriented and functional programming paradigms. WebAssembly, on the other hand, is a low-level language that is more akin to machine code. Wasm modules are typically written in higher-level languages and compiled to the Wasm format for execution in a Wasm runtime environment.</p>\n<p>Despite these differences, Wasm and JavaScript can be used together to provide a more powerful and versatile web development environment. For example, Wasm can be used to improve the performance of JavaScript applications by offloading computationally intensive tasks to Wasm modules.</p>\n<h2 id=\"wazero-and-wasm-for-go\">Wazero And Wasm For Go</h2>\n<p><a href=\"/external-link/\">wazero</a></p>",{headings:922,localImagePaths:963,remoteImagePaths:964,frontmatter:965,imagePaths:968},[923,926,929,932,935,938,939,942,945,948,951,954,957,960],{depth:29,slug:924,text:925},"history-of-wasm","History Of Wasm",{depth:29,slug:927,text:928},"how-wasm-works","How Wasm Works",{depth:29,slug:930,text:931},"benefits-of-wasm","Benefits Of Wasm",{depth:427,slug:933,text:934},"performance","Performance",{depth:427,slug:936,text:937},"portability","Portability",{depth:427,slug:279,text:280},{depth:427,slug:940,text:941},"interoperability","Interoperability",{depth:29,slug:943,text:944},"wasm-use-cases","Wasm Use Cases",{depth:427,slug:946,text:947},"web-applications","Web Applications",{depth:427,slug:949,text:950},"game-development","Game Development",{depth:427,slug:952,text:953},"serverless-computing","Serverless Computing ",{depth:427,slug:955,text:956},"desktop-and-mobile-applications","Desktop And Mobile Applications",{depth:29,slug:958,text:959},"whats-the-difference-between-wasm-and-javascript","What’s The Difference Between Wasm And Javascript?",{depth:29,slug:961,text:962},"wazero-and-wasm-for-go","Wazero And Wasm For Go",[],[],{title:911,slug:908,date:966,description:915,categories:967,excerpt:912},["Date","2024-09-15T03:52:11.000Z"],[17],[],"what-is-wasm/index.md","what-is-zero-trust-architecture",{id:970,data:972,body:978,filePath:979,digest:980,rendered:981,legacyId:1010},{title:973,excerpt:974,categories:975,date:976,description:977,draft:20},"What Is Zero Trust Security?","Zero Trust Security—sometimes called Zero Trust Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information security model(/learn/kubernetes-security-best-practices/) that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by John Kindervag at Forrester in.",[17],["Date","2025-04-18T10:04:15.000Z"],"Explore Zero Trust Security: principles, implementation, and its transformative impact on cybersecurity with Tetrate's comprehensive guide.","## Overview\n\nZero Trust Security—sometimes called Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information [security model](/learn/kubernetes-security-best-practices) that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by [John Kindervag at Forrester in 2010](/external-link/)\n\nInstead, [Zero Trust](/zero-trust) requires continuous verification and authentication of users and devices before granting access to any network resources. This is typically done through multi-factor authentication, role-based access control and monitoring of user activity to detect and respond to any suspicious behavior.\n\nZero Trust also requires strict segmentation of network resources, so users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.\n\nZero Trust systems have a simple litmus test:  if exposed to a public network, they wouldn’t need to change. If properly implemented, a Zero Trust security architecture would be as secure on the public Internet as it is behind a firewall.\n\n## Principles of Zero Trust\n\nZero Trust is an approach—a way of thinking about network security—more than it is any particular topology, technology, or implementation. It starts from an assumption that there are no safe places on the network.\n\nIn the Zero Trust model, unlike traditional perimeter security, reachability does not imply authorization. Zero Trust seeks to shrink implicitly trusted zones around resources, ideally to zero. In a Zero Trust network, all access to resources should:\n\n**Assume a breach.** Zero Trust assumes attackers may already be present within the network, and therefore requires continuous monitoring and analysis of user and device behavior to detect and respond to any suspicious activity.\n\n**Authenticate and dynamically authorize access.** Authentication and authorization should be verified not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.\n\n**Bind trust in space.** The perimeter of trust around a resource should be as small as possible.\n\n**Bind trust in time.** Authentication and authorization should be bound to a short-lived session and re-established frequently.\n\n**Encrypt communication**. Network communication should be encrypted to prevent eavesdropping and to ensure messages are authentic and unaltered.\n\n**Use least privilege access.** Zero Trust requires granting users and devices only the minimum level of access required to perform their tasks, based on their roles and responsibilities within the organization.\n\n**Ensure observability.** A Zero Trust system should be observable so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.\n\n## Zero Trust vs Perimeter Security\n\nPerimeter security is based on the idea of creating a secure boundary around an organization’s network, typically using firewalls and other network security devices to block unauthorized access from external sources. Once a user or device is inside the perimeter, however, it is often assumed to be trusted and granted access to network resources without further authentication or verification.\n\nTraditional network security models that rely on a hardened perimeter have become less effective in the face of advanced persistent threats and targeted attacks. With the rise of cloud computing, mobile devices, and remote work, traditional network perimeters have become less relevant, making it easier for attackers to infiltrate networks and gain access to sensitive data.\n\nZero Trust, on the other hand, assumes that all users and devices,, both internal and external, may be potential threats and requires continuous verification and authentication of their identity and access permissions. This means that users and devices—and other system components—must be authenticated and authorized every time they attempt to access network resources, even if they are already inside the organization’s perimeter.\n\nAdditionally, Zero Trust requires strict segmentation of network resources, so that users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.\n\n## Benefits of Zero Trust Architecture\n\n- Accessibility is not authorization—unlike perimeter security, access to a resource is not granted solely because that resource is reachable; it must be explicitly authenticated and authorized as well.\n- Authenticated and authorized resources are protected from perimeter breaches.\n- Bounding trust in time limits the risk of compromised credentials.\n- Bounding trust in space allows for high granularity of policy enforcement.\n- Dynamic policy enforcement ensures authorization policy is up-to-date.\n- Encryption limits reconnaissance and ensures authenticity of communication.\n- Least privilege ensures that users, applications, and systems have only the access rights and permissions necessary to perform their specific functions. This approach helps prevent unauthorized access to sensitive data and resources and limits the damage that can be caused by a security incident\n- Fine-grained observability allows real-time assurance policy is being enforced and allows post-facto auditability of how policy has been enforced historically, plus the necessary data for troubleshooting and analysis.\n\n## Challenges of Implementing Zero Trust\n\nWhile Zero Trust architecture offers many benefits, there are also several challenges organizations may face when adopting this approach:\n\n**Complexity.** Implementing Zero Trust architecture requires a comprehensive understanding of an organization’s network environment, including all users, devices, applications and data. This can be a complex and time-consuming process, especially for organizations with large, distributed networks.\n\n**User experience.** Zero Trust requires continuous authentication and verification of users and devices, which can impact the user experience. If authentication processes are too cumbersome or time-consuming, users may resist using the system, leading to lower productivity and user satisfaction.\n\n**Cost.** Implementing Zero Trust architecture requires investment in new technologies, such as identity and access management systems, behavioral analytics tools, and network segmentation solutions. This can be costly, especially for smaller organizations with limited budgets.\n\n**Legacy systems.** Many organizations still rely on legacy systems and applications that may not be compatible with Zero Trust architecture. Upgrading these systems can be costly and time-consuming.\n\n**Culture and organizational change.** Implementing Zero Trust architecture requires a cultural shift within an organization, as it requires a change in the way users and devices are granted access to network resources. This can be challenging, especially in organizations with a long history of traditional perimeter security approaches.\n\n**Training and education.** Adopting Zero Trust architecture requires training and education for IT staff and end-users to understand the new security protocols and practices. This can be time-consuming and costly, especially for organizations with large and distributed workforces.\n\n## U.S. Government Standards for Zero Trust Security\n\nThe US government has published several standards and guidelines for Zero Trust Architecture. One of the most notable is the National Institute of Standards and Technology (NIST) [Special Publication 800-207](/blog/nist-sp-207-the-groundwork-for-zero-trust), which provides an overview of Zero Trust Architecture and its key components. NIST has also published the [SP 800-204 series](/blog/nist-standards-for-zero-trust-the-sp-800-204-series) on Zero Trust security recommendations for microservices applications.\n\nThe Cybersecurity and Infrastructure Security Agency (CISA) has also published guidance on implementing Zero Trust, including a [Zero Trust Maturity Model](/external-link/)\n\nAdditionally, the Office of Management and Budget (OMB) [has issued a memorandum on Zero Trust Architecture implementation](/blog/us-government-endorses-zero-trust-architecture-for-security), outlining the government’s commitment to transitioning to a Zero Trust model and providing guidance for agencies to follow.\n\n## Service Mesh as a Critical Component of a Zero Trust Architecture\n\nIn SP 800-204B, NIST has established a reference platform consisting of Kubernetes for orchestration and resource management, with Istio service mesh providing the core security features.\n\nA service mesh is important because it closes certain gaps in the communication mechanism of Kubernetes. Kubernetes has some limitations the service mesh addresses, including insecure communications by default, lack of built-in certificate management mechanisms, lack of service identity and access management and firewall policy that operates at OSI L3, but not L7 and is, therefore, unable to peek into data packets or make metadata-driven decisions.\n\nKubernetes doesn’t address certain application-level development and operational needs unique to microservices the service mesh was designed to satisfy. A service mesh provides a unified way to address cross-cutting application concerns, standard plugins to quickly address those concerns and a framework for building custom plugins. It also manages operational complexity, enables easy governance of third-party developers and integrators, and can reduce costs for development and operations.\n\nIstio’s data plane of Envoy proxies provides authentication and authorization, secure service discovery via a dedicated service registry, secure communications including mTLS and encryption, network resilience and unified observability data. Istio’s ingress controller provides a common, external-facing API for all clients, protocol translation, composition of results, load balancing and public TLS termination. Istio’s egress controller provides a single, centralized set of whitelisted external workloads, credential exchange and protocol translation back to web-friendly protocols.\n\n- Learn more about the Istio service mesh ›\n- [Get started with Istio using Tetrate Istio Distro, Tetrate’s hardened, performant, and fully upstream Istio distribution ›](/external-link/)","src/content/learn/what-is-zero-trust-architecture/index.md","cbacfef5838e90d0",{html:982,metadata:983},"<h2 id=\"overview\">Overview</h2>\n<p>Zero Trust Security—sometimes called Zero Trust Architecture (ZTA) and Zero Trust Network Access (ZTNA)—is an information <a href=\"/learn/kubernetes-security-best-practices\">security model</a> that requires strict identity verification for all users, devices, and applications attempting to access resources within a network, regardless of their location. Popularized by <a href=\"/external-link/\">John Kindervag at Forrester in 2010</a></p>\n<p>Instead, <a href=\"/zero-trust\">Zero Trust</a> requires continuous verification and authentication of users and devices before granting access to any network resources. This is typically done through multi-factor authentication, role-based access control and monitoring of user activity to detect and respond to any suspicious behavior.</p>\n<p>Zero Trust also requires strict segmentation of network resources, so users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.</p>\n<p>Zero Trust systems have a simple litmus test:  if exposed to a public network, they wouldn’t need to change. If properly implemented, a Zero Trust security architecture would be as secure on the public Internet as it is behind a firewall.</p>\n<h2 id=\"principles-of-zero-trust\">Principles of Zero Trust</h2>\n<p>Zero Trust is an approach—a way of thinking about network security—more than it is any particular topology, technology, or implementation. It starts from an assumption that there are no safe places on the network.</p>\n<p>In the Zero Trust model, unlike traditional perimeter security, reachability does not imply authorization. Zero Trust seeks to shrink implicitly trusted zones around resources, ideally to zero. In a Zero Trust network, all access to resources should:</p>\n<p><strong>Assume a breach.</strong> Zero Trust assumes attackers may already be present within the network, and therefore requires continuous monitoring and analysis of user and device behavior to detect and respond to any suspicious activity.</p>\n<p><strong>Authenticate and dynamically authorize access.</strong> Authentication and authorization should be verified not only at the network layer and the service-to-service layer, but also at the application layer. Network location does not imply trust. Service identity and end-user credentials are authenticated and dynamically authorized before any access is allowed.</p>\n<p><strong>Bind trust in space.</strong> The perimeter of trust around a resource should be as small as possible.</p>\n<p><strong>Bind trust in time.</strong> Authentication and authorization should be bound to a short-lived session and re-established frequently.</p>\n<p><strong>Encrypt communication</strong>. Network communication should be encrypted to prevent eavesdropping and to ensure messages are authentic and unaltered.</p>\n<p><strong>Use least privilege access.</strong> Zero Trust requires granting users and devices only the minimum level of access required to perform their tasks, based on their roles and responsibilities within the organization.</p>\n<p><strong>Ensure observability.</strong> A Zero Trust system should be observable so the integrity and security posture of all assets may be continuously monitored and policy enforcement continuously assured. Also, insights gained from observing should be fed back to improve policy.</p>\n<h2 id=\"zero-trust-vs-perimeter-security\">Zero Trust vs Perimeter Security</h2>\n<p>Perimeter security is based on the idea of creating a secure boundary around an organization’s network, typically using firewalls and other network security devices to block unauthorized access from external sources. Once a user or device is inside the perimeter, however, it is often assumed to be trusted and granted access to network resources without further authentication or verification.</p>\n<p>Traditional network security models that rely on a hardened perimeter have become less effective in the face of advanced persistent threats and targeted attacks. With the rise of cloud computing, mobile devices, and remote work, traditional network perimeters have become less relevant, making it easier for attackers to infiltrate networks and gain access to sensitive data.</p>\n<p>Zero Trust, on the other hand, assumes that all users and devices,, both internal and external, may be potential threats and requires continuous verification and authentication of their identity and access permissions. This means that users and devices—and other system components—must be authenticated and authorized every time they attempt to access network resources, even if they are already inside the organization’s perimeter.</p>\n<p>Additionally, Zero Trust requires strict segmentation of network resources, so that users and devices only have access to the resources they need to perform their tasks, and nothing more. This reduces the risk of lateral movement by attackers who may gain access to one part of the network and then move laterally to gain access to more sensitive resources.</p>\n<h2 id=\"benefits-of-zero-trust-architecture\">Benefits of Zero Trust Architecture</h2>\n<ul>\n<li>Accessibility is not authorization—unlike perimeter security, access to a resource is not granted solely because that resource is reachable; it must be explicitly authenticated and authorized as well.</li>\n<li>Authenticated and authorized resources are protected from perimeter breaches.</li>\n<li>Bounding trust in time limits the risk of compromised credentials.</li>\n<li>Bounding trust in space allows for high granularity of policy enforcement.</li>\n<li>Dynamic policy enforcement ensures authorization policy is up-to-date.</li>\n<li>Encryption limits reconnaissance and ensures authenticity of communication.</li>\n<li>Least privilege ensures that users, applications, and systems have only the access rights and permissions necessary to perform their specific functions. This approach helps prevent unauthorized access to sensitive data and resources and limits the damage that can be caused by a security incident</li>\n<li>Fine-grained observability allows real-time assurance policy is being enforced and allows post-facto auditability of how policy has been enforced historically, plus the necessary data for troubleshooting and analysis.</li>\n</ul>\n<h2 id=\"challenges-of-implementing-zero-trust\">Challenges of Implementing Zero Trust</h2>\n<p>While Zero Trust architecture offers many benefits, there are also several challenges organizations may face when adopting this approach:</p>\n<p><strong>Complexity.</strong> Implementing Zero Trust architecture requires a comprehensive understanding of an organization’s network environment, including all users, devices, applications and data. This can be a complex and time-consuming process, especially for organizations with large, distributed networks.</p>\n<p><strong>User experience.</strong> Zero Trust requires continuous authentication and verification of users and devices, which can impact the user experience. If authentication processes are too cumbersome or time-consuming, users may resist using the system, leading to lower productivity and user satisfaction.</p>\n<p><strong>Cost.</strong> Implementing Zero Trust architecture requires investment in new technologies, such as identity and access management systems, behavioral analytics tools, and network segmentation solutions. This can be costly, especially for smaller organizations with limited budgets.</p>\n<p><strong>Legacy systems.</strong> Many organizations still rely on legacy systems and applications that may not be compatible with Zero Trust architecture. Upgrading these systems can be costly and time-consuming.</p>\n<p><strong>Culture and organizational change.</strong> Implementing Zero Trust architecture requires a cultural shift within an organization, as it requires a change in the way users and devices are granted access to network resources. This can be challenging, especially in organizations with a long history of traditional perimeter security approaches.</p>\n<p><strong>Training and education.</strong> Adopting Zero Trust architecture requires training and education for IT staff and end-users to understand the new security protocols and practices. This can be time-consuming and costly, especially for organizations with large and distributed workforces.</p>\n<h2 id=\"us-government-standards-for-zero-trust-security\">U.S. Government Standards for Zero Trust Security</h2>\n<p>The US government has published several standards and guidelines for Zero Trust Architecture. One of the most notable is the National Institute of Standards and Technology (NIST) <a href=\"/blog/nist-sp-207-the-groundwork-for-zero-trust\">Special Publication 800-207</a>, which provides an overview of Zero Trust Architecture and its key components. NIST has also published the <a href=\"/blog/nist-standards-for-zero-trust-the-sp-800-204-series\">SP 800-204 series</a> on Zero Trust security recommendations for microservices applications.</p>\n<p>The Cybersecurity and Infrastructure Security Agency (CISA) has also published guidance on implementing Zero Trust, including a <a href=\"/external-link/\">Zero Trust Maturity Model</a></p>\n<p>Additionally, the Office of Management and Budget (OMB) <a href=\"/blog/us-government-endorses-zero-trust-architecture-for-security\">has issued a memorandum on Zero Trust Architecture implementation</a>, outlining the government’s commitment to transitioning to a Zero Trust model and providing guidance for agencies to follow.</p>\n<h2 id=\"service-mesh-as-a-critical-component-of-a-zero-trust-architecture\">Service Mesh as a Critical Component of a Zero Trust Architecture</h2>\n<p>In SP 800-204B, NIST has established a reference platform consisting of Kubernetes for orchestration and resource management, with Istio service mesh providing the core security features.</p>\n<p>A service mesh is important because it closes certain gaps in the communication mechanism of Kubernetes. Kubernetes has some limitations the service mesh addresses, including insecure communications by default, lack of built-in certificate management mechanisms, lack of service identity and access management and firewall policy that operates at OSI L3, but not L7 and is, therefore, unable to peek into data packets or make metadata-driven decisions.</p>\n<p>Kubernetes doesn’t address certain application-level development and operational needs unique to microservices the service mesh was designed to satisfy. A service mesh provides a unified way to address cross-cutting application concerns, standard plugins to quickly address those concerns and a framework for building custom plugins. It also manages operational complexity, enables easy governance of third-party developers and integrators, and can reduce costs for development and operations.</p>\n<p>Istio’s data plane of Envoy proxies provides authentication and authorization, secure service discovery via a dedicated service registry, secure communications including mTLS and encryption, network resilience and unified observability data. Istio’s ingress controller provides a common, external-facing API for all clients, protocol translation, composition of results, load balancing and public TLS termination. Istio’s egress controller provides a single, centralized set of whitelisted external workloads, credential exchange and protocol translation back to web-friendly protocols.</p>\n<ul>\n<li>Learn more about the Istio service mesh ›</li>\n<li><a href=\"/external-link/\">Get started with Istio using Tetrate Istio Distro, Tetrate’s hardened, performant, and fully upstream Istio distribution ›</a></li>\n</ul>",{headings:984,localImagePaths:1004,remoteImagePaths:1005,frontmatter:1006,imagePaths:1009},[985,986,989,992,995,998,1001],{depth:29,slug:186,text:187},{depth:29,slug:987,text:988},"principles-of-zero-trust","Principles of Zero Trust",{depth:29,slug:990,text:991},"zero-trust-vs-perimeter-security","Zero Trust vs Perimeter Security",{depth:29,slug:993,text:994},"benefits-of-zero-trust-architecture","Benefits of Zero Trust Architecture",{depth:29,slug:996,text:997},"challenges-of-implementing-zero-trust","Challenges of Implementing Zero Trust",{depth:29,slug:999,text:1000},"us-government-standards-for-zero-trust-security","U.S. Government Standards for Zero Trust Security",{depth:29,slug:1002,text:1003},"service-mesh-as-a-critical-component-of-a-zero-trust-architecture","Service Mesh as a Critical Component of a Zero Trust Architecture",[],[],{title:973,slug:970,date:1007,description:977,categories:1008,excerpt:974},["Date","2025-04-18T10:04:15.000Z"],[17],[],"what-is-zero-trust-architecture/index.md","why-do-i-we-need-kubernetes",{id:1011,data:1013,body:1020,filePath:1021,digest:1022,rendered:1023,legacyId:1042},{title:1014,excerpt:1015,categories:1016,date:1018,description:1019,draft:20},"Why Do I/We Need Kubernetes?","Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their.",[1017],"Why",["Date","2024-10-17T08:33:18.000Z"],"Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises…","Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their applications.\n\n## Benefits of Kubernetes\n\nUsing Kubernetes to manage your containerization infrastructure offers numerous benefits. It improves scalability, portability, resource efficiency and more. It is an invaluable tool for modern application deployment and management. Some of the key advantages of using Kubernetes include:\n\n### Container Orchestration\n\n- Automated Deployments and Rollbacks\n- Application Self-Healing\n- Service Discovery and Ingress Management\n- Load Balancing\n\n### Scalability\n\n- Scaling up and down again based on demand\n- Autoscaling as needs change without human intervention\n\n### Portability\n\n- Cloud platform agnostic\n- Technology stack neutral\n\n## Common Use Cases for Kubernetes\n\nKubernetes is a versatile tool that IT teams can use for many deployment scenarios. Its primary strength lies in its ability to manage complex, distributed applications through automation, making it an ideal solution for many scenarios. Here are some common real-world use cases where Kubernetes excels:\n\n- Microservices architecture deployments\n- Continuous integration and continuous deployment (CI/CD) workflows\n- DevSecOps and agile development\n- Cloud-native application deployments\n- Supporting [multi-cloud](/resource/simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh) and hybrid cloud strategies\n- Big data and machine learning projects\n- Internet of Things (IoT) and edge computing infrastructure management\n\nAdopting Kubernetes as the platform for deploying, managing, and decommissioning your application containers will significantly reduce the management overhead for your infrastructure.","src/content/learn/why-do-i-we-need-kubernetes/index.md","b8c77209010dc83d",{html:1024,metadata:1025},"<p>Kubernetes is a favored solution among developers and DevSecOps teams due to its wide range of features, container management tools, and support by on-premises infrastructure suppliers and cloud service providers. The main reason for its popularity is that Kubernetes has revolutionized modern application deployment. It has transformed how organizations deploy, scale, and manage their applications.</p>\n<h2 id=\"benefits-of-kubernetes\">Benefits of Kubernetes</h2>\n<p>Using Kubernetes to manage your containerization infrastructure offers numerous benefits. It improves scalability, portability, resource efficiency and more. It is an invaluable tool for modern application deployment and management. Some of the key advantages of using Kubernetes include:</p>\n<h3 id=\"container-orchestration\">Container Orchestration</h3>\n<ul>\n<li>Automated Deployments and Rollbacks</li>\n<li>Application Self-Healing</li>\n<li>Service Discovery and Ingress Management</li>\n<li>Load Balancing</li>\n</ul>\n<h3 id=\"scalability\">Scalability</h3>\n<ul>\n<li>Scaling up and down again based on demand</li>\n<li>Autoscaling as needs change without human intervention</li>\n</ul>\n<h3 id=\"portability\">Portability</h3>\n<ul>\n<li>Cloud platform agnostic</li>\n<li>Technology stack neutral</li>\n</ul>\n<h2 id=\"common-use-cases-for-kubernetes\">Common Use Cases for Kubernetes</h2>\n<p>Kubernetes is a versatile tool that IT teams can use for many deployment scenarios. Its primary strength lies in its ability to manage complex, distributed applications through automation, making it an ideal solution for many scenarios. Here are some common real-world use cases where Kubernetes excels:</p>\n<ul>\n<li>Microservices architecture deployments</li>\n<li>Continuous integration and continuous deployment (CI/CD) workflows</li>\n<li>DevSecOps and agile development</li>\n<li>Cloud-native application deployments</li>\n<li>Supporting <a href=\"/resource/simplify-kubernetes-and-multi-cloud-complexity-with-the-service-mesh\">multi-cloud</a> and hybrid cloud strategies</li>\n<li>Big data and machine learning projects</li>\n<li>Internet of Things (IoT) and edge computing infrastructure management</li>\n</ul>\n<p>Adopting Kubernetes as the platform for deploying, managing, and decommissioning your application containers will significantly reduce the management overhead for your infrastructure.</p>",{headings:1026,localImagePaths:1036,remoteImagePaths:1037,frontmatter:1038,imagePaths:1041},[1027,1030,1031,1032,1033],{depth:29,slug:1028,text:1029},"benefits-of-kubernetes","Benefits of Kubernetes",{depth:427,slug:351,text:352},{depth:427,slug:513,text:514},{depth:427,slug:936,text:937},{depth:29,slug:1034,text:1035},"common-use-cases-for-kubernetes","Common Use Cases for Kubernetes",[],[],{title:1014,slug:1011,date:1039,description:1019,categories:1040,excerpt:1015},["Date","2024-10-17T08:33:18.000Z"],[1017],[],"why-do-i-we-need-kubernetes/index.md","resources",["Map",1045,1046],"example-resource",{id:1045,data:1047,body:1052,filePath:1053,assetImports:1054,digest:1056,rendered:1057,legacyId:1073},{title:1048,categories:1043,featuredImage:1049,date:1050,draft:20,description:1051},"Example Resource","__ASTRO_IMAGE_https://tetrate.io/wp-content/uploads/2024/01/example-image.png",["Date","2024-01-01T00:00:00.000Z"],"This is an example resource to test the resources section","This is an example resource content.\n\n## Introduction\n\nThis resource demonstrates how the resources section works.\n\n## Content\n\nYou can add any markdown content here.\n\n## Conclusion\n\nResources are now working!","src/content/resources/example-resource/index.md",[1055],"https://tetrate.io/wp-content/uploads/2024/01/example-image.png","c93f69affabb69ad",{html:1058,metadata:1059},"<p>This is an example resource content.</p>\n<h2 id=\"introduction\">Introduction</h2>\n<p>This resource demonstrates how the resources section works.</p>\n<h2 id=\"content\">Content</h2>\n<p>You can add any markdown content here.</p>\n<h2 id=\"conclusion\">Conclusion</h2>\n<p>Resources are now working!</p>",{headings:1060,localImagePaths:1068,remoteImagePaths:1069,frontmatter:1070,imagePaths:1072},[1061,1064,1067],{depth:29,slug:1062,text:1063},"introduction","Introduction",{depth:29,slug:1065,text:1066},"content","Content",{depth:29,slug:360,text:361},[],[],{title:1048,description:1051,date:1071,categories:1043,featuredImage:1055},["Date","2024-01-01T00:00:00.000Z"],[],"example-resource/index.md"];

export { _astro_dataLayerContent as default };
